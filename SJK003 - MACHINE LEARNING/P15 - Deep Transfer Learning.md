---
Subject: Machine Learning
Teacher: "@RamónAlbertoMollinedaCárdenas @JoséSalvadorSánchezGarreta"
Topic: 
date: 2024-12-04
tags:
  - SJK003-MachineLearning
---
# Context

- deep learning inherits this practice… 
- promise – deep networks are universal approximators, capable of solving any problem 
- fine print – they depend on millions of parameters to optimize! 
- <s>Myth – therefore, you will not be able to create a DL solution unless you have sufficient amounts of data from the target task</s>
- truth – it's possible to… 
	- transfer learned representations to related tasks 
	- learn good representations from unlabeled data 
	- learn representations common to different domains


## Actuality

learn from scratch with insufficient data => overfitting

## Utopia
learn from scratch with unlimited resources… 
- unlimited labeled data 
- unlimited computing power 
- unlimited time 
`(optimal solution at a prohibitive cost)`


# Definition and popular strategy

"*Transfer learning … refers to the situation where what has been learned in one setting … is exploited to improve generalization in another setting.*"

![[Pasted image 20241204174940.png]]

## Formal definition

![[Pasted image 20241204175024.png]]