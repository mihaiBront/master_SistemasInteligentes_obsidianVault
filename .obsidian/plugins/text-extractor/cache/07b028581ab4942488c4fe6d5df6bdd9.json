{"path":"_aula_virtual/SJK003/16-reinforcementLearning.pdf","text":"Ramón A. Mollineda Cárdenas reinforcement learning MACHINE LEARNING University Master's Degree in Intelligent Systems a quote “When it is not in our power to determine what is true, we ought to follow what is most probable.” ― René Descartes french philosopher, mathematician and scientist 1596 – 1650 introduction to reinforcement learning use case Source: Reinforcement learning. Mathworks.com (link). introduction pipeline Source: EBatlleP, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons. sample actions, observe rewards, tune the policy introduction agent Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. • an agent is an entity that interacts with an environment • it receives information from sensing the environments • it interacts with the environment using its actuators • its interactions are based on a policy (agent's brain) introduction use case Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. goal? agent? state? action? environment? short-term rewards? long-term rewards? A mobile robot decides whether it should enter a new room to collect more trash or try to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly it was able to find the recharger in the past. introduction goal RL approach involves learning how to map states to actions, so as to gain the highest long-term cumulative reward. introduction RL summary Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. 1. The agent observes the environment state (environment observation) 2. The agent decides how to act base on some strategy (policy) that maps states into actions; at the beginning, through trial and error 3. The agent acts according to the policy 4. The agent receives a reward or penalty (based on sensor reading) 5. Learning from the experiences and refining the strategy (policy) 6. Iterate until an optimal strategy is found introduction learning paradigms Source: Reinforcement learning. Mathworks.com (link). unsupervised learning supervised learning reinforcement learning introduction learning paradigms • unsupervised learning… o discovers similarities between data samples o identifies clusters based on similarities o is able to detect anomalous data • supervised learning… o learns a mapping from instances to data labels o requires a labelled / annotated dataset o uses labels to guide, correct and monitor the learning process • reinforcement learning… o given a state, there exists an optimal action, but it is not known! o takes actions based on short- and long-term reward (strategy learning) o is guided by sparse and uncertain feedback / supervision o does not require collecting, preprocessing and labeling data before training Source: Chris V. Nicholson. A Beginner's Guide to Deep Reinforcement Learning. Pathmind Inc., 2023. introduction learning paradigms Source: Deepika Yadav. DIFFERENT MACHINE LEARNING MODELS, medium.com, 2022. (link). introduction key principles • it is a closed-loop problem: system's actions affect later inputs. • actions may affect not only the immediate reward but also the next states and, through that, all subsequent rewards. • the agent is not taught what action to take on each state (as in supervised learning); instead, it must discover which action maximizes immediate and future rewards (by interacting with the environment). Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. introduction exploitation – exploration dilemma / trade-off exploit known actions that have proven to be effective (positive rewards) OR explore new (unknown) actions to discover more promising solutions Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. introduction key elements • a policy 𝜋: the brain of a RL agent o a mapping from perceived states to actions to be taken o a stimulus-response rule • a reward signal: what is good in an immediate sense o 𝑟 ∶ 𝑆 × 𝐴 → ℝ o the agent cannot alter 𝑟 o measures the effectiveness/quality of the agent’s interaction • a value function: what is good in the long run (accumulated rewards) o maps a state to the expected return (cumulative reward) when starting from that state o measures the quality of (some sequence of) actions taken from a state o policy tuning is based on value judgments • a model of the environment: it mimics the behavior of the environment o given a state and action, it moves to the next state and provides a reward Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. objective function over an episode ෍ 𝑡=0 ∞ 𝛾𝑡𝑟 𝑠 𝑡 , 𝑎 𝑡 𝑡: time step 𝑟: reward function 𝛾: discount factor 𝑠(𝑡): state at a given time step/epoch 𝑡 𝑎 𝑡 = 𝜋 𝑠 𝑡 policy agent‘s goal ෍ 𝑡=0 ∞ 𝛾𝑡𝑟 𝑠 𝑡 , 𝜋 𝑠 𝑡 policy agent‘s goal ₿1.13 policy agent‘s goal 𝜋∗ = 𝑎𝑟𝑔𝑚𝑎𝑥𝜋 ෍ 𝑡=0 ∞ 𝛾𝑡 𝑟 𝑠 𝑡 , 𝜋 𝑠 𝑡 𝜋 : 𝒮 → 𝒜 𝒮 ~ state space 𝒜 ~ action space RL paradigms tabular solutions: the action-value function is represented as an array or a table approximate, gradient-descent, solutions: the state-action spaces involve continuous domains or complex representations (e.g., an image) Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. tabular solutions representative methods tabular solutions • Q-learning: off policy, model-free RL • State-Action-Reward-State-Action (SARSA): on policy, model-free RL Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. Q-learning* off policy, model-free RL algorithm • 𝑄-learning approximates the optimal action-value function 𝑄∗ that maximizes the expected value over all successive steps** 𝑄: 𝑆 × 𝐴 → ℝ • 𝑄-learning does not require an environment model (model-free) • 𝜋 (policy) depends on 𝑄 • 𝑄 depends on 𝜋 (𝜋 determines which state-action pair should be updated) • 𝑄’s update (learning) does not depend on 𝜋 (off policy) * Watkins, C.J.C.H., Dayan, P. Q-learning. Mach Learn 8, 279–292 (1992). https://doi.org/10.1007/BF00992698. ** 𝑄 has been shown to converge with probability 1 to 𝑄∗ under minimal requirements. Q-learning Bellman equation where... • 𝑎𝑡 = 𝜋 𝑠𝑡 • 𝑟𝑡+1 is the reward associated to the state-action pair 𝑠𝑡, 𝑎𝑡 • 𝑠𝑡+1 is the new state after selecting the action 𝑎𝑡 at state 𝑠𝑡 • 𝛼 is the learning rate 0 ≤ 𝛼 ≤ 1 ; the extent to which our 𝑄-values are being updated in every iteration • 𝛾 is the discount factor 0 ≤ 𝛾 ≤ 1 ; relevance of future rewards (uncertain) • 𝑚𝑎𝑥𝑎𝑄 𝑠𝑡+1, 𝑎 is the maximum value from state 𝑠𝑡+1 (given the current 𝑄) Source: Q-learning. Wikipedia.org. 𝑄 𝑠𝑡, 𝑎𝑡 = 1 − 𝛼 · 𝑄 𝑠𝑡, 𝑎𝑡 + 𝛼 · 𝑟𝑡+1 + 𝛾 · 𝑚𝑎𝑥𝑎𝑄 𝑠𝑡+1, 𝑎 Q-learning off-policy learning • the behavior policy (used to collect data) is different from the policy being optimized • 𝑄's update (learning) does not involve the behavior policy 𝜋 • 𝑄's update depends on 𝑚𝑎𝑥𝑎𝑄 𝑠𝑡+1, 𝑎 , a greedy action from 𝑠𝑡+1 • it is usually assumed an 𝘀-greedy behavior policy 𝜋 𝑄 𝑠𝑡, 𝑎𝑡 = 1 − 𝛼 · 𝑄 𝑠𝑡, 𝑎𝑡 + 𝛼 · 𝑟𝑡+1 + 𝛾 · 𝑚𝑎𝑥𝑎𝑄 𝑠𝑡+1, 𝑎 𝑝 ∈ 0,1 random exploration exploitation 𝑝 < 𝘀 𝑝 ≥ 𝘀 random action best known action 𝑎𝑟𝑔𝑚𝑎𝑥𝑎𝑄 𝑠, 𝑎 Q-learning Bellman equation: remarks • the Bellman equation is a recursion that computes the weighted average of the current value and a new learned value • an episode of the algorithm ends when state 𝑠𝑡+1 is a final state • action values are finite even when no final state exists (provided that 𝛾 < 1) • for final states 𝑠𝑓, 𝑄 𝑠𝑓, 𝑎 is set to the reward observed and is never updated • 𝛼 determines to what extent newly acquired information overrides old information o 𝛼 = 0 makes the the agent learn nothing (exclusively exploiting prior knowledge) o 𝛼 = 1 makes the agent ignore prior knowledge (to explore new possibilities) • 𝛾 determines the importance of future rewards o 𝛾 = 0 makes the agent “myopic” o 𝛾 ≥ 1 the action values might diverge • 𝑄-learning fails with an infinite (or even large) number of states/actions Source: Q-learning. Wikipedia.org. Q-learning algorithm Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. SARSA: State-Action-Reward-State-Action* on policy, model-free RL algorithm * RUMMERY, G.A.; NIRANJAN, M. On-line Q-learning using connectionist systems. Univ. of Cambridge, Dept. of Engineering, UK, 1994. • SARSA estimates 𝑄𝜋 𝑠, 𝑎 for the current agent policy 𝜋 and for all state-action pairs 𝑠, 𝑎 . 𝑄𝜋: 𝑆 × 𝐴 → ℝ • SARSA does not require an environment model (model-free) • 𝜋 depends on 𝑄; 𝑄 depends on 𝜋 (𝜋 determines which state-action pair should be updated), and 𝑄’s update does depend on 𝜋 (on policy) • SARSA explicitly uses the transition from one state-action pair to the next state-action pair (not just the state) in its learning process SARSA: State-Action-Reward-State-Action Bellman equation where... • 𝑎𝑡 = 𝜋 𝑠𝑡 , 𝑎𝑡+1 = 𝜋 𝑠𝑡+1 • 𝑟𝑡+1 is the reward associated to the state-action pair 𝑠𝑡, 𝑎𝑡 • 𝑠𝑡+1 is the new state after selecting the action 𝑎𝑡 at state 𝑠𝑡 • 𝛼 is the learning rate 0 ≤ 𝛼 ≤ 1 • 𝛾 is the discount factor 0 ≤ 𝛾 ≤ 1 • 𝑄 𝑠𝑡+1, 𝑎𝑡+1 is the accumulated reward determines by the current 𝜋 Source: Q-learning. Wikipedia.org. 𝑄 𝑠𝑡, 𝑎𝑡 = 1 − 𝛼 · 𝑄 𝑠𝑡, 𝑎𝑡 + 𝛼 · 𝑟𝑡+1 + 𝛾 · 𝑄 𝑠𝑡+1, 𝑎𝑡+1 SARSA: State-Action-Reward-State-Action on-policy learning 𝑎𝑡+1 = 𝜋 𝑠𝑡+1 • 𝑄's update does depend on the current (agent) behavior policy 𝜋 𝑄 𝑠𝑡, 𝑎𝑡 = 1 − 𝛼 · 𝑄 𝑠𝑡, 𝑎𝑡 + 𝛼 · 𝑟𝑡+1 + 𝛾 · 𝑄 𝑠𝑡+1, 𝑎𝑡+1 SARSA: State-Action-Reward-State-Action algorithm Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. Q-learning versus SARSA Cliff walking example Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. • S: Start state • G: Goal state • actions: up, down, right, and left • reward = -1 except into “The Cliff” • reward = -100 in the “The Cliff” • 𝘀 = 0.1 which path do you think was learned by each method? -> Q-learning -> SARSA Q-learning versus SARSA Cliff walking example Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. • S: Start state • G: Goal state • actions: up, down, right, and left • reward = -1 except into “The Cliff” • reward = -100 in the “The Cliff” • 𝘀 = 0.1 which path do you think was learned by each method? Q-learning versus SARSA Cliff walking example Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. Reward per episode tabular solutions limitations • assumption: value estimates are represented as a table with one entry for each state or for each state-action pair • memory problems: it is limited to tasks with small numbers of states and actions; if not, there could be memory problems • generalization problems: large tables also need large volumes of data and time to generalize (fill) them properly • unobserved states: in many RL tasks, most states encountered will never have been experienced exactly before Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. a self-driving cab case study Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. smart-cab goals are… • pick up a passenger at one location • drop off the passenger to the right location • save time by taking the shortest path • ensure passenger safety • comply with rules a self-driving cab state space Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. state space • taxi locations: a coordinate in a 5 x 5 grid • passenger locations: 5 = 4 coord. to pick up the passenger (R, G, Y, B) + 1 (taxi location) • destination locations: 4 (R, G, Y, B) • state space size = 5 x 5 x 5 x 4 = 500 states • state example: (3, 1, 2, 0) -> 328 • taxi location: coordinate (3, 1) • passenger location: 2 (Y coordinate) • destination location: 0 (R coordinate) • encoded state: 328 ∈ [0, 499] • enconded state = ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination a self-driving cab action space Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. action space (set of all the actions) 1. south 2. north 3. east 4. west 5. pickup 6. dropoff • action space size = 6 actions • some actions are impossible due to walls • wall hits are penalized: • penalty/reward: -1 • the taxi does not move anywhere a self-driving cab rewards Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. rewards • a high positive reward for a successful dropoff • a high negative reward for a wrong dropoff • a slight negative reward for every time-step • reward table: State Space X Action Space matrix • reward table[state] returns a dictionary… {action: [(probability, nextstate, reward, done)]} • rewards at state (row, entry) 328 { 0: [(1.0, 428, -1, False)], 1: [(1.0, 228, -1, False)], 2: [(1.0, 348, -1, False)], 3: [(1.0, 328, -1, False)], 4: [(1.0, 328, -10, False)], 5: [(1.0, 328, -10, False)]} a self-driving cab 𝑄-learning Source: Satwik Kansal, Brendan Martin. Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. LearnDataSci, 2023. RL paradigms tabular solutions: the action-value function is represented as an array or a table approximate, gradient-descent, solutions: the state-action spaces involve continuous domains or complex representations (e.g., an image) Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. approximate solutions motivation • continuous, infinite state/action spaces: it requires generalization from previously experienced states to ones that have never been seen. • online function approximation o it takes earlier examples from previous interactions between the agent and the environment (bootstrap) o it learns by bootstrapping from current estimates of the value function o generalization from selected examples as in supervised learning (by comparing current function outputs against expectations) Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. approximate solutions introduction learning problem (the usual one) • to learn a policy 𝜋 based on a state-value function 𝜈𝜋 𝑠 novelty • the unknown state-value function 𝜈𝜋 𝑠 is approximated by a parameterized function Ƹ𝜈𝑤 𝑠 with a parameter vector 𝑤 ∈ ℝ 𝑛 • Ƹ𝜈𝑤 𝑠 can be a deep artificial neural network, with 𝑤 being the vector of connection weights (deep reinforcement learning) Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. assumption: there exist 𝜈𝜋 𝑠𝑡 that computes the true value from a state 𝑠𝑡 at time step 𝑡 under de policy 𝜋, ∀𝑡 = 1, 2, 3, … goal: to find 𝑤 such that Ƹ𝜈𝑤 𝑠𝑡 best approximates 𝜈𝜋 𝑠𝑡 gradient-descent method: 𝑤𝑡+1 = 𝑤𝑡 − 1 2 𝛼𝛻 𝜈𝜋 𝑠𝑡 − Ƹ𝜈𝑤𝑡 𝑠𝑡 2 𝑤𝑡+1 = 𝑤𝑡 + 𝛼 𝜈𝜋 𝑠𝑡 − Ƹ𝜈𝑤𝑡 𝑠𝑡 𝛻 Ƹ𝜈𝑤𝑡 𝑠𝑡 Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. the change is proportional to the negative gradient of the example's squared error learning rate step-size param. 𝜕 Ƹ𝜈𝑤𝑡 𝑠𝑡 𝜕𝑤𝑡,1 , 𝜕 Ƹ𝜈𝑤𝑡 𝑠𝑡 𝜕𝑤𝑡,2 , … , 𝜕 Ƹ𝜈𝑤𝑡 𝑠𝑡 𝜕𝑤𝑡,𝑛 under certain conditions, the method converges to a local minimum approximate solutions state value function approximation what if 𝜈𝜋 𝑠𝑡 is unknown? the true value does not exist alternative: a rough/noisy approximation 𝜈𝑡 of 𝜈𝜋 𝑠𝑡 ; for example: 𝜈𝑡 = 𝑟𝑡+1 + 𝛾 Ƹ𝜈𝑤𝑡 𝑠𝑡+1 𝑠𝑡+1, 𝑟𝑡+1 ← 𝑒𝑛𝑣 𝑠𝑡, 𝜋 𝑠𝑡 gradient-descent method: 𝑤𝑡+1 = 𝑤𝑡 + 𝛼 𝜈𝑡 − Ƹ𝜈𝑤𝑡 𝑠𝑡 𝛻 Ƹ𝜈𝑤𝑡 𝑠𝑡 Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. approximate solutions state value function approximation approximate solutions deep reinforcement learning neural networks are function approximators neural networks are particularly useful in RL when the state space or action space are too large to be completely known Source: Chris V. Nicholson. A Beginner's Guide to Deep Reinforcement Learning. Pathmind Inc., 2023. neural networkstate action policy 𝑎 = ො𝜋𝑤 𝑠 neural networkstate value value function 𝜈 = Ƹ𝜈𝑤 𝑠 source: flickr.com CNN run? jump? stop? neural network(state, action) value Q-value function 𝜈 = Ƹ𝜈𝑤 𝑠, 𝑎 approximate solutions deep reinforcement learning previous methods have policy implementations that directly use the value function of states or state-action pairs in choosing an action (e.g., 𝜀-greedy) actor-critic methods have a separate memory structure to explicitly represent the policy independent of the value function Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. approximate solutions actor-critic methods actor-critic methods introduction Source: Continuous Action Space Actor Critic Tutorial. https://youtu.be/kWHSH2HgbNQ policy net (actor) observations, sensor state value net (critic) s s 𝜋 𝑎 𝑠 𝑉 𝑠 policy net (actor) s 0.17 0.04 0.72 0.07 𝜋 𝑎 𝑠 probability distribution over the action space Source: Continuous Action Space Actor Critic Tutorial. https://youtu.be/kWHSH2HgbNQ actor-critic methods discrete policy policy net (actor) s Source: Continuous Action Space Actor Critic Tutorial. https://youtu.be/kWHSH2HgbNQ (μ, σ) actor-critic methods continuous policy actor (policy): select actions critic (value): criticizes the actions made by the actor Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. actor-critic methods architecture Source: Fuji, Taiki & Ito, Kiyoto & Matsumoto, Kohsei & Yano, Kazuo. (2018). Deep Multi-Agent Reinforcement Learning using DNN-Weight Evolution to Optimize Supply Chain Performance. 10.24251/HICSS.2018.157. Source: SUTTON, Richard S.; BARTO, Andrew G. Reinforcement learning: An introduction. MIT press, 2018. actor-critic methods architecture • the critique error (a scalar) drives learning in both actor and critic • after each action 𝑎𝑡, the critic evaluates the new state 𝑠𝑡 to determine whether things have gone better or worse than expected. • critique error: 𝛿𝑡 = 𝑟𝑡+1 + 𝛾𝑣𝑡 𝑠𝑡+1 − 𝑣 𝑠𝑡 • where… 𝑣𝑡 is the critique value at time 𝑡 𝑣 is an expected state value Deep Deterministic Policy Gradient (DDPG) an actor-critic method learning overview • 𝑄-function (critic) and policy 𝜇 (actor) are deep neural network models • critic and actor models are learned iteratively and alternately • learning involves off-policy data (a policy different from the target) • the actor model is a deterministic policy optimized using the critic's evaluation scope overview • DDPG is an off-policy algorithm: o the behavior policy (used to collect data) is different from the target policy o the behavior model (policy) changes continuously • DDPG can only be used for environments with continuous action spaces • DDPG uses an exploratory policy by adding noise to actions chosen by the actor Source: Deep Deterministic Policy Gradient. Copyright 2018, OpenAI. Link. Deep Deterministic Policy Gradient (DDPG) two key tricks experience replay buffer • the replay buffer (ℛ) is the set of previous experiences 𝑠𝑡, 𝑎𝑡, 𝑟𝑡+1, 𝑠𝑡+1 • ℛ should be as large and diverse as possible (useful for learning) • a random minibatch ℬ𝑡 of 𝑁 transitions is sampled from ℛ at each learning step 𝑡 • ℬ𝑡 is used to update both the 𝑄-function and the policy networks target networks (intended to make the loss minimization stable) • lagging versions of the 𝑄-function and the policy (current) networks • target networks do not change when current networks are being updated • target networks provide more stable expectations for current networks updates • target network are updated once after current network update by Polyak average (+) Source: Deep Deterministic Policy Gradient. Copyright 2018, OpenAI. Link. Deep Deterministic Policy Gradient (DDPG) experience replay buffer Source: Deep Deterministic Policy Gradient. Copyright 2018, OpenAI. Link. Replay Buffer 𝑠𝑡, 𝑎𝑡, 𝑟𝑡+1, 𝑠𝑡+1 ℬ𝑡 = 𝑠𝑖, 𝑎𝑖, 𝑟𝑖+1, 𝑠𝑖+1 𝑖=1 𝑁 Deep Deterministic Policy Gradient (DDPG) target networks 𝑠𝑖+1 𝜇′ (target) 𝑄′ (target) 𝜇′ 𝑠𝑖+1 *𝛾 + 𝑟𝑖+1 Bellman equation 𝑄′ 𝑠𝑖+1, 𝜇′ 𝑠𝑖+1 𝑄 𝜇 𝑎𝑖 𝑠𝑖 𝐿 𝑄 𝜃𝑄 𝐿 𝑄 𝜃𝑄, 𝜃𝜇 𝑄 𝑠𝑖, 𝑎𝑖 𝑄 𝑠𝑖, 𝜇 𝑠𝑖 expected 𝑄-value 𝜇 𝑠𝑖 Agent ℬ𝑡 = 𝑠𝑖, 𝑎𝑖, 𝑟𝑖+1, 𝑠𝑖+1 𝑖=1 𝑁 ... Deep Deterministic Policy Gradient algorithm Source: LILLICRAP, Timothy P., et al. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. bootstrapping overview • bootstrap method: statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples (sets). • a data sample is constructed by drawing individual observations from a large data sample with replacement • an observation can thus be included in a given sample more than once • bootstrapping usually perform better than nonbootstrapping methods • bootstrapping methods are of great interest to RL Source: Jason Brownlee. Gentle Introduction to the Bootstrap Method. machinelearningmastery.com, 2019 (link). bootstrapping algorithm 1. Choose a number of bootstrap samples (iterations) to perform 2. Choose a bootstrap sample size 3. For each bootstrap sample a. Draw a sample with replacement with the chosen size b. Calculate the statistic on the sample 4. Calculate the mean of the sample statistics Source: Jason Brownlee. Gentle Introduction to the Bootstrap Method. machinelearningmastery.com, 2019 (link). summary • RL goal: to learn how to map each state to the most promising action • exploitation/exploration dilemma: exploit known actions or explore new actions • RL elements: a policy 𝜋, a reward signal, a value function, an environment model • tabular solutions: suitable for finite and small spaces of state and action • gradient-descent solutions: suitable for continuous and infinite spaces of st & act • off-policy learning: learning does not depend on the agent policy 𝜋 • on-policy learning: learning does depend on the agent policy 𝜋 • Deep Deterministic Policy Gradient (DDPG) is an off-policy actor-critic method • actor-critic method: policy and value functions are fully independent","libVersion":"0.3.2","langs":""}