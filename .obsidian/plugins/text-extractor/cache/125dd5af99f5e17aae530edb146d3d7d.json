{"path":"_aula_virtual/SJK003/05-decisionTrees.pdf","text":"Department of Computer Languages and Systems Decision Trees Decision trees: introduction Objectives of a decision tree • inference, learning, tree generation: – to achieve the best possible split, at leaf nodes, of the (training) samples of different classes – to build small tress • use, exploitation: – to predict the class (classification) – to predict the value of a target variable (regression) Decision trees: introduction Strategy to build a decision tree from a training set X: • progressive splitting of the training set into smaller and smaller subsets • recursive splitting of X based on the value of attributes • creating a labelled leaf, when all instances of a subset belong to the same class • pruning: a branch with a mixed subset of distinct classes is labelled with the majority class (the resulting subtree is pruned) Decision trees: an example class holes curved strokes A yes no B yes yes C no yes You want to define a system to recognise 3 capital letters (e.g., Arial) based on the presence or absence of 2 features: holes and curved strokes. comments: - 3 classes: A, B, C - 1 instance per class - 2 discrete features with holes? C yes no with curved strokes? B A noyes {A, B, C} {A, B} {C} {B} {A} Decision trees: introduction • tree structure with the following types of nodes: – root node is the top-most decision node and represents the entire set X that further gets divided according to the value of attributes – internal decision node fm(x) based on an attribute m; it produces as many branches as allowed by the splitting criterion adopted – leaf node (or terminal node), with output value (class, prediction); it groups instances, for example, of a single class • decision: for each new input instance x, fm(x) guides the analysis of x from the root to a leaf that provides the result (class) Decision trees: types of nodes Decision trees: learning (induction) of a decision tree w1 w2 x1 x2 x1>w1 x2>w2 Data set 2 classes: ,  2 dimensions: x1, x2 Decision tree rectangular nodes → decision nodes circular nodes → leaf nodes 1st split: x1>w1 2nd split: x2>w2 (orthogonal to 1st) Decision trees: classification x1>w1 x2>w2 Given a point p = (w1+, w2−),  > 0, the induced tree classifies p as belonging to class  (see coloured path): it goes through the tree structure from the root node until reaching a leaf w1 w2 x1 x2 p Decision trees: interpretability • decision tree  set of rules IF-THEN-ELSE • for example, the previous tree is equivalent to: IF x1 > w1 THEN IF x2 > w2 THEN class =  ELSE class =  ELSE class =  Univariate trees: introduction the fm(x) of each internal node is defined in terms of a single dimension (attribute) xi (see previous example) if xi is discrete, for example: xi is an attribute colour … if xi is numeric (ordered), for example: xi is an attribute weight ... colour is red blue green weight < 60 yes no binary partition of space L = {people | weight < 60} R = {people | weight  60} Univariate trees: tree learning algorithm • tree induction  tree learning • algorithms of local search (greedy): – begin: root node with the whole set of instances – heuristic (recursive): to divide the set that arrives at a node m using the most discriminating attribute, that is, the one that generates more class homogeneous (i.e., pure) disjoint subsets • if the attribute is numeric, binary division; if the attribute is discrete, as many “children” (new nodes) as possible values – end: until get (labelled) pure nodes Univariate trees: impurity of a node objective: evaluate the degree of homogeneity of the set of instances X that reaches a node m example: let C = {1=A, 2=B} be the set of classes • case 1: X1 = {(x1, A), (x2, B), (x3, B), (x4, A), (x5, A), (x6, B)} • case 2: X2 = {(x1, A), (x2, B), (x3, A), (x4, A), (x5, A), (x6, A)} observations: • X1 has 3 instances of each class (heterogeneous) • X2 has 5 instances of A and 1 of B (almost homogeneous) • intuition: X1 is more impure than X2 or X2 is more pure than X1 Univariate trees: an impurity measure impurity measure of a node m; given: • Nm, the number of instances that reach node m • Ni,m, the number of instances in node m that belong to class i • Pi,m = Ni,m/Nm, the probability of class i in node m • Im, entropy, a measure of impurity (c classes): 𝑰𝒎 = − ෍ 𝒊=𝟏 𝒄 𝑷𝒊,𝒎 𝐥𝐨𝐠𝟐 𝑷𝒊,𝒎 (Note: 0 log 0 = 0) Univariate trees: another impurity measure impurity measure of a node m; given: • Nm, the number of instances that reach node m • Ni,m, the number of instances in node m that belong to class i • Pi,m = Ni,m/Nm, the probability of class i in node m • Gm, Gini index, a measure of impurity (c classes): 𝑮𝒎 = 𝟏 − ෍ 𝒊=𝟏 𝒄 𝑷𝒊,𝒎 𝟐 Univariate trees: how to apply the measure (entropy) … it continues from previous example: suppose we can choose whether node m is reached by X1 or X2 • in both cases Nm = 6 • in X1: N1,m = 3, N2,m = 3  p1,m = 0.50, p2,m = 0.50 • in X2: N1,m = 5, N2,m = 1  p1,m = 0.83, p2,m = 0.17 evaluation of entropy (impurity measure): • in X1, Im = − (0.5 log20.5 + 0.5 log20.5) = 1 (+ impure) • in X2, Im = − (0.83 log20.83 + 0.17 log20.17) = 0.66 (+ pure) it would be better X2 to reach m because its entropy is lower (i.e. higher purity) Univariate trees: how to apply the measure (Gini index) … it continues from previous example: suppose we can choose whether node m is reached by X1 or X2 • in both cases Nm = 6 • in X1: N1,m = 3, N2,m = 3  p1,m = 0.50, p2,m = 0.50 • in X2: N1,m = 5, N2,m = 1  p1,m = 0.83, p2,m = 0.17 evaluation of Gini index (impurity measure): • in X1, Gm = 1 − (0.52 + 0.52) = 0.50 (+ impure) • in X2, Gm = 1− (0.832 + 0.172) = 0.28 (+ pure) it would be better X2 to reach m because its Gini index is lower (i.e. higher purity) Univariate trees: another example Gini index: 1 − 0 54 2 + 49 54 2 + 5 54 2 = 0.168 Entropy: − 0 54 log 0 54 − 49 54 log 49 54 − 5 54 log 5 54 = 0.134 Univariate trees: generalization to 2 classes context: impurity value at node m for 2 classes • Nm = N1,m + N2,m  p1,m + p2,m = 1  p2,m = 1 − p1,m • Im, entropy for c = 2 classes (let it be p  p1,m): )1(log)1()(log 22 ppppI m −−−−= Univariate tree: pure node (impurity 0) a node m is pure if all instances reaching m are of the same class, that is, if N1,m = 0 or N2,m = 0; suppose N1,m = 0, then N1,m = 0  p1,m= 0, p2,m= 1  Im = 0, Gm = 0 a node m is pure  Im = 0 or Gm = 0 (impurity 0) a pure node becomes a leaf that is assigned the class label of its instances Univariate trees: impurity of a split • K classes or categories • x is the attribute chosen to split an impure node 1mN 2mNmnN mN fm(x) … instances of the 𝑁𝑚,𝑗 belong to i such that is the probability of i in the branch j Impurity of a split: 𝐼𝑚 ∗ = − ෍ 𝑗=1 𝑛 𝑁𝑚,𝑗 𝑁𝑚 ෍ 𝑖=1 𝐾 𝑝𝑚,𝑗 𝑖 log2 𝑝𝑚,𝑗 𝑖 ෍ 𝑗=1 𝑛 𝑁𝑚,𝑗 = 𝑁𝑚 ෍ 𝑖=1 𝐾 𝑁𝑚,𝑗 𝑖 = 𝑁𝑚,𝑗 𝑝𝑚,𝑗 𝑖 = 𝑁𝑚,𝑗 𝑖 𝑁𝑚,𝑗 𝑁𝑚,𝑗 𝑖 Univariate trees: choice of the attribute; local optimization Local optimization: in each node (not pure) in which it is intended to continue splitting the training set, the most discriminating attribute will be chosen, that is, the one that produces a split that is: - the least impure  - the most homogeneous  - with the greatest class separation Univariate trees: general tree induction algorithm generate_tree (Ttra,): m if entropy Im(Ttra) ≤ : m  class label of the most represented class in Ttra else a = the_most_discriminating_atribute(Ttra) m  a for each branch ai of a: trai = {x  Ttra | x satisfies the condition of ai} h = generate_tree(trai) add_child(m,h) return m Univariate trees: function “the most discriminating attribute” the_most_discriminating_attribute(tra): a mín_entr  MAX for each attribute xi: if xi is discrete: divide tra into tra1, …, traN based on the values of xi e  I* m(tra1, tra2, …, traN) if e < mín_entr: mín_entr  e; a  xi else // xi is numeric for each possible binary division (tra1, tra2): e  I*m(tra1, tra2) if e < mín_entr: mín_entr  e; a  xi return a Pruning: introduction Context • it is common to find high overlapping or outliers • to generate a “pure” tree trying to learn from: – outliers – small sample size (small number of instances) – overlapping reduces the generalization of the model (overfitting) Alternative • pre-pruning: do not split nodes with few instances while growing the tree • post-pruning: after building the tree to its depth, remove unnecessary subtrees Pruning: pre-pruning Idea • let m be a node and Nm the number of samples (instances) • stop splitting the training set if m is pure (“natural end”) or if Nm <  (pruning) • label m with the majority class among the Nm instances Pruning: post-pruning Idea: to generate 3 independent (and disjoint) sets for training, pruning, and test • training: generate the complete tree, with all its pure leaves, using the training set • pruning (it is part of the learning stage): – replace each subtree by a leaf node with the label of the majority instances covered by that subtree – prune the subtree if the “surrogate” leaf node does not worsen its performance with the pruning set • evaluation: obtain a performance measure of the final (pruned) tree by classifying the test set Pruning: summary Experiments show that: • post-pruning produces more accurate trees (both classification and regression) than pre-pruning • pre-pruning is much faster than post-pruning Decision trees: advantages • Comprehensive: it is good for interpreting data in a highly visual way • Simplicity: it is one of the simplest algorithms since it has no complex formulas or data structures Decision trees: disadvantages • It is computationally expensive. At each node, the candidate split must be sorted before determining the best • It is sometimes unstable as small variations in data might lead to the formation of a new tree","libVersion":"0.3.2","langs":""}