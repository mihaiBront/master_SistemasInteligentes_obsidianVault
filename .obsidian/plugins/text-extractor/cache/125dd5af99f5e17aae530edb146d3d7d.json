{"path":"_aula_virtual/SJK003/05-decisionTrees.pdf","text":"Department of Computer Languages and Systems Decision Trees Decision trees: introduction Objectives of a decision tree â€¢ inference, learning, tree generation: â€“ to achieve the best possible split, at leaf nodes, of the (training) samples of different classes â€“ to build small tress â€¢ use, exploitation: â€“ to predict the class (classification) â€“ to predict the value of a target variable (regression) Decision trees: introduction Strategy to build a decision tree from a training set X: â€¢ progressive splitting of the training set into smaller and smaller subsets â€¢ recursive splitting of X based on the value of attributes â€¢ creating a labelled leaf, when all instances of a subset belong to the same class â€¢ pruning: a branch with a mixed subset of distinct classes is labelled with the majority class (the resulting subtree is pruned) Decision trees: an example class holes curved strokes A yes no B yes yes C no yes You want to define a system to recognise 3 capital letters (e.g., Arial) based on the presence or absence of 2 features: holes and curved strokes. comments: - 3 classes: A, B, C - 1 instance per class - 2 discrete features with holes? C yes no with curved strokes? B A noyes {A, B, C} {A, B} {C} {B} {A} Decision trees: introduction â€¢ tree structure with the following types of nodes: â€“ root node is the top-most decision node and represents the entire set X that further gets divided according to the value of attributes â€“ internal decision node fm(x) based on an attribute m; it produces as many branches as allowed by the splitting criterion adopted â€“ leaf node (or terminal node), with output value (class, prediction); it groups instances, for example, of a single class â€¢ decision: for each new input instance x, fm(x) guides the analysis of x from the root to a leaf that provides the result (class) Decision trees: types of nodes Decision trees: learning (induction) of a decision tree w1 w2 x1 x2 x1>w1 x2>w2 Data set 2 classes: ï‚™, ï‚£ 2 dimensions: x1, x2 Decision tree rectangular nodes â†’ decision nodes circular nodes â†’ leaf nodes 1st split: x1>w1 2nd split: x2>w2 (orthogonal to 1st) Decision trees: classification x1>w1 x2>w2 Given a point p = (w1+ï¤, w2âˆ’ï¤), ï¤ > 0, the induced tree classifies p as belonging to class ï‚™ (see coloured path): it goes through the tree structure from the root node until reaching a leaf w1 w2 x1 x2 p Decision trees: interpretability â€¢ decision tree ï‚º set of rules IF-THEN-ELSE â€¢ for example, the previous tree is equivalent to: IF x1 > w1 THEN IF x2 > w2 THEN class = ï‚£ ELSE class = ï‚™ ELSE class = ï‚™ Univariate trees: introduction the fm(x) of each internal node is defined in terms of a single dimension (attribute) xi (see previous example) if xi is discrete, for example: xi is an attribute colour â€¦ if xi is numeric (ordered), for example: xi is an attribute weight ... colour is red blue green weight < 60 yes no binary partition of space L = {people | weight < 60} R = {people | weight ï‚³ 60} Univariate trees: tree learning algorithm â€¢ tree induction ï‚º tree learning â€¢ algorithms of local search (greedy): â€“ begin: root node with the whole set of instances â€“ heuristic (recursive): to divide the set that arrives at a node m using the most discriminating attribute, that is, the one that generates more class homogeneous (i.e., pure) disjoint subsets â€¢ if the attribute is numeric, binary division; if the attribute is discrete, as many â€œchildrenâ€ (new nodes) as possible values â€“ end: until get (labelled) pure nodes Univariate trees: impurity of a node objective: evaluate the degree of homogeneity of the set of instances X that reaches a node m example: let C = {ï·1=A, ï·2=B} be the set of classes â€¢ case 1: X1 = {(x1, A), (x2, B), (x3, B), (x4, A), (x5, A), (x6, B)} â€¢ case 2: X2 = {(x1, A), (x2, B), (x3, A), (x4, A), (x5, A), (x6, A)} observations: â€¢ X1 has 3 instances of each class (heterogeneous) â€¢ X2 has 5 instances of A and 1 of B (almost homogeneous) â€¢ intuition: X1 is more impure than X2 or X2 is more pure than X1 Univariate trees: an impurity measure impurity measure of a node m; given: â€¢ Nm, the number of instances that reach node m â€¢ Ni,m, the number of instances in node m that belong to class ï·i â€¢ Pi,m = Ni,m/Nm, the probability of class ï·i in node m â€¢ Im, entropy, a measure of impurity (c classes): ð‘°ð’Ž = âˆ’ à· ð’Š=ðŸ ð’„ ð‘·ð’Š,ð’Ž ð¥ð¨ð ðŸ ð‘·ð’Š,ð’Ž (Note: 0 log 0 = 0) Univariate trees: another impurity measure impurity measure of a node m; given: â€¢ Nm, the number of instances that reach node m â€¢ Ni,m, the number of instances in node m that belong to class ï·i â€¢ Pi,m = Ni,m/Nm, the probability of class ï·i in node m â€¢ Gm, Gini index, a measure of impurity (c classes): ð‘®ð’Ž = ðŸ âˆ’ à· ð’Š=ðŸ ð’„ ð‘·ð’Š,ð’Ž ðŸ Univariate trees: how to apply the measure (entropy) â€¦ it continues from previous example: suppose we can choose whether node m is reached by X1 or X2 â€¢ in both cases Nm = 6 â€¢ in X1: N1,m = 3, N2,m = 3 ïƒž p1,m = 0.50, p2,m = 0.50 â€¢ in X2: N1,m = 5, N2,m = 1 ïƒž p1,m = 0.83, p2,m = 0.17 evaluation of entropy (impurity measure): â€¢ in X1, Im = âˆ’ (0.5 log20.5 + 0.5 log20.5) = 1 (+ impure) â€¢ in X2, Im = âˆ’ (0.83 log20.83 + 0.17 log20.17) = 0.66 (+ pure) it would be better X2 to reach m because its entropy is lower (i.e. higher purity) Univariate trees: how to apply the measure (Gini index) â€¦ it continues from previous example: suppose we can choose whether node m is reached by X1 or X2 â€¢ in both cases Nm = 6 â€¢ in X1: N1,m = 3, N2,m = 3 ïƒž p1,m = 0.50, p2,m = 0.50 â€¢ in X2: N1,m = 5, N2,m = 1 ïƒž p1,m = 0.83, p2,m = 0.17 evaluation of Gini index (impurity measure): â€¢ in X1, Gm = 1 âˆ’ (0.52 + 0.52) = 0.50 (+ impure) â€¢ in X2, Gm = 1âˆ’ (0.832 + 0.172) = 0.28 (+ pure) it would be better X2 to reach m because its Gini index is lower (i.e. higher purity) Univariate trees: another example Gini index: 1 âˆ’ 0 54 2 + 49 54 2 + 5 54 2 = 0.168 Entropy: âˆ’ 0 54 log 0 54 âˆ’ 49 54 log 49 54 âˆ’ 5 54 log 5 54 = 0.134 Univariate trees: generalization to 2 classes context: impurity value at node m for 2 classes â€¢ Nm = N1,m + N2,m ïƒž p1,m + p2,m = 1 ïƒž p2,m = 1 âˆ’ p1,m â€¢ Im, entropy for c = 2 classes (let it be p ï‚º p1,m): )1(log)1()(log 22 ppppI m âˆ’âˆ’âˆ’âˆ’= Univariate tree: pure node (impurity 0) a node m is pure if all instances reaching m are of the same class, that is, if N1,m = 0 or N2,m = 0; suppose N1,m = 0, then N1,m = 0 ïƒž p1,m= 0, p2,m= 1 ïƒž Im = 0, Gm = 0 a node m is pure ïƒ› Im = 0 or Gm = 0 (impurity 0) a pure node becomes a leaf that is assigned the class label of its instances Univariate trees: impurity of a split â€¢ K classes or categories â€¢ x is the attribute chosen to split an impure node 1mN 2mNmnN mN fm(x) â€¦ instances of the ð‘ð‘š,ð‘— belong to ï·i such that is the probability of ï·i in the branch j Impurity of a split: ð¼ð‘š âˆ— = âˆ’ à· ð‘—=1 ð‘› ð‘ð‘š,ð‘— ð‘ð‘š à· ð‘–=1 ð¾ ð‘ð‘š,ð‘— ð‘– log2 ð‘ð‘š,ð‘— ð‘– à· ð‘—=1 ð‘› ð‘ð‘š,ð‘— = ð‘ð‘š à· ð‘–=1 ð¾ ð‘ð‘š,ð‘— ð‘– = ð‘ð‘š,ð‘— ð‘ð‘š,ð‘— ð‘– = ð‘ð‘š,ð‘— ð‘– ð‘ð‘š,ð‘— ð‘ð‘š,ð‘— ð‘– Univariate trees: choice of the attribute; local optimization Local optimization: in each node (not pure) in which it is intended to continue splitting the training set, the most discriminating attribute will be chosen, that is, the one that produces a split that is: - the least impure ï‚º - the most homogeneous ï‚º - with the greatest class separation Univariate trees: general tree induction algorithm generate_tree (Ttra,ï¡): m if entropy Im(Ttra) â‰¤ ï¡: m ï‚¬ class label of the most represented class in Ttra else a = the_most_discriminating_atribute(Ttra) m ï‚¬ a for each branch ai of a: trai = {x ïƒŽ Ttra | x satisfies the condition of ai} h = generate_tree(trai) add_child(m,h) return m Univariate trees: function â€œthe most discriminating attributeâ€ the_most_discriminating_attribute(tra): a mÃ­n_entr ï‚¬ MAX for each attribute xi: if xi is discrete: divide tra into tra1, â€¦, traN based on the values of xi e ï‚¬ I* m(tra1, tra2, â€¦, traN) if e < mÃ­n_entr: mÃ­n_entr ï‚¬ e; a ï‚¬ xi else // xi is numeric for each possible binary division (tra1, tra2): e ï‚¬ I*m(tra1, tra2) if e < mÃ­n_entr: mÃ­n_entr ï‚¬ e; a ï‚¬ xi return a Pruning: introduction Context â€¢ it is common to find high overlapping or outliers â€¢ to generate a â€œpureâ€ tree trying to learn from: â€“ outliers â€“ small sample size (small number of instances) â€“ overlapping reduces the generalization of the model (overfitting) Alternative â€¢ pre-pruning: do not split nodes with few instances while growing the tree â€¢ post-pruning: after building the tree to its depth, remove unnecessary subtrees Pruning: pre-pruning Idea â€¢ let m be a node and Nm the number of samples (instances) â€¢ stop splitting the training set if m is pure (â€œnatural endâ€) or if Nm < ï± (pruning) â€¢ label m with the majority class among the Nm instances Pruning: post-pruning Idea: to generate 3 independent (and disjoint) sets for training, pruning, and test â€¢ training: generate the complete tree, with all its pure leaves, using the training set â€¢ pruning (it is part of the learning stage): â€“ replace each subtree by a leaf node with the label of the majority instances covered by that subtree â€“ prune the subtree if the â€œsurrogateâ€ leaf node does not worsen its performance with the pruning set â€¢ evaluation: obtain a performance measure of the final (pruned) tree by classifying the test set Pruning: summary Experiments show that: â€¢ post-pruning produces more accurate trees (both classification and regression) than pre-pruning â€¢ pre-pruning is much faster than post-pruning Decision trees: advantages â€¢ Comprehensive: it is good for interpreting data in a highly visual way â€¢ Simplicity: it is one of the simplest algorithms since it has no complex formulas or data structures Decision trees: disadvantages â€¢ It is computationally expensive. At each node, the candidate split must be sorted before determining the best â€¢ It is sometimes unstable as small variations in data might lead to the formation of a new tree","libVersion":"0.3.2","langs":""}