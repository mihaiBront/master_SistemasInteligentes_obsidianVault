{"path":"_aula_virtual/SJK002/SJK002-U11-03-SequenceAndOpticFlow.pdf","text":"Deep Learning for various CV tasks Computer Vision (SJK02) Universitat Jaume I Part A: Classification Segmentation Object detection (Image-based) biometrics Part B: Sequence processing Optical flow Action Recognition Self-supervised learning Transformers Sequence processing: RNNs, LSTMs 3 Recurrent Neural NetworksUnrollingFlexible input/output sizes The Unreasonable Effectiveness of Recurrent Neural Networks (http://karpathy.github.io/2015/05/21/rnn-effectiveness) Image classification Image captioning Sentiment analysis Machine translation Frame-wise video classificationMatch problems to models 7https://towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22 Backpropagation through time (BTT)Exploding/vanishing gradients https://towardsdatascience.com/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22 Dealing with this problem • Skip connections (like ResNets) • Remove 1-length connections • Leaky recurrent units (regulate how much passed by α) • Gated RN (learnable α) • LSTMs Long short-term memory (LSTM) Understanding LSTM Networks Unrolling Cell state can go unchanged Gates regulate how much it is changed Forget gate How much, between 0 (nothing) and 1 (all), each element in Ct-1 is kept Input gate New information and how much of it to add to Ct-1 Update state Update Ct-1 to get CtUpdate ht-1 to get ht, a filtered version of the cell state Ct Update outputBackpropagation through time (BTT) A Beginner’s Guide on Recurrent Neural Networks with PyTorch Further possibilities Bidirectional LSTMsStacked LSTMsConvLSTMs Attention Transformers LSTMs with PyTorch lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers) Long Short-Term Memory: From Zero to Hero with PyTorch ​What about the sequence length?​ n_layers = 1 (non-stacked LSTM) Example: egocentric gesture recognition Head and Eye Egocentric Gesture Recognition for Human-Robot Interaction Using Eyewear Cameras (IEEE Rob. & Aut. Letters, 2022) Results (video) Optical flow: FlowNet, SpyNet, LikeNet 25 FlowNet (ICCV2015) Can optic flow estimation be learned? FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015) Supervised learning: what about the labelled data? FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (CVPR 2017) FlyingChairs FlyingThings3D Sintel 28FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015) Correlation layer Like convolution between feature maps (no learnable weights) What's the key difference between FlowNetSimple and FlowNetCorr? Refinement 30 Upsampling Findings It is possible to learn to estimate optic flow Training data need not be realistic FlowNetSimple vs FlowNetCorr? --> Depends on dataset FlowNetCorr: • slighlty overfits the training data • has some problems with large displacements Why do you think authors relate the limitation of FlowNetCorr with large displacements with the correlation layer? Some results 32 Find where FlowNetS is better than EpicFlow (CVPR 2015) Find where FlowNetC is better than FlowNetS Loss 33 End-point-error (EPE) Ground truth Estimated FlowNet 2.0 (CVPR 2017) 34 FlowNet 2.0 vs FlowNet (ICCV 2015) • schedule of presenting data • stacked architecture with warping • subnetwork specializing on small motions Data presentation is important 35 • Training only on Things3D (more realistic) --> worse results • Best results: training on FlyingChairs first, then on Things3D What would be a \"take-home lesson\" for other problems? A form of curriculum learning? Does iteration and warping help? 36 Stacking may help; stacking+warping always help Final net 37 Some results on Sintel 38 Some results on real data 39UCF101Middlebury Conclusions 40 • FlowNet 2.0 is marginally slower than FlowNet • Estimation error reduced by more than 50% • Performs on par with state-of-the-art methods • Runs at interactive frame rates • Faster variants run up to 140fps SPyNet (CVPR 2017) 41Optical Flow Estimation using a Spatial Pyramid Network (CVPR 2017) • Each Gi trained independently (previous Gi already trained) • Since each Gi assumes small motion (simpler task), less overall parameters are required LikeNet (BMVC 2018) 42 LikeNet: A Siamese Motion Estimation Network Trained in an Unsupervised Way (BMVC 2018) • Motion as dense classification problem • Unsupervised (no ground truth required!) • Siamese architecture Loss function? 44 We don’t have ground truth to compare with! D(·) could be a function of image values, but with features works better. Any idea why? Motion should be quantised! 45 How many branches would be required for 50 values in tx and ty each? Multiscale approach: # branches: 121, 169, 49, 9, 9 More branches at lowest resolution Comparison 46 • performs better than the other unsupervised methods • generalizes well to unknown datasets (without finetunning)","libVersion":"0.3.2","langs":""}