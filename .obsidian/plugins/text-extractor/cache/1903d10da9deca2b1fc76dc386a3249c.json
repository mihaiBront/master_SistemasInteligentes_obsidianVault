{"path":"_aula_virtual/SJK004/Unit 1_ Modelling an Intelligent agent_1.pdf","text":"Unit 1: Modelling an Intelligent agent. Why, how, and for what is it interesting? What is an agent? An agent is autonomous : in some way, this sw is able to self-control its own behaviour. This definition, although helps, is too wide open (a thermostat is also an agent!). An intelligent agent is a sw able to do autonomous flexible actions in an environment Flexible implies this sw to show reactive, proactive and social behaviour. What means reactive? To hold a continuous interaction with the environment, to answer back to the detected changes in a suitable way. What means pro-active? To pursue the sw designed goal, taking the initiative. To show an opportunistic behaviour. What has to prevalence, a reactive or proactive behaviour? What means to be social? To be able to interact with other agents (or even humans) by using some kind of formal agent communication language. Interactions are driven by the pro-activity of each involved agent, its design goals, and overall behavior. What is an environment? It is the (physical or virtual) medium common to all agents. Agents get events from the environment and they also act on the environment. Environment classification: ‚óè Accessible vs not accessible. ‚óè Determinist vs not determinist. ‚óè Episodic vs not episodic. ‚óè Static vs dynamic. ‚óè Discrete vs continuous. How to design Intelligent agents? How Intelligence can be approached? 1. Intelligence is a consequence of a statistical model feed with an enormous quantity of pre-processed data (machine learning ). 2. Intelligence emerges from the massive interaction of simple rules. It deals with intelligence as a kind of complex system (small modifications in the values of some variables provide very different behaviors). 3. Intelligence can be modeled by symbol manipulation procedures and mathematical logic. Intentional systems , from which BDI logic (and many extensions) is the most frequently used. 4. Intelligence is a hybrid of the previous models. Machine learning Machine learning is a discipline aims to design, understand and apply computer programs that learn from experience (i.e. data) for the purpose of modelling, prediction or control. One of the main domains on using Machine learning is for prediction problems Time Value What plates have these vehicles? Supervised learning is one of the disciplines of machine learning. It assumes that some problems are very hard if we try to program ‚Äúgeneral‚Äù logical solutions (as traditional computer programs do). Alternatively, we can obtain more accurate ‚Äúgeneral‚Äù solutions by giving enough examples to the program about what it can be understood as good input data and outputs pairs. Plum Maple Eucaliptus Sets of labelled examples The set of labelled examples are split into two sets: ‚óè One for training the classifier or predictor (training set ). ‚óè Another one smaller for testing the trainer classifier or predictor (validation set ). Usually the set of possible classifiers is very very big. So, we have to look for one ‚Äúenough‚Äù good. For measuring how good a classifier is the concepts of error and generalization are heavily applied. Types of machine learning ‚ùè Supervised learning. Prediction based on examples of correct behavior. ‚ùè Unsupervised learning. Goal is discover a model behind provided data. ‚ùè Semi-supervised learning. A mix of the previous types. ‚ùè Active learning. Learn to query a user to label data with the desired outputs. ‚ùè Transfer learning. How to apply what was already learned to other agents. ‚ùè Reinforcement learning . Learning to act, not just predict; goal is to maximize the overall reward obtained while executing actions on a environment. ‚ùè And many more nowadays and to come ‚Ä¶ Reinforcement learning Inspired by psychology studies with animals and infants Which are the main elements common in these two examples? Well, there is an agent (cat, infant), an action to perform by the agent (drawn from a set of available actions), an environment where the actions chosen by the agent are executed, and as consequence the environment changes, and also a reward (positive or negative) are given to the agent for each executed action. Overall view of Reinforcement learning Environment Agent Policy Value function Model State Action Reward Policy: how to choose an action. Action : performed by the agent on the environment. Value function : to evaluate how well it is the current situation for the agent. Model: an idealized representation of the real environment (should be useful for the goals of the agent). State: the set of values that characterize the current situation in the environment. Reward: the value returned by the environment to the agent in response to the consequences of the action performed. Overall view of Reinforcement learning There are three great kinds of States: Discrete and small ones Discrete and big ones Continuous x O Capacity Speed Rewards: ‚óè It uses to be a scalar value, strongly related to the goal or goals to be achieved by the agent. ‚óè It is determined by the designer/programmer of the RL system. ‚óè The notation rt denotes the reward obtained at time t. Overall view of Reinforcement learning Actions : Can be ‚óè Discrete (choose one among the N available actions ) ‚óè Continuous (an array of scalar values, e.g.) Policy: A projection from the set of states to the set of actions. This projection can be: ‚óè Deterministic. ‚óè Stochastic. The goal of RL is to learn a good policy without supervision. Value functions : Two types ‚óè V‚Ñº(s), the value that can be obtained accumulating the potential reward obtained from state s following policy ‚Ñº ‚óè Q‚Ñº(s,a), the value that can be obtained accumulating the potential reward obtained from state s, performing action a, and then, following policy ‚Ñº Overall view of Reinforcement learning Agent Environment Action At Rt+1, St+1 State St Reward Rt Time t St ‚àà S (The set of states) At ‚àà A (The set of actions) Rt ‚àà ‚Ñù (The set of real numbers) Returns Is the long term accumulation of rewards starting from time step t Another version, the discounted return, using a discount factor œí in [0,1] in continuous settings There is an inherent recursive relationship between discounted returns in consecutive time steps: Both cases can be notated with the same equation:: Markov Decision Process (MDP) Is a RL setting that satisfies the Markov property : the transition probabilities (at which state to go from state s if action a is executed) and rewards depend only on current state, and maybe also in the current action, and this fact remains unchanged regardless of the history that leads to the current state. If state representation is enough to figure out which action to take, then it is a Markov scenario (e.g. Tic-Tac-Toe) Mathematically is a tuple of 4 elements: < S, A, T, R> where: ‚óè S is the set of States, ‚óè A is the set of actions available, ‚óè T transition probabilities, T(s,a,s‚Äô) = P(s‚Äô|s,a), so that for each pair s and a, ‚óè R is the reward function from (s,a,s‚Äô), (s,a) or s to a real number. Example: Recycling robot It can be approached as a MDP: ‚óè States: [low, high] the level of the charge of the battery ‚óè Actions : Described as available actions for each state ‚óã A(low) = {search, wait, recharge} ‚óã A(high) ={search, wait} ‚óè Rewards: 1 for each can collected, 0 no can is collected, -3 if stopped due to a lack of battery And the transitions? Example: Recycling robot s a s‚Äô p(s‚Äô|s,a) r(s,a,s‚Äô) high seach high ùõÇ r search high seach low 1-ùõÇ r search high wait high 1 r wait high wait low 0 r wait low seach low ùõÉ r search low seach high 1-ùõÉ -3 low wait high 0 r wait low wait low 1 r wait low recharge high 1 0 low recharge low 0 0 such that: ‚óè ùõÇ is the probability to go from high to high on search ‚óè ùõÉ is the probability to go from low to low on search ‚óè rsearch is the expected number of cans while searching ‚óè rwait is the expected number of cans while waiting Exercise: Represent this table as a transition graph (to be solved in classroom) How to solve a MDP? Value functions They are one of the fundamental concepts in RL. There are two types: as function of state or function on (state, action) pairs. The goal is that these functions estimate how good is to be in a particular state, or take some chosen action in a particular state (by means of the expected return) Value functions can be learned from experience How to solve a MDP? Policies They represent a mapping from state to probability of executing an action ùúã(a|s) RL methods show how policy changes on experience. Value functions Two types: ‚óè State value function for policy ùúã: ‚óè State-Action value function for policy ùúã: Great!! They satisfy recursive relationships as G t does. For State value function this is: The value of a state is expressed in terms of the value of the successor states. Optimal Value functions What is the goal of a RL method? To find a policy that achieves a big reward in the long term. But, by value functions, we can define a partial ordering over potential policies. How? Easy! Policy ùõë is defined to be better or equal to policy ùõë‚Äô if vùùÖ(s) >= v ùùÖ‚Äô(s) for all possible s For any task, there is at least one policy that is >= to all other policies. Therefore: ‚óè This policy, ùõë*, is an optimal policy for the task. ‚óè All optimal policies share the same optimal state value function ‚óè All optimal policies share the same optimal action-state value function Value functions: Bellman equations","libVersion":"0.3.2","langs":""}