{"path":"_aula_virtual/SJK001/T1C9  - A motivating case study.pdf","text":"Robotic\tIntelligence\t\t a\tmotivating\tcase\tstudy\t \t \t Angel\tP.\tdel\tPobil\t\t \t Professor,\tComputer\tScience\t&\tEngineering\tDept.,\tJaume-I\tUniversity,\tSpain\t Director,\tUJI\tRobotic\tIntelligence\tLaboratory,\tSpain\t Visiting\tProfessor,\tInteraction\tScience\tDepartment,\tSungkyunkwan\tUniversity,\tKorea\t \t \t The\tChallenge\tof\tRobotic\tIntelligence\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 2\t In\t1998\tI\tpublished\ta\tpaper\tin\twhich\tI\tcoined\tthe\tterm\tRobotic\tIntelligence\t[1].\t I\t advocated\t the\t central\t role\t of\t physical\t interaction\t in\t the\t real\t world\t through\t perception\t and\t action\t for\t achieving\t human\t intelligence,\t since\t most\t of\t human\t behaviors\tare\tmediated\tby\tthis\tinteraction.\tWe\tpropose\ta\tpath\tin\tthis\tdirection\t through\tthe\tseamless\tintegration\tof\tNeuroscience\t[2]\tand\tTechnology\t[3].\t \t 1. https://doi.org/10.1007/3-540-64574-8_387 2. https://doi.org/10.1007/978-3-319-20303-4 3. https://doi.org/10.1007/978-3-642-33241-8\t Simple\tindustrial\tgrippers\t Industrial\tApplications\t Food Industry Lettuce Processing and Packing\t Segmentation Process Orientation Process Sanz,\tP.J.,\tRequena,\tA.,\tIñesta,\tJ.M.,\tdel\tPobil,\tA.P.,\t \"Grasping\tthe\tnot-so-obvious:\tVision-Based\tObject\tHandling\t for\tIndustrial\tApplications\",\tIEEE\tRobotics\tand\tAutomation\t Magazine,\t12:3,\t44-52,\t2005.\t \t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 3\t Visual\tGrasping\tsolved\tfor\t2D\tand\tsimple\t3D\tshapes\t UMass\ttorso\t2002\t Chinellato,\tE.,\tMorales,\tA.,\tFisher,\tR.B.,\tdel\tPobil,\tA.P.,\t2005,\t\"Visual\tQuality\t Measures\tfor\tCharacterizing\tPlanar\tRobot\tGrasps\",\tIEEE\tTransactions\ton\t Systems,\tMan\tand\tCybernetics\tC,\tVol.\t35,\tpp.\t30-41.\t\t \t Morales,\tA.,\tSanz,\tP.J.,\tdel\tPobil,\tA.P.,\tFagg,\tA.H.,\t2006,\t\"Vision-Based\tThree- Finger\tGrasp\tSynthesis\tConstrained\tby\tHand\tGeometry\",\tRobotics\tand\t Autonomous\tSystems,\tVol.\t54,\tpp.\t496-512.\t\t \tA.P.\tdel\tPobil\t–\tIMCOM\t2018\tLangkawi\t 4\t Back\tin\t2005…\t State\tof\tthe\tart\tin\tvisually-guided\tgrasping\tand\tmanipulation\t Back\tin\t2005…\t Empty\tthe\tbox\t2011\t\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 5\t Unscrew\tthe\tbottle\t2014\t\t Adaptive\tGrasping\twith\tthe\tUJI\tHumanoid\tTorso\t J.\tFelip,\tA.\tMorales\tet\tal.\t\t-\tGRASP\tProject\t\t-\tFP7\tICT-215821\t A.P.\tdel\tPobil\t–\tIMCOM\t2018\tLangkawi\t 6\tUnscrew\tthe\tbottle\t2014\t\t Empty\tthe\tbox\t2011\t\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 7\t Chapter 1 Introduction Figure 1.1: A brand new robot waiter called HERMES is put into service at a restaurant. HERMES must deliver one cappuccino to a table. 1 A\tbrand\tnew\trobot\twaiter\tis\tput\tinto\tservice\tat\ta\trestaurant.\tIt\tmust\tdeliver\tone\tcappuccino\tto\ta\ttable.\t\t From\tM.\tMansouri\t“A\tConstraint-Based\tApproach\tfor\tHybrid\tReasoning\tin\tRobotics”,\tPh.D.\tThesis\tÖrebro\t University\t2016.\t \t A\tnew\trobot\twaiter…\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 8\t 5.5. EXPERIMENTAL EVALUATION 69 platforms. In the second set, we study features that affect the time performance of the planner in an artiﬁcial domain. 5.5.1 Problem Set A Six experiments were carried out to validate the feasibility of our approach. Videos of the experiments are available on-line2. The ﬁrst three involve the use of a MetraLabs G5 robot base with a Kinova Jaco arm and an Asus XtionPro RGB-D camera. In order to facilitate manipulation, a cup was used instead of a dish, and forks and knifes were equipped with graspable appendices (see Figure 5.4). (d)(c)(a) (b) Figure 5.4: Snapshots of the ﬁrst experiment with a physical robot — initial situation (a); general spatial knowledge vs. observed placements (b); execution of a pick action (c); achieved placements (d). The full domain used in the experiments is provided in the Appendix A. It includes two resources with capacities Cap(Arm)= 1, Cap(camera)= 100. The ﬁrst models the fact that the robot has one arm; the second one models the fact that the camera’s ﬁeld of view cannot be simultaneously “used” for visual recognition and manipulation. Operators for picking and placing objects from/on a table or the robot’s on-board tray, and moving between tables are also modeled. Pick and place operators use one unit of camera, whereas sensing uses the full capacity (100 units), thus allowing the robot to perform several (ideally up to the arbitrary amount of 100) simultaneous manipulations, but not to perform sensing during manipulation. The Place and Pick operators are parametrized with two locations: ?location and ?robotLocation. The former is the location of the support plane of the object to be picked/placed, e.g., table1; the latter represents the location of the robot, e.g., robotLocationTable1. The fact that objects on table1 are reach- able from robotLocationTable1 is encoded as ARA+ relations in SK — e.g., robotLocationTable1 ⟨d, m⟩ table1. Tables have four robot locations, one on each side. SK also describes the admissible table layout: cups should be between 2Videos of the experiments are available at the following links: https://www.youtube.com/watch?v=hhNkKz042EM https://www.youtube.com/watch?v=k5oA9QgxjaM https://www.youtube.com/watch?v=QeC_I5wEFcY 5.5. EXPERIMENTAL EVALUATION 71 was changed only to reﬂect: (1) the size of the objects, and (2) the capacity of the Arm resource (the PR2 has two arms). The environment contained counter1 and table1. The PR2 was given the goal to place a cup on table1, which in the initial situation was on counter1. When table1 is reached, new observations reveal that the situation does not adhere to general spatial knowledge. Thanks to the higher capacity of the Arm resource, the new plan does not require to use a tray to free the arm for further manipulation, and the robot employs its other arm for re-arranging the objects on the table. Figure 5.6: Salient moments of the ﬁfth experiment. In the ﬁfth experiment, a real PR2 is employed to demonstrate the use of the spatial ﬂexibility heuristic (see Figure 5.6). Initially, the generated plan was similar to that generated in the previous experiments. As the PR2 reached the table, it observed a spatially inconsistent situation, where the fork and knife were too close to each other. The new plan called for re-placing the knife and then placing the cup on the table. The choice of moving the knife rather than moving the fork was due to h SK val. In this experiment, the reference frame of the support plane (table1) is deﬁned with respect to the guest’s chair, meaning that the robot has to place cutlery “in front of” the guest. This limits the robot’s reach, hence depending on how far the fork should be placed from the knife (which depends on the knife’s initial position), placing the fork could have been infeasible. In this case, the robot would have planned to go to the robot location on the other side of the table in order to place the fork. In all experiments, object segmentation and pose estimation were realized with the ROS [100] tabletop object detector package, while object recognition occurred based on color. Perception provides 3D bounding boxes, which we extend so as to make them axis-parallel. Then, we project the 3D bounding boxes to 2D, and generate the unary At constraints used to represent observed poses of objects. The overall time spent planning in all of the ﬁve runs was less than ﬁve seconds in total. From the knowledge engineering point of view, one of the highlights of these experiments is the amount of domain variance when the platforms are changed. The test cases show that if we change the capacity of the modeled Arm resource, we observe different behaviors. A\tnew\trobot\twaiter…\tMotivation:\tbeyond\tgrasping\tfor\tpick\tand\tplace\t The\trepertoire\tof\tmanipulation\ttasks\tneeded\tfor\ta\tservice\trobot\tgoes\t far\tbeyond\tgrasping\tfor\tpick\tand\tplace!!\t A.P.\tdel\tPobil\t–\tIMCOM\t2018\tLangkawi\t 9\t How\tto\tprepare\ta\tcup\tof\tcoffee\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 10\t A\tFeasible\tPlan\tto\tprepare\ta\tcup\tof\tcoffee?\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 11\t •  Check\tif\tplugged,\tif\tnot,\tplug\tit\tin\tand\tswitch\ton\t •  Check\twater\tin\tcontainer,\tif\tnot,\tfill\tit\t •  Take\tcoffee\tcapsule\tand\tinsert\tit\t •  Push\tdown\tthe\tlever\t •  Take\tcup\tand\tput\tit\tunder\tthe\tnozzle\t •  Press\tright\tbutton\tand\twait\t •  Remove\tcup\tand\ttake\tspoon\t •  Add\tsugar\tand/or\tmilk\tand\tstir\t Generic\tTasks\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 12\t •  Open\ta\tdoor/drawer\t(5\ttimes)\t •  Pull/push\tlever,\tdishwasher\ttray\t •  Insert\tpeg\tin\thole\t(plug,\tcapsule,\tcontainer)\t •  Press/switch\tbutton\t •  Turn\ttap\topen\t •  Fill\ta\tcontainer\twith\ta\tliquid\tfrom\ttap\t •  Pour\ta\tliquid\tfrom\tbottle/brick,\tsugar\tfrom\tsachet\t •  Take/”pour”\tsomething\tout\tof\ta\tbox\t Learning\tfor\tFine\tManipulation:\tinsertion\ttask\t UJI\tRob.\tIntell.\tLab.\t A.P.\tdel\tPobil\t–\tIMCOM\t2018\tLangkawi\t 13\t •  Q-learning:\tfor\tdelayed\treward\t •  No\texplicit\tmodel\tof\tthe\tsystem\t •  Input:\tsensed,\tprocessed,\tstate\t •  Output:\taction\t Tasks\tRequiring\tManual\tIntelligence\t A.P.\tdel\tPobil\t–\tAffecTech\t\tWorkshop\tLisbon\t 14\t •  More\tdifficult\ttask\tfor\ta\trobot?\t/\tTwo\thands?\t –  Tear\topen\tthe\tsugar\tsachet\t –  Find\tspoon\tin\tthe\tdishwasher\tamong\tcutlery\t –  Take\tcapsule\tout\tof\tthe\tbox\t –  Put\tback\twater\tcontainer\t •  Error\tdetection\tand\trecovery\t –  Capsules\tscattered\ton\tthe\tfloor\t –  Wrong\tcup\t –  Overflown\twater\tcontainer\t –  No\tspoons","libVersion":"0.3.2","langs":""}