{"path":"_aula_virtual/SJK002/SJK002-U11-06-Transformers.pdf","text":"Transformers for Vision Computer Vision (SJK02) Universitat Jaume I Understanding Deep Learning (MIT, 2023) Chapter 12: Transformers Luis Serrano Attention mechanisms Attention with maths Transformers Transformer models Probabilistic Machine Learning (MIT 2022) Chapter 15: NN for sequences Jay Alammar The Illustrated Transformer Peter Bloem Transformers from scratch Why are they called \"transformers\"? 3 Hypothesis 1: Just marketing Hypothesis 2: Paradigm shift Hypothesis 3: Inputs are transformed Other? A bottom-up explanation 4 Attention mechanism 56 Standard layer: fixed operation What about something more flexible, input-dependent? 7 Self-attention as \"routing\" of values How many attention weights needed for input sequence of N elements? 8 Attention = soft dictionary lookup If q is most similar to key i, then use more value i 9 Differentiable lookup 10https://peterbloem.nl/blog/transformers 12 Scaled dot-product attentionBlock diagram Transformation matrices are shared among inputs; their size is independent of sequence length! Visual explanation Sentence 1: The bank of the river. Sentence 2: Money in the bank. https://txt.cohere.com/what-is-attention-in-language-models/ Modified sentence 1: The bank1 of the river. Modified sentence 2: Money in the bank2. Bank1 = 0.9*Bank + 0.1*River Bank2 = 0.8*Bank + 0.2*Money We find these \"weights\" through mechanisms such as similarity and attention Similarity matrices 18 Seq2Seq model Context vector c encodes the whole input sequence x Output has no access to input tokens If we had access, which token should we \"pay attention to\"? 19 Seq2Seq with attention 20 Are transformers better than RNNs? • Transformers deal better with long-range dependencies • Transformers process the input at same time (not step-by-step) Example: NLP translation x = Hace mucho frío aquí. y = It is very cold here. Transformer 22 Transformers 23 Transformer = Seq2Seq model with attention at both E and D \"Any architecture designed to process a connected set of units—such as the tokens in a sequence or the pixels in an image—where the only interaction between units is through self-attention.\" Do we really need both encoder and decoder? Don't we need also cross-attention? Self attention (at encoder) 24 Input attends to itself! At decoder: masked self-attention 25 Example: Translation English-French 26 x = The animal didn't cross the street because it was too tired x = The animal didn't cross the street because it was too wide Multi-head attention Enrich the representation: having several notions of similarity 28 x = The animal didn't cross the street because it was too wide Multihead – block diagram Positional encoding Sequences have order, but self-attention is permutation invariant! 31 Transformer: MH SA + something else 32 Encoder [+ decoder] transformers Self-attention: • input tokens attend to one another Masked self-attention • output tokens attend to previous output tokens Cross-attention • output tokens attend to input tokens Let's identify the (q,k,v) at the three different attention blocks Pseudocode def EncoderBlock(X): Z = LayerNorm(MHA(Q=X, K=X, V=X) + X) # note the residual connection E = LayerNorm(FeedForward(Z) + Z) return E Encoder = series of M encoder blocks def Encoder(X, M): # M = number of layers E = POS(Embed(X)) for _ in range(M): E = EncoderBlock(E) return E def DecoderBlock(X, E): Z = LayerNorm(MHA(Q=X, K=X, V=X) + X) Z' = LayerNorm(MHA(Q=Z, K=E, V=E) + Z) D = LayerNorm(FeedForward(Z') + Z') return D Decoder has access to: • encoder (via another MHA) • Previously generated output def Decoder(X, E, N): D = POS(Embed(X)) for _ in range(N): E = DecoderBlock(D, E) return D Attention is not explanation 36https://towardsdatascience.com/is-attention-explanation-b609a7b0925c Or is it? Many variations proposed 37 Conformer (conv layers inside Transformer) Reformer Linformer Performer ... A top-down explanation 38https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452 Encoder, Decoder and how they relate At training... Loss: compares output sequence with target sequence (known because it's in the training data) Teacher Forcing At inference... Last word of output sequence added at the end of the decoder input Repeat 3, 4, 5, 6 until \"end of sequence\" We don't alwyas have a decoder... Encoder as a building block for different applications https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3 Words (sequence items) go through a series of transformations Transformations are learnable! Role of Q, K, V The Query word: word for which we are calculating Attention The Key and Value word: word to which we are paying attention (how relevant is that word to the Query word) And how is \"relevance\" learned? The more \"aligned\" (similar embeddings) two words are, the higher their attention score So, the transformer learns to produce low or high scores And how are such scores found? From the training instances Vision Transformers 51 CNNs vs Transformers for images 52 CNNs have built-in inductive bias: • locality (small kernel) • equivariance (weight sharing) • location invariance (pooling) Transformers don't seem a good fit for 2D data... An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR 2021) ViT: transformer encoderSWin (shifted-window): multi-scale transformer Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows (ICCV 2021) Efficiency: • Limiting self-attention to non-overlapping local windows Hierarchical architecture: • Flexibility to model at various scales • Linear computational complexity with respect to image size Local attention Hierarchical O(N) Global attention Single and coarse level O(N2) Window partitioning is shifted in alternate layers --> this changes the interacting patches (and connect windows) Ablation study (SWin) Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows (ICCV 2021) Findings Expensive training: 30 days on ImageNet-21k on Google Clout TPUx3 with 8 cores Transformers also work well (despite the lack of inductive bias!) • If trained with enough data (to compensate this lack)Video Transformers 61Video Transformers: A Survey (PAMI 2023)Video Transformers: A Survey (PAMI 2023)","libVersion":"0.3.2","langs":""}