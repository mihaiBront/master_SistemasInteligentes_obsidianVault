{"path":"_aula_virtual/SJK003/02-experimentalDesignI.pdf","text":"Department of Computer Languages and Systems Model Evaluation Introduction â€¢ model evaluation can be performed in supervised learning as the actual values are available â€¢ there is a fundamental difference between the methods used for evaluating a regressor and a classifier: â€“ in regression, we deal with continuous values and the focus is on the error between the actual and predicted values â€“ in classification, the focus is on the number of data that are classified correctly Classification models â€¢ In classification, we deal with two types of models: â€“ in some classifiers (e.g., k-NN, SVM), the output is simply the class label â€“ in others (e.g., logistic regression, random forest), the output is the probability of a data point belonging to a particular class where through the use of a cut off value we are able to convert these probabilities into class labels Confusion matrix Suppose a data set with class labels 0 (â€˜negativeâ€™ class) and 1 (â€˜positiveâ€™ class), and a classifier that can produce correct and incorrect predictions: â€¢ True positive: the number of positive observations that were correctly predicted by the model â€¢ True negative: the number of negative observations that were correctly predicted by the model â€¢ False positive: the number of negative observations that were incorrectly predicted as positive by the model â€¢ False negative: the number of positive observations that were incorrectly predicted as negative by the model Confusion matrix (ii) For a two-class problem, the confusion matrix will have two rows and two columns where the rows represent predicted values and the columns represent actual values Our objective is to minimize the FP and FN and maximize the TP and TN, that is, to maximize the diagonal values Confusion matrix (iii) From the confusion matrix, various evaluation metrics can be calculated. The most basic measures are accuracy and classification error: ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘ƒ + ğ¹ğ‘ ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ = ğ¹ğ‘ƒ + ğ¹ğ‘ ğ‘‡ğ‘ƒ + ğ‘‡ğ‘ + ğ¹ğ‘ƒ + ğ¹ğ‘ = 1 âˆ’ ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ Confusion matrix (iv) Accuracy and classification error can be very misleading For instance, suppose a data set with 100 instances where 99 belong to class â€˜Aâ€™ and only 1 belongs to class â€˜Bâ€™. A model that predicts all instances to class â€˜Aâ€™ will have an accuracy of 99%, but the accuracy here provides us with little information on how actually the model is performing We need other more elaborated measures to evaluate the models in some problems, and more specifically in class imbalanced problems Other evaluation measures â€¢ False positive rate (false alarm): the proportion of negative observations that were wrongly (as positive) predicted by the model ğ¹ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ = ğ¹ğ‘ƒ ğ¹ğ‘ƒ + ğ‘‡ğ‘ â€¢ False negative rate (miss rate): the proportion of the positives that were classified as negative by the model ğ¹ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ = ğ¹ğ‘ ğ¹ğ‘ + ğ‘‡ğ‘ƒ Other evaluation measures (ii) â€¢ True positive rate (recall, sensitivity): the proportion of positives that were classified correctly by the model. It is the opposite of FNrate ğ‘‡ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ = ğ‘‡ğ‘ƒ ğ‘‡ğ‘ƒ + ğ¹ğ‘ = 1 âˆ’ ğ¹ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ â€¢ True negative rate (specificity): the proportion of negatives that were classified correctly by the model. It indicates how good the model is at avoiding the misclassification of the negatives ğ‘‡ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ = ğ‘‡ğ‘ ğ‘‡ğ‘ + ğ¹ğ‘ƒ = 1 âˆ’ ğ¹ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ Other evaluation measures (iii) â€¢ Precision (positive predicted value): the proportion of the positives compared to what the model predicted as positive. Unlike TPrate, Precision provides the percentage of TP over all instances predicted as positive ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ğ‘ƒ ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ â€¢ Negative predicted value: the proportion of the negative compared to what the model predicted as negative ğ‘ğ‘ƒğ‘‰ = ğ‘‡ğ‘ ğ‘‡ğ‘ + ğ¹ğ‘ Other evaluation measures (iv) All these metrics are useful, but they are often used in pairs, such as Recall-Precision, FPrate-FNrate, TPrate-TNrate, etc. Other evaluation measures (v) â€¢ Geometric mean of accuracies: it achieves the maximum value when the accuracy on each of the classes is maximum while keeping these accuracies balanced ğºğ‘šğ‘’ğ‘ğ‘› = ğ‘‡ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ âˆ™ ğ‘‡ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ â€¢ Mean class weighted accuracy: ğ¶ğ‘Šğ´ = ğ‘¤ Ã— ğ‘‡ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ + (1 âˆ’ ğ‘¤) Ã— ğ‘‡ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ where ğ‘¤ âˆˆ [0,1] Other evaluation measures (vi) â€¢ F-measure: ğ¹ âˆ’ ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ = (1 + ğ›½2) âˆ™ ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ âˆ™ ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› ğ›½2 âˆ™ ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ + ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› where ğ›½ âˆˆ [0,1]. If precision and recall are equally important, then ï¢ is set to 1, and F-measure is known as the F1-measure â€¢ Area under the ROC curve: ğ´ğ‘ˆğ¶ = ğ‘‡ğ‘ƒğ‘Ÿğ‘ğ‘¡ğ‘’ + ğ‘‡ğ‘ğ‘Ÿğ‘ğ‘¡ğ‘’ 2 Other evaluation measures (vi) â€¢ Matthews correlation coefficient: ğ‘€ğ¶ğ¶ = ğ‘‡ğ‘ƒ âˆ™ ğ‘‡ğ‘ âˆ’ ğ¹ğ‘ƒ âˆ™ ğ¹ğ‘ (ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ) âˆ™ (ğ‘‡ğ‘ƒ + ğ¹ğ‘) âˆ™ (ğ‘‡ğ‘ + ğ¹ğ‘ƒ) âˆ™ (ğ‘‡ğ‘ + ğ¹ğ‘) MCC ranges between -1 and 1 (the best agreement between true values and predictions). MCC = 0 means that the prediction is random with the true values. â€¢ Gini coefficient: ğºğ‘–ğ‘›ğ‘– = 2 Ã— ğ´ğ‘ˆğ¶ âˆ’ 1 Regression models â€¢ In regression, evaluation methods depend on the ways of calculating the residual (difference between the actual and predicted values): â€“ some methods (e.g., sum squared error, mean squared error and root mean squared error) just calculate this difference â€“ others (e.g., adjusted coefficient of determination) also take into account the problem of overfitting Regression models (ii) â€¢ In regression, unlike classification (where we can count the number of observations correctly classified), our predictions are either bigger or smaller than the actual value (rarely it is same as the actual value) â€¢ Thus, we are not concerned with how many times we were wrong but rather what matters is the size of residuals â€¢ To validate a regression model, you must use residual plots to visually confirm the validity of the model Regression models (iii) Residual plots: the residual values on the y-axis and the predicted values on the x-axis â€¢ if it has a high density of points close to the origin and a low density of points far from the origin and they are distributed symmetrically around the horizontal axis, the regression model is appropriate Regression models (iv) How to know whether a residual plot is good based on the characteristics just presented? If we project all the residuals onto the y-axis, we get a normally distributed curve. This satisfies the assumption that residuals are independent and normally distributed Measures to compare multiple data sets â€¢ Mean/median of prediction: we can understand the bias in prediction between two models using the arithmetic mean of the predicted values. If there are outliers, it is better to use the median â€¢ Standard deviation of prediction: it is a measure of the amount of variation or dispersion of a set of values ğ‘†ğ· = Ïƒğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à´¤ğ‘¦)2 ğ‘› âˆ’ 1 A low SD indicates that the values tend to be close to the mean (also called the expected value) of the set. In contrast, a high SD indicates that the values are distributed over a wider range Measures to compare multiple data sets (ii) â€¢ Range of prediction: it is the maximum and minimum values in a data set, helping to understand its dispersion â€¢ Coefficient of variation (relative standard deviation): it simply represents the variance standardized by its mean ğ¶ğ‘‰(%) = ğ‘†ğ· ğ‘šğ‘’ğ‘ğ‘› Ã— 100 SD is the most common measure of variability/dispersion for a single data set, but it is not useful when comparing two different data sets. In such cases, CV is the method of choice Measures to compare multiple data sets (iii) For instance, consider two different data sets: â€¢ Data 1: Mean1 = 120000; SD1 = 2000 â€¢ Data 2: Mean2 = 900000; SD2 = 10000 It seems that Data 2 is more spread out than Data 1 However, let us now calculate CV for both data sets: â€¢ CV1 = SD1/Mean1x100 = 1.6% â€¢ CV2 = SD2/Mean2x100 = 1.1% We can conclude Data 1 is more spread out than Data 2 Common evaluation measures â€¢ Sum squared error: it calculates the difference between the true (ğ‘¦ğ‘–) and predicted ( à·œğ‘¦ğ‘–) values across the n test observations ğ‘†ğ‘†ğ¸ = à· ğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘–)2 â€¢ Mean squared error: it is the SSE divided by the number of test observations. This is the most common metric for regression ğ‘€ğ‘†ğ¸ = 1 ğ‘› à· ğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘–)2 â€¢ Root mean squared error: it is the square root of MSE ğ‘…ğ‘€ğ‘†ğ¸ = ğ‘€ğ‘†ğ¸ Common evaluation measures (ii) â€¢ The lower the MSE/RMSE value, the better the model is with predictions â€¢ A high MSE/RMSE indicates that there are large deviations between the predicted and actual values â€¢ Drawbacks of MSE/RMSE: â€“ sensitive to outliers â€“ a single big error will have the same effect as many small errors Common evaluation measures (iii) The predictions of Model-One had an error of 1 for all 10 samples; for Model-Two, 9 samples were predicted correctly, but one prediction had an error of 4.16 â†’ the single big error made by Model-Two has the same impact as the 10 small errors made by Model-One Other evaluation measures â€¢ Relative squared error: it takes the total squared error of our model and normalizes by the total squared error of a model that uses the mean as the predictor ğ‘…ğ‘†ğ¸ = Ïƒğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘–) 2 Ïƒğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à´¤ğ‘¦)2 where à´¤ğ‘¦ = 1 ğ‘› Ïƒğ‘–=1 ğ‘› ğ‘¦ğ‘– RSE > 1 indicates that our model is even worse than a model that simply predicts the mean as a prediction for each sample RSE = 0 corresponds to the ideal case Other evaluation measures (ii) â€¢ Mean absolute error: it is similar to MSE, but instead of squaring the difference between the actual and predicted values, this takes an absolute value of it ğ‘€ğ´ğ¸ = 1 ğ‘› à· ğ‘–=1 ğ‘› ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘– Unlike MSE/RMSE, here a big error does not overpower a lot of small errors â†’ the output provides us with a relatively unbiased understanding of how the model is performing Other evaluation measures (iii) â€¢ Median absolute error: it is the median of all absolute differences between the actual and predicted values ğ‘€ğ‘’ğ‘‘ğ´ğ¸ = ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›ğ‘–=1 ğ‘› ( ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘– ) â€¢ Mean absolute percentage error: it is calculated as the absolute difference between the actual and predicted values divided over every observation ğ‘€ğ´ğ‘ƒğ¸ = 1 ğ‘› à· ğ‘–=1 ğ‘› ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘– ğ‘¦ğ‘– Ã— 100 Other evaluation measures (iv) â€¢ Relative absolute error: like RSE, it is also used to compare between models whose errors are measured in different units ğ‘…ğ´ğ¸ = Ïƒğ‘–=1 ğ‘› ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘– Ïƒğ‘–=1 ğ‘› ğ‘¦ğ‘– âˆ’ à´¤ğ‘¦ where à´¤ğ‘¦ = 1 ğ‘› Ïƒğ‘–=1 ğ‘› ğ‘¦ğ‘– RAE ranges from 0 to infinity, with 0 corresponding to the ideal case Other evaluation measures (v) â€¢ Coefficient of determination (R2): this is a statistical measure that determines how well a regression model predicts an outcome. This means how good is a model for a data set ğ‘…2 = 1 âˆ’ Ïƒğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à·œğ‘¦ğ‘–) 2 Ïƒğ‘–=1 ğ‘› (ğ‘¦ğ‘– âˆ’ à´¤ğ‘¦)2 R2 lies between 0 and 1 R2 = 0 indicates that the model fails to accurately model the data at all The closer its value to 1, the better a model is Other evaluation measures (vi) â€¢ Adjusted coefficient of determination: it penalizes adding more independent variables which do not increase the explanatory power of the regression model. It is always less than or equal to the value of R2 ğ‘ğ‘‘ğ‘—_ğ‘…2 = 1 âˆ’ (1 âˆ’ ğ‘…2) ğ‘› âˆ’ 1 ğ‘› âˆ’ ğ‘ âˆ’ 1 where p is the number of independent variables and n the sample size","libVersion":"0.3.2","langs":""}