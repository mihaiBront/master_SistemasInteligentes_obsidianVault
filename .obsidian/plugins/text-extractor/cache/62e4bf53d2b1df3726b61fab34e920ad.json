{"path":"_aula_virtual/SJK001/Reading Assessments/[Billard19] Trends and challenges in robot manipulation.pdf","text":"REVIEW SUMMARY ◥ ROBOTICS Trends and challenges in robot manipulation Aude Billard* and Danica Kragic BACKGROUND: Humans have a fantastic abi- lity to manipulate objects of various shapes, sizes, and materials and can control the ob- jects’ position in confined spaces with the advanced dexterity capabilities of our hands. Building machines inspired by human hands, with the functionality to autonomously pick up and manipulate objects, has always been an essential component of robotics. The first robot manipulators date back to the 1960s and are some of the first robotic devices ever con- structed. In these early days, robotic manipula- tion consisted of carefully prescribed movement sequences that a robot would execute with no ability to adapt to a changing environment. As time passed, robots gradually gained the abil- ity to automatically generate movement se- quences, drawing on artificial intelligence and automated reasoning. Robots would stack boxes according to size, weight, and so forth, extending beyond geometric reasoning. This task also re- quired robots to handle errors and uncertainty in sensing at run time, given that the slightest imprecision in the position and orientation of stacked boxes might cause the entire tower to topple. Methods from control theory also be- came instrumental for enabling robots to com- ply with the environment’s natural uncertainty by empowering them to adapt exerted forces upon contact. The ability to stably vary forces upon contact expanded robots’ manipulation repertoire to more-complex tasks, such as in- serting pegs in holes or hammering. However, none of these actions truly demonstrated fine or in-hand manipulation capabilities, and they were commonly performed using simple two- fingered grippers. To enable multipurpose fine manipulation, roboticists focused their efforts on designing humanlike hands capable of using tools. Wielding a tool in-hand became a prob- lem of its own, and a variety of advanced algorithms were developed to facilitate stable holding of objects and provide optimality guarantees. Because optimality was difficult to achieve in a stochastic environment, from the 1990s onward researchers aimed to increase the robustness of object manipulation at all levels. These efforts initiated the design of sensors and hardware for improved control of hand–object contacts. Studies that followed were focused on robust perception for coping with object occlusion and noisy measurements, as well as on adaptive control approaches to infer an object’s physical properties, so as to handle objects whose properties are unknown or change as a result of manipulation. ADVANCES: Roboticists are still working to develop robots capable of sorting and pack- aging objects, chopping vegetables, and folding clothes in unstructured and dynamic environ- ments. Robots used for modern manufactur- ing have accomplished some of these tasks in structured settings that still require fences be- tween the robots and human operators to ensure safety. Ideally, robots should be able to work side by side with humans, offering their strength to carry heavy loads while presenting no danger. Over the past decade, robots have gained new levels of dexterity. This enhancement is due to breakthroughs in mechanics with sensors for perceiving touch along a robot’s body and new mechanics for soft actuation to offer natural compliance. Most notably, this development leverages the immense progress in machine learning to encapsulate models of uncertainty and support further advances in adaptive and robust control. Learning to manipulate in real- world settings is costly in terms of both time and hardware. To further elaborate on data- driven methods but avoid generating examples with real, physical systems, many researchers use sim- ulation environments. Still, grasping and dexterous manipulation require a level of reality that exist- ing simulators are not yet able to deliver—for example, in the case of modeling contacts for soft and deformable objects. Two roads are hence pursued: The first draws inspiration from the way humans acquire interaction skills and prompts robots to learn skills from observing humans per- forming complex manipulation. This allows robots to acquire manipulation capabilities in only a few trials. However, generalizing the acquired knowledge to apply to actions that differ from those previously demonstrated re- mains difficult. The second road constructs databases of real object manipulation, with the goal to better inform the simulators and generate examples that are as realistic as pos- sible. Yet achieving realistic simulation of friction, material deformation, and other phys- ical properties may not be possible anytime soon, and real experimental evaluation will be unavoidable for learning to manipulate highly deformable objects. OUTLOOK: Despite many years of software and hardware development, achieving dexter- ous manipulation capabilities in robots remains an open problem—albeit an interesting one, given that it necessitates improved understand- ing of human grasping and manipulation techniques. We build robots to automate tasks butalsotoprovide toolsfor humans to easily perform repetitive and dangerous tasks while avoiding harm. Achieving robust and flexible collaboration between humans and robots is hence the next major challenge. Fences that currently separate humans from robots will gradually disappear, and robots will start manip- ulating objects jointly with humans. To achieve this objective, robots must become smooth and trustable partners that interpret humans’ intentions and respond accordingly. Further- more, robots must acquire a better understand- ing of how humans interact and must attain real-time adaptation capabilities. There is also a need to develop robots that are safe by design, with an emphasis on soft and lightweight struc- turesaswellascontrol andplanningmethod- ologies based on multisensory feedback. ▪ RESEARCH Bilard et al., Science 364, 1149 (2019) 21 June 2019 1of 1 Holding two objects in one hand requires dexterity. Whereas a human can grab multiple objects at the same time (top), a robot (bottom) cannot yet achieve such dexterity. In this example, a human has placed the objects in the robot’shand. The list of author affiliations is available in the full article online. *Corresponding author. Email: aude.billard@epfl.ch Cite this article as A. Billard, D. Kragic, Science 364, eaat8414 (2019). DOI: 10.1126/science.aat8414PHOTOS:LEARNINGALGORITHMSANDSYSTEMSLABORATORY,EPFL ON OUR WEBSITE ◥ Read the full article at http://dx.doi. org/10.1126/ science.aat8414 ..................................................on August 8, 2020 http://science.sciencemag.org/Downloaded from REVIEW ◥ ROBOTICS Trends and challenges in robot manipulation Aude Billard 1* and Danica Kragic 2 Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human. H ave you ever found yourself busy forag- ing in your bag in search of a set of keys? If so, you may recall that it took only a few seconds to find them among the disparate contents of the bag. For certain, you did not reflect on your abilities and may have carried on this display of unique dexterity through a swift in-hand manipulation, taking out the cor- rect key and inserting it into the lock even though the corridor lights had gone out. All day long, our fingers grasp, move, and transform objects and interact with objects in various media such as air, water, and oil. We do not spend time thinking about what our hands and fingers are doing or how the continuous integration of var- ious sensory modalities—such as vision, touch, proprioception, and hearing—help us outperform any other biological system in the breadth of the interaction tasks we can execute. Largely over- looked, and perhaps most fascinating, is the ease with which we perform these interactions, re- sulting in a belief that they are also easy to ac- complish in artificial systems such as robots. Manipulating objects is such a ubiquitous ac- tivity that we forget how difficult it was to ac- quire this competence as a child. Children are born with simple grasp reflexes. It takes them 3 years to develop an individuated control of each finger and another 6 years to display an adult-equivalent ability for making smooth con- tact and for planning sequences of manipulation skills (1). Even for humans, some dexterous activ- ities may pose a challenge. For example, tying shoes may be done in various ways, and there may be several valid models of how to execute such an activity. In addition, we can visually dem- onstrate how to do something and what the expected result may be, but we cannot easily communicate the magnitude of the applied forces and torques or the size of the friction co- efficient necessary to satisfy stability conditions. Still, we find ways of achieving manipulation goals through training and exploration even if the end result is not always optimally performed. We may also adapt as circumstances dictate (e.g., tying shoes with an excess of free shoelace or when the ends are quite short), forcing us to deviate from our normal methods. Thus, the context in which interactions are performed af- fects various parameters of the execution. Although robotics has made vast progress in mechanical design, perception, and robust con- trol targeted to grasping and handling objects, robotic manipulation is still a poor proxy for hu- man dexterity. To date, no robots can easily hand- wash dishes, button a shirt, or peel a potato. What can robots do today? Robots are skilled at picking up and manipu- lating objects in repetitive and familiar settings such as industrial assembly setups. In such set- tings, the geometry, material properties, and weight of the objects are commonly known. Robots can handle some variation in routine movements in terms of adapting to small differ- ences in the object properties, but the whole pro- cess is typically optimized to a limited set of expected variations. In early factory settings, robot arms followed predetermined trajectories and assumed that objects would always appear at the same place. Today, robots can adapt their trajectory to retrieve objects at different loca- tions, making it possible for objects to be placed by humans or simply dropped on a conveyer belt instead of being deposited at exact positions by other machines. The classical assembly lines in which robots were bolted into the floor and placed one after another, typical for the automobile in- dustry, can now be made more flexible. Objects moving on conveyers can be detected fairly easily by cameras and picked up if fully visible. How- ever, detection of transparent objects or objects partially hidden (e.g., when stacked on top of one another) remains difficult. With the need to frequently change the type of goods produced, the robotics industry strives for multipurpose object grasping and handling solutions. One step toward this objective is to provide robots with a choice of grippers varying in size and strength and to enable robots with tool-changing mechanisms so that they can select the correct tool. To determine which tool to use for a given task, a robot must have knowledge of an object’s properties, such as shape, weight, ma- terial, and so forth. This information is readily available in factories where all objects are known. However, this requirement presents a limitation for robots in other settings, where the set of objects to be manipulated may not be known beforehand. What can robots not do today? Although robots are adept at handling rigid objects, they still struggle with flexible materials— such as fruits and vegetables or clothing items— that differ in size, weight, and surface proper- ties. Manipulations that produce a deformation (e.g., inserting, cutting, or bending) are particu- larly difficult, as accurate models of the defor- mations are needed. Industrial grippers often use pneumatic vacuum pumps to pick up objects bysucking. Thistechniqueisunbeatablewhen it comes to grasping an object but is much less useful for object manipulation (e.g., reorienting the object and placing it in a confined space). Onestep toaddressthischallenge is to provide robots with more dexterous hands. Yet creating hands asdexterousashuman handsisdifficult, owing to a lack of sensors and actuators equiv- alent in size, precision, and efficiency to our skin and muscles. Improvements in robots’ dexterity are not lim- ited to the engineering of more-capable hands. Advanced software programs are required to analyze in real time the large flux of visual, tac- tile, and force information and to relate these different senses to recognize objects and model their transformations. Additionally, robots need advanced cognitive capabilities to predict where, how, and why to manipulate objects. The rest of this Review describes why overcoming these challenges is difficult and where the field of robotics stands today. Why is designing robotic hands difficult? Although research on robot hands has been on- going for more than five decades (2–4), the most common hand used in many applications to date is still a parallel jaw gripper, usually without any extra sensing. Picking up objects with a gripper devoid of sensing is akin to grasping with the tip of your thumb and index finger when both are numb! This tool may suffice for simple pick- and-place actions, but not for more-complex motions such as shuffling keys. Because the human hand performs intricate movements with RESEARCH Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 1of 8 1Learning Algorithms and Systems Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland. 2Robotics, Perception and Learning (RPL), EECS, Royal Institute for Technology (KTH), Stockholm, Sweden. *Corresponding author. Email: aude.billard@epfl.chon August 8, 2020 http://science.sciencemag.org/Downloaded from ease, it is a natural inspiration for robotics. But designing robotic hands with sensors and ac- tuators similar to those of the human hand is difficult for many reasons. When constructing anthropomorphic robot hands, it is challenging to fit all of the necessary actuators, sensors, and mechanical structure in the limited available space. Another obstacle is to keep the total weight of the hand low so that it satisfies the payload requirements of the arm to which it is attached. Hence, compared with human hands, most anthropomorphic hands and prostheses do not have nearly as many controllable degrees of freedom (5, 6). The human hand is soft and flexible, with a dexterous thumb whose distinctive range of mo- tion remains difficult to replicate mechanically (7), as the intricate combinations of tendons and muscles differ markedly from traditional serial robotic joint design (8). Today, robotic hands are still largely composed of rigid plastic and metal components, with electric motors as actuators. This rigidity is partially the cause of the lack of dexterity, as it allows no room for mistakes when executing grasps. Rigid fingers closing on an ob- ject mayeasilymove ratherthangrasp an object if its pose is not perfectly estimated, and apply- ing too much force may crush the object. A grow- ing trend in robotics is the development of soft hands that can conform to an object’s shape, absorb unexpected forces at contact, and com- pensate for load change during manipulation (9, 10). Softness can be achieved through a change in hardware or software or a combination of both (Fig. 1). Softness from material used to construct hands builds on solutions from 3D manufac- turing and materials science. For instance, one can manufacture rigid and flexible materials in a layer-by-layer manner to create foldable fingers that can deploy and retract as needed (11). Cur- rently, the low payload and slow speed of these elastomers restricts manipulation to light objects only. As an alternative to generate more power, pneumatic or hydraulic actuation may be used (12, 13). Human hands are covered with a multipurpose skin that provides the appropriate level of friction and damping. Human skin is a high-frequency and high-resolution sensor that provides precise information on normal and tangential forces, information that is critical for grip adjustment. Human skin can also measure stretch and tem- perature. By contrast, robot hands typically mea- sure exerted forces through miniature force sensors placed solely at the fingertips (14). Force sensors yield very accurate 3D measurements, but they cannot easily reveal the exact location of contact. To move objects once held in the hand or to hold multiple objects at once (Fig. 1), one needs to measure precise contact points, not just at the fingertips but also along the length and side of the fingers and inside the palm. This can be achieved through artificial skins that provide contact measurements all along the limbs. In- terest in artificial skins can be traced back to the 1980s (15, 16), but major advances were achieved in the past decade. At present, we find a variety of affordable commercial products, several of which can be customized to a robot’s shape. Touch sensors measure the normal contact force; a few also provide data on tangential forces, torque, tem- perature, vibrations, or surface properties. Never- theless, most touch sensors are rigid, and their placement is constrained to fingertips and along limb segments. Touch detection at the joint (knuckle, elbow, knee) is, however, crucial to de- tect entrapment. It is also useful to guide explo- ration inside objects (17). Such contact can be detected only by soft sensors that bend and ex- tend along the flexion and extension points of the limb (Fig. 2, right) (18). Hence, flexible and stretchable skins are of utmost interest to robot- icists (19). Prototypes exist in laboratories, and we can expect to see their deployment soon, given the current interest in soft electronics (20). As an alternative to using skin, it is possible to deduce haptic (contact and force) information from vision. One can, for instance, infer forces from vision through a dynamic model of contacts (21) (Fig. 2, left) or use an optical sensor that renders deformation of an object’s geometry at a high spatial resolution (22). The necessity of estimating the exact position of an object, its local geometry, and other properties such as weight and weight distribution depends strongly on the application. It is the interplay among hand design, material, and internal and external sensing that offers the appropriate redundancy. One additional challenge is the need to mea- sure contact at a very high frequency for accu- rate and timely detection of slip (23). Such high spatial and temporal resolution, together with Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 2of 8 Fig. 1. Soft hands. Robotic hands are traditionally made of hard materials with rigid control of fingers. Recent designs aim to mimic the human hand’s natural compliance by using soft actuators, soft materials, and advanced controllers. (A) Rigid material and actuators and (B) a rigid cover with partially soft cable-driven actuation: Both hands become soft through software intervention, modulating pressure at the fingertips via tactile feedback. [Reproduced from (17, 33)] (C) A soft, foldable gripper that can adapt shape and stiffness. [Reproduced from (11)] (D) Soft actuation and material for a rehabilitation glove that can be worn by a human. [Reproduced from (12)] Fig. 2. Sense of touch for robots. (Left) Vision can be used to infer contact forces (red). [Reproduced from (21)] (Right) Stretchable artificial skin measures contact at knuckles, which may be useful for exploring internal parts of objects. [Reproduced from (18)] RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from the real-time processing of vision data for ob- ject tracking, leads to a computational overload with a massive data stream that must be inter- preted in real time. This processing is usually carried out by a CPU (central processing unit) located away from the hand. Alternatively, pro- cessing may be performed on the hand itself through the use of a dedicated CPU (24), but such CPUs have focused solely on processing vision data. Additional studies are needed to develop hardware for processing tactile infor- mation in conjunction with vision. Dexterous robot hands may hence be realized by using research in materials science for the design of soft actuation, enabling contact sensing along the entire surface of the hand, and by using advances in electronics for onboard, real-time processing of multisensory data. Design beyond anthropomorphism Although the human hand is fascinating, it does not have to be the ultimate solution for robotics. A human hand design may be desirable for aes- thetic reasons; for instance, when designing hand prostheses or humanoid robots. But this same design may be superfluous for many robots. In- dustrial hands remain a good solution for spe- cific tasks. Rather than try to replicate the positioning of human fingers, these hands have two or three fingers arranged symmetrically around the palm, a design particularly suited for industrial screwing. Robotics keeps oscillating between anthropo- morphic and traditional industrial designs for hands. But the gripping systems of simpler ani- mals may also provide inspiration. For instance, fish suck in their prey. Adding suction at robots’ fingertips is useful under water, as this technol- ogy cancels the flow generated by the hand (25). Why not create hands that both leverage and go beyond nature? For instance, the human thumb is amazing, but it creates an asymmetry that constrains the orientation of the hand for manipulation. Two thumbs on the same hand, however, would provide a dexterity beyond human capability (Fig. 3). Desiderata for the next generation of robotic hands The objects around us have been built and adapted to our hands, which are still rather small and very robust in comparison with con- temporary robot hands. Enabling robots to pick up small items such as pens, raisins, screws, and needles is a clear functionality goal. Today, ro- botic arms and hands are commonly developed separately, and integrating them is an engineer- ing job of its own. Industrial arms have sub- stantial payloads but are commonly designed to be bolted into the floor and are too large to be deployed outside industrial settings. The arms of humanoid robots and robots intended for fine assembly tasks have low payloads, which are typically not sufficient to carry a hand and an object held by the hand. Adding sensing functionalitytoarmsand handsrequirescab- ling that can quickly become complicated. Fur- thermore, many hands come with no or limited means of measuring contact and forces. Thus, a change in paradigm is needed to move away from developing robotic arms devoid of hands and hands devoid of arms. We must further ensure that hands are developed in a “plug- and-play” manner and can easily be attached and detached through existing tool-switching systems. State-of-the-art force and tactile sen- sors must become an inherent part of the arm– hand system. Robot dexterity is as much a by-product of advances in hardware as it is of advances in soft- ware. It requires suitable algorithms to rapidly and efficiently process the vast amount of in- formation collected through sensors and actua- tors. At the same time, it needs algorithms to adequately control the movement of a hand in relation to object, scene, and task properties. We next review advances in perception, control, and learning for manipulation. Perception for manipulation As for humans, robot perception for manipu- lation is multimodal (Fig. 4). Vision is instru- mental for recognizing and localizing objects. When associated with a database of existing objects, robot vision can help infer geometric and physical properties of known and even unknown objects (26), and this information is important for shaping the aperture of the hand and the forces to be applied. Proprioception— namely, knowledge of where the robot’s limbs are located—is needed to guide the arm and hand toward the object, with visual support to contin- uously track the object. Touch and force mea- surements become important once contact has occurredand theobject isheldorexploredbythe hand. The associated control algorithms are used to guide the grasp and/or to infer the object’s physical properties, such as rigidity and mass distribution, that may have been poorly estimated or unknown previously. Sound has also received attention recently as a means to infer an invisible object’s content and to monitor changes in content during manipulation (27). As an example, a robot is tasked with fetching a package of milk from a refrigerator. Before the robot holds the package in hand, it may not know how much milk the package contains nor the package’s actual weight. Given that the package may be made of cardboard, the robot needs to know the weight in order to apply a suitable grasping force and avoid destroying Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 3of 8 Fig. 4. Manipulation is multimodal. Vision is used before contact, whereas haptics and sound are involved upon contact to estimate an object’s physical properties that cannot be directly observed. [Photo: Learning Algorithms and Systems Laboratory, EPFL] Fig. 3. Designing hands beyond human dexterity. Two thumbs would make it possible to execute screwing and unscrewing motions with one hand rather than two. This capability may be useful for robots and humans via prostheses. [Illustrations: Laura Cohen] RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from thepackage.Inthe case of milk,sound may also provide information about the viscosity when the package is shaken, as milk will sound different from another substance, such as yogurt. Over the past few years, major efforts have been undertaken to analyze visual information, and progress has been considerable. Neverthe- less, robots still struggle to recognize objects that are partially occluded (28), particularly when viewed from a moving camera or when an object moves in a robot’s hands (29). In comparison to developing vision algorithms, much less effort has been devoted to analyzing haptic informa- tion, given that solutions for covering entire hands with haptic sensors are still lacking. To- day, visual and haptic information are still used primarily in a sequential manner [e.g., with vis- ual information provided in the preparation phase and haptic data provided upon contact (30)], and only a few recent works integrate both modalities for recognition, grasping, in-hand adaptation, and shape reconstruction (31–35). In comparison, humans are proficient at alter- nating between different senses, from vision to touch and back, and can do so rapidly even if these senses change in processing frequency. By contrast, robots still lack the ability to decide what sensors to use, when to use them, and when to switch between sensors. Grasping: A stepping stone Before a robot can manipulate an object in hand, it must be able to grasp its fingers around the object. If grasping is conceptualized only as get- ting fingers around an object with no additional constraints considered, the challenge of grasping may appear to be solved. However, grasping an object is a far more daunting problem. For dec- ades, researchers have worked to establish the theory of how to form a stable grasp. This be- came an intricate mathematical exercise aimed at determining the minimal number and optimal positions of the fingertips on the object’ssurface to ensure stability (36). Although it is valuable, most of this theoretical work relies on assumptions such as a known 3D model of the object, a rigid point contact, and no uncertainty in the process. To incorporate uncer- tainty originating from imperfect object models and dynamics in the interaction process, we must go beyond modeling a single point contact and pursue substantial advances in the basic theory. Thus, many of the more recent approaches are data driven (37). To avoid computing an op- timal grasp each time a robot encounters an object, one can build a database of grasps and employ methodologies for sampling and rank- ing candidate grasps in real time. This approach deals with uncertainty in perception and pro- vides fast and online generation of grasps for known, familiar, and even unknown objects. Prior knowledge of object properties determines the necessary perceptual processing and associated object representations for generating and rank- ing grasp candidates. Although this method works well for known and familiar objects, un- known objects necessitate additional heuristics for the discovery of geometric structures (e.g., handles, for which a robot would have a can- didate grasp). This challenge is closely related to the classical problems of instance recognition and categorization in computer vision, but the notion that grasping is not an isolated process adds a new dimension. In addition to being object dependent, grasps are also robot dependent. Moreover, as the num- ber of degrees of freedom of the hand increases, so does the complexity of the control. This is particularly an issue for anthropomorphic hands. One avenue of research to simplify the control draws inspiration from biology and promotes the use of postural synergies (38). Synergies form a basis of the subspace of effective human move- mentsinrelationtothose that arepossiblebythe kinematics of the body. These have been used as a tool for robot hand analysis, control, and de- sign choice (39–42). Several studies have also demonstrated how underactuated hands can be leveraged to grasp and manipulate objects in unstructured environments and how this work may lead to adaptive hands that are relatively cheap, lightweight, and easy to control compared with fully actuated hands (43–48). More recent work has optimized hand design to improve manipulation capabilities (49, 50), providing open-source software for such design. Other recent work has suggested that the ability of compliant hands to deform in and with the environment may reduce the cognitive load of manipulation (51). Furthermore, this idea can be studied systematically using morphological computation (52), in which compliant interac- tions allow adaptation of behavior to a particular context, without the need for explicit control. From grasping to manipulation Grasping is not an end on its own; it is also rel- ated to the task a human or robot is executing. For example, one grasps a cup differently de- pending on whether the goal is to drink from it, fill it with fluid, put it in a dishwasher, or serve it to another person (53) (Fig. 5). Similarly, al- though a knife, fork, or spoon may be held with the same grasp when used to mix soup, this grasp differs from those employed when these utensils are used for eating or cutting. To de- termine the optimal way to grasp an object, one must understand the purpose of the grasp. Hence, while roboticists aimed to solve the prob- lem of how to grasp an object, they first had to identify the reason for executing the grasp. To- day, researchers consider grasping as part of an overall plan for object manipulation. To determine the correct grasp to use with the correct tool, one must first have the correct tool at their disposal. When in need of a hammer but no hammer is in reach, a human will instead select the first object sturdy enough to act as a hammer. Future efforts to develop robots that can reason in this manner when the most ap- propriatetoolisnot availablewillbecritical to facilitate deployment of robots in natural envi- ronments. Additionally, robots with this capabil- ity will be able to use tools originally designed for human dexterity to perform household tasks without making undesired modifications to our households. How to program such “common sense” tool use is hence an important avenue for research, and some initial work has been conducted in this direction (54–57). Manipulations that remain difficult The previous sections detail the many problems that remain to be solved before robots can per- form grasps with a human level of intelligence. This said, robots are already fairly efficient at grasping and releasing certain types of objects. They are also capable of performing a variety of simple manipulation actions such as throwing (58), sliding (59), poking (60), pivoting (61), and pushing (62). Difficulties arise when these ac- tions must be performed in cluttered environ- ments or require contact-rich interactions (e.g., when an object of interest is placed close to or Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 4of8 Fig. 5. Grasp functionality. (Top)A human grasps an item differently depending on whether the aim is to hold it, open the cap, or hand it to someone else. (Bottom) Robots can also be programmed to hold the same glass differently depending on whether they are tasked with handing it to a human or pouring out its contents. [Photos: Learning Algorithms and Systems Laboratory, EPFL] RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from is covered by other objects or is located in a confined space such as a shelving unit). It is ne- cessary to plan a feasible path and generate a set of intermediate actions to ensure no damage to the hand or other objects. Today, it is also recog- nized that perception and control are tightly coupled, and the field of interactive perception (63) regards manipulation as a means to perceive and perception as a means to achieve better manipulation. Manipulation actions that generate changes on the object (cutting, crushing) remain partic- ularly difficult, as they require a model of the deformation and advanced perception to moni- tor the alterations (64). To facilitate adaptation to the changes induced, these actions also neces- sitate that the forces be applied by the hand (e.g., a reduction of friction when unscrewing a bottle cap, an increase in viscosity when digging into a melon) (Fig. 6) (65). Thus, modeling an object’s friction and viscosity properties is still an im- portant open problem. In-hand manipulations in which an object is moved while being held are also particularly complicated. Examples include twirling a pen across fingers or preparing a key to be inserted into a keyhole. These actions comprise an exten- sive combination of (re)grasping movements and sliding and rotating maneuvers, as well as interactions between two arms and hands, in some cases. When discussing such advanced interac- tions with objects in robotics,wecommonlytalk about intrinsic and extrinsic dexterity. The former denotes the ability of the hand to manipulate objects using its available degrees of freedom. Hands with high intrinsic dexterity often mimic the structure of the human hand (66). Alterna- tively, the hand can be simpler, and the end- effector is designed specifically for a particular task (67, 68). Extrinsic dexterity is the ability to compensate for the lack of degrees of freedom by using external support, such as friction, gravity, and contact surfaces (69). This functionality also enables dexterous manipulation with simple pa- rallel grippers. One of the largely underdeveloped areas in robotics is dual-arm or bimanual (70) manip- ulation, as well as the use of the second hand and/or arm to support both intrinsic and ex- trinsic dexterity (71). Some recent work in this area (72) proposes integration of object repre- sentation, definition of simple movement pri- mitives, and planning to model the problem in an efficient manner. This area will gradually produce more and more contributions in the future, given that most of today’s humanoid ro- bots have bimanual capabilities. Furthermore, manipulation does not stop at simply controlling the hand; it requires control of the arm, torso, and ultimately the entire body (73). The chal- lenges listed above only increase in scale when one wishes to enable a full humanoid robot to manipulate objects while maintaining its balance (74) (Fig. 7). Finally, control of more-complex manipulation skills that require reasoning, such as using an object to retrieve another object, are still in infancy. Learning for manipulation Human dexterity is a skill acquired during child- hood and further refined throughout life, in activities such as playing a musical instrument or practicing a craft. Similarly, robot dexterity cannot be achieved in the confines of our labor- atories. To be able to manipulate the vast array of objects that exist throughout the world, robots must be able to learn continuously, adapt their perception, and control unfamiliar objects. Learning also addresses some challenges linked to the lack of accurate models of objects and contact dynamics and the increasing complexity of control for robots with large degrees of free- dom. Hence, many of the present approaches to dexterous manipulation rely on learning meth- odologies in place of control-theoretic approaches. For instance, learning can be used to embed rep- resentations of stable or suitable grasps (75–78), which can then be applied to verify stability and generate regrasping motions at run time or to catch a fast-moving object (79). Learning is par- ticularly suitable for embedding the dynamic nature of grasping and manipulation, as well as for modeling manipulation of complex non- rigid objects. Learning has been used to model contacts (80) and is also beneficial for reducing control dimensions by determining latent space, as required in bimanual dynamics (65). Nevertheless, solving all problems by solely relying on learning is not a viable solution and has certain limitations. First, learning requires data for training, and a common approach is to generate data from trial-and-error experiments. However, this process is tedious and may dam- age the robot. A growing trend in providing training data is to test the algorithms in sim- ulation first and then refine the learning on a real platform; e.g., for learning dexterous in-hand manipulation (81–83). Training in simulation de- pends on having an accurate simulator of the tasks. Alternatively, robots may learn from image data and videos available on the internet (84)or from demonstration by a live expert, usually a human. Yet itmay notalwaysbepossibletofind an expert, especially when the tasks are danger- ous or require extreme precision. Hence, although learning is important, it cannot be the answer to every problem in robotics. Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 5of 8 Fig. 6. Remaining challenges for robot manipulation. Dexterous movement of objects within the hand (left), manipulation of deformable objects (e.g., fruits and vegetables) (65)(middle), and manipulation of objects in collaboration with humans (right) present ongoing difficulties. [Photos: Learning Algorithms and Systems Laboratory, EPFL] Fig. 7. Whole-body manipulation. Manipulation of a heavy object by a humanoid robot requires coordination of arms and body to maintain balance. [Reproduced from (74)] RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from Manipulating objects in interaction and collaboration with humans: Reality and challenges Human–robot collaboration in manufacturing setups has been deemed crucial for the industry (85, 86). Although historically, humans were pro- hibited from entering the robot’s environment (ISO 10218; ANSI/RIA R15.06-1999), it is now accepted that robots can work in close proximity to and in collaboration with humans. However, potentially dangerous scenarios may still occur and need to be addressed. Currently, human– robot collaboration is allowed through the use of manipulators that remain fairly light and are endowed with internal force sensors for detect- ing unexpected contact or collision with humans. For applications that require maneuvering heavy weight, robots capable of managing the weight may be coupled with an external vision system for monitoring human presence. Yet challenges remain for accurately detecting human pres- ence. At present, the best solution is to com- bine sensing of proximity and force with external vision-based monitoring. Nonetheless, the 100% fence-based safety paradigm is gone, and indus- trial standards now target risk minimization and mitigation (ISO/TS 15066). In addition to facing a world in which objects move and change, robots are now expected to manipulate these objects in collaboration with humans. Interactive and collaborative manipu- lation adds a new dimension to robot manipu- lation but presents a wealth of challenges (87). For example, when a robot is tasked with hand- ing an object to a person or carrying a large object jointly with someone, the robot must grasp and move the object carefully and with foresight, so that the robot can infer where the human will move and the human does not get injured. As simple as it may seem, the act of a robot handing an object to a human entails several complex questions, which have in turn inspired studies on how to enable a robot to properly perform this task (88–92). These ques- tions range from how to present an object for optimal human grasping to others related to social factors, such as the role of gaze, social cues, and awareness of user state. There is no agreement on what factors are most important in determining how handovers are carried out between two humans, let alone between a robot and a human. Although most research has foc- used on a robot handing objects to humans, there have also been studies on robots taking objectsfromhumans(93–95). Additionally, sev- eral efforts have been aimed at enabling a robot to manipulate objects jointly with humans, and the act of carrying objects jointly with humans has been demonstrated using both humanoid ro- bots (96–99)and mobile manipulators (100–103). Notable recent efforts have explored human– robot joint manipulation of deformable mate- rials (104), helping humans to dress (105), and assistive support (106). Hence, for robots to work seamlessly with humans, researchers are striving to equip robots with the tools for better perception of humans and more adaptive control modes. In addition, roboticists seek guarantees in terms of machine performance and the use of common evaluation scenarios and benchmarks. Outlook Since the 1960s, substantial progress has been achieved in several areas of robotic manipu- lation. We have established the basic theory of evaluating stability of a grasp, control algo- rithms that can adapt to unpredicted situations, and changing dynamics when the appropriate sensor feedback is available to perform state es- timation. Lately, the field has also seen advances in data-driven approaches in which even dexter- ous in-hand manipulation can be accomplished, but only for very specific problems and in highly tailored environments. Achievement of robust, flexible, and adaptive grasping and manipulation of completely unknown objects in media such as water and oil (not solely in air) is expected to result in a major manufacturing revolution, which will affect most of the work that relies on fine manipulation and high dexterity. However, sys- tematic development is ongoing toward several technologies vital for meeting and exceeding hu- man dexterity and fine manipulation capabilities. First, there is still a need for basic theoretical development. We must seek to understand and model soft point contact and provide stability rules for both point contacts and surface con- tacts. We also need to develop a better method for modeling objects whose states change mark- edly after manipulation (e.g., a cucumber after being sliced, an onion after being chopped). A thorough description of manipulation and task goals will be required for planning and gener- ating appropriate intermediate grasping and manipulation actions. This emphasis on theory and planning is also relevant for data-driven approaches, as we need better tools for sim- ulating soft bodies and generating relevant scenarios and examples that include force and torque information. In addition to the aforementioned modeling and software aspects, we also seek to achieve substantial progress in hardware development and design. One area of particular relevance is that of robot sensing. It will be important to develop skinlike sensors that are well integ- rated with hand design but do not require ex- cessive cabling or add substantial weight. This sensing functionality should facilitate force and torque measurements, determining shear forces to detect and counteract slippage. To achieve dexterous in-hand manipulation, we also need actuated hands that can be controlled with high frequencies. Such hands must function in different media (air, water, and oil) without being damaged or needing to be covered by special gloves. Overall, we need hands that are light, cheap, robust, and easily integrated with anytypeofrobotic arm. Finally, an important industrial challenge will be to bring robots in closer proximity to humans and enable safe physical interaction and collab- oration. Fences that used to separate humans from robots will disappear gradually. Robots will thus need to be engaged in collaborative tasks to jointly manipulate objects with humans while adapting to unexpected human behavior. Equipping robots with advanced physical inter- action capabilities to achieve safe and smooth synchronization of motion between machine and human is still a major hurdle. This objective will require advances in detailed tracking of human fine body movement, as well as a better under- standing of how humans collaborate and achieve joint goals through planning and direct physical interaction. Furthermore, there is a demand for robots that are safe by design, putting focus toward soft and lightweight structures as well as control and planning methodologies based on multisensory feedback. Human ways of acting will continue to serve as inspiration for future robot systems, and robots will serve as a tool for better understanding humans. REFERENCES AND NOTES 1. J. P. Thibaut, L. Toussaint, Developing motor planning over ages. J. Exp. Child Psychol. 105, 116–129 (2010). doi: 10.1016/j.jecp.2009.10.003; pmid: 19919864 2. H. Hanafusa, H. Asada, Stable prehension by a robot hand with elastic fingers. T. Soc. Instr. Control Eng. 13, 370–377 (1977). doi: 10.9746/sicetr1965.13.370 3. A. Bicchi, Hands for dexterous manipulation and robust grasping: A difficult road toward simplicity. IEEE Trans. Robot. Autom. 16,652–662 (2000). doi: 10.1109/ 70.897777 4. R. A. Grupen, T. C. Henderson, I. D. McCammon, A survey of general-purpose manipulation. Int. J. Robot. Res. 8,38–62 (1989). doi: 10.1177/027836498900800103 5. A. M. Okamura, N. Smaby, M. R. Cutkosky, “An overview of dexterous manipulation” in Proceedings of the 2000 IEEE International Conference on Robotics and Automation (IEEE, 2000), pp. 255–262. 6. T. Feix, J. Romero, C. H. Ek, H. B. Schmiedmayer, D. Kragic, A metric for comparing the anthropomorphic motion capability of artificial hands. IEEE Trans. Robot. 29,82–93 (2013). doi: 10.1109/TRO.2012.2217675 7. M. Chalon, M. Grebenstein, T. Wimböck, G. Hirzinger, “The thumb: Guidelines for a robotic design” in Proceedings of the 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2010), pp. 5886–5893. 8. L. Y. Chang, Y. Matsuoka, “A kinematic thumb model for the ACT hand” in Proceedings of the 2006 IEEE International Conference on Robotics and Automation (IEEE, 2006), pp. 1000–1005. 9. Y. Li, I. Kao, “A review of modeling of soft-contact fingers and stiffness control for dextrous manipulation in robotics” in Proceedings of IEEE International Conference on Robotics and Automation (IEEE, 2001), pp. 3055–3060. 10. B. S. Homberg, R. K. Katzschmann, M. R. Dogar, D. Rus, “Haptic identification of objects using a modular soft robotic gripper” in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2015), 1698–1705. 11. A. Firouzeh, J. Paik, Grasp mode and compliance control of an underactuated origami gripper using adjustable stiffness joints. IEEE/ASME T. Mech. 22, 2165–2173 (2017). doi: 10.1109/TMECH.2017.2732827 12. P. Polygerinos, Z. Wang, K. C. Galloway, R. J. Wood, C. J. Walsh, Soft robotic glove for combined assistance and at-home rehabilitation. Robot. Auton. Syst. 73, 135–143 (2015). doi: 10.1016/j.robot.2014.08.014 13. R. Deimel, O. Brock, A novel type of compliant and underactuated robotic hand for dexterous grasping. Int. J. Robot. Res. 35, 161–185 (2016). doi: 10.1177/ 0278364915592961 14. A. Bicchi, J. K. Salisbury, D. L. Brock, Contact sensing from force measurements. Int. J. Robot. Res. 12, 249–262 (1993). doi: 10.1177/027836499301200304 15. L. D. Harmon, Automated tactile sensing. Int. J. Robot. Res. 1, 3–32 (1982). doi: 10.1177/027836498200100201 Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 6of 8 RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from 16. H. R. Nicholls, M. H. Lee, A survey of robot tactile sensing technology. Int. J. Robot. Res. 8,3–30 (1989). doi: 10.1177/ 027836498900800301 17. N. Sommer, A. Billard, Multi-contact haptic exploration and grasping with tactile sensors. Robot. Auton. Syst. 85, 48–61 (2016). doi: 10.1016/j.robot.2016.08.007 18. A. P. Gerratt, N. Sommer, S. Lacour, A. Billard, “Stretchable capacitive tactile skin on humanoid robot fingers - First experiments and results” in Proceedings of the 2014 IEEE International Conference on Humanoid Robots (IEEE, 2014), pp. 238–245. 19. M. Amjadi, K. U. Kyung, I. Park, M. Sitti, Stretchable, skin‐mountable, and wearable strain sensors and their potential applications: A review. Adv. Funct. Mater. 26, 1678–1698 (2016). doi: 10.1002/adfm.201504755 20. S. Xu et al., Soft microfluidic assemblies of sensors, circuits, and radios for the skin. Science 344,70–74 (2014). doi: 10.1126/science.1250169; pmid: 24700852 21. T. H. Pham, N. Kyriazis, A. A. Argyros, A. Kheddar, Hand-object contact force estimation from markerless visual tracking. IEEE Trans. Pattern Anal. Mach. Intell. 40, 2883–2896 (2018). doi: 10.1109/TPAMI.2017.2759736; pmid: 29989962 22. W. Yuan, S. Dong, E. H. Adelson, Gelsight: High-resolution robot tactile sensors for estimating geometry and force. Sensors 17, 2762 (2017). doi: 10.3390/s17122762; pmid: 29186053 23. W. Chen, H. Khamis, I. Birznieks, N. F. Lepora, S. J. Redmond, Tactile sensors for friction estimation and incipient slip detection - towards dexterous robotic manipulation: A review. IEEE Sens. J. 18, 9049–9064 (2018). doi: 10.1109/ JSEN.2018.2868340 24. D. Bhowmik, K. Appiah, “Embedded vision systems: A review of the literature” in Applied Reconfigurable Computing. Architectures, Tools, and Applications, N. Voros et al., Eds. (Lecture Notes in Computer Science Series, Springer, 2018), pp. 204–216. 25. M. R. Cutkosky, “Reach, grasp, and manipulate” in Living Machines: A Handbook of Research in Biomimetics and Biohybrid Systems, T. J. Prescott, N. Lepora, P. F. M. J. Verschure, Eds. (Oxford Univ. Press, 2018), chap. 30. 26. N. Vahrenkamp, L. Westkamp, N. Yamanobe, E. E. Aksoy, T. Asfour, “Part-based grasp planning for familiar objects” in Proceedings of the IEEE/RAS 16th International Conference on Humanoid Robots (IEEE, 2016), pp. 919–925. 27. S. Clarke, T. Rhodes, C. G. Atkeson, O. Kroemer, “Learning audio feedback for estimating amount and flow of granular material” in Proceedings of the 2nd Conference on Robot Learning (Microtome Publishing, 2018), pp 529–550. 28. Y. Guo, M. Bennamoun, F. Sohel, M. Lu, J. Wan, 3D object recognition in cluttered scenes with local surface features: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 36, 2270–2287 (2014). doi: 10.1109/TPAMI.2014.2316828; pmid: 26353066 29. S. Ojha, S. Sakhare, “Image processing techniques for object tracking in video surveillance-A survey” in 2015 IEEE International Conference on Pervasive Computing (IEEE, 2015), pp 1–6. 30. R. S. Dahiya, G. Metta, G. Cannata, M. Valle, Special issue on robotic sense of touch. IEEE Trans. Robot. 27, 385–388 (2011). doi: 10.1109/TRO.2011.2155830 31. J. Ilonen, J. Bohg, V. Kyrki, “Fusing visual and tactile sensing for 3-D object reconstruction while grasping” in Proceedings of the 2013 IEEE International Conference on Robotics and Automation (IEEE, 2013), pp. 3547–3554. 32. H. Dang, P. K. Allen, “Grasp adjustment on novel objects using tactile experience from similar local geometry” in Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2013), pp. 4007–4012. 33. M. Li, Y. Bekiroglu, D. Kragic, A. Billard, “Learning of grasp adaptation through experience and tactile sensing” in Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2014), pp. 3339–3346. 34. K. Hang et al., Hierarchical fingertip space: A unified framework for grasp planning and in-hand grasp adaptation. IEEE Trans. Robot. 32, 960–972 (2016). doi: 10.1109/ TRO.2016.2588879 35. M. M. Zhang, R. Detry, L. Matthies, K. Daniilidis, “Tactile- vision integration for task-compatible fine-part manipulation” in Robotics: Science and Systems (RSS) Workshop on Revisiting Contact - Turning a Problem into a Solution (2017); www.seas.upenn.edu/~zmen/MabelZhang_rss2017ws.pdf. 36. A. Bicchi, V. Kumar, “Robotic grasping and contact: A review” in Proceedings of the 2000 IEEE International Conference on Robotics and Automation (IEEE, 2000), pp. 348–353. 37. J. Bohg, A. Morales, T. Asfour, D. Kragic, Data-driven grasp synthesis-A survey. IEEE Trans. Robot. 30, 289–309 (2014). doi: 10.1109/TRO.2013.2289018 38. M. L. Latash, Fundamentals of Motor Control (Academic Press, 2012). 39. A. Bicchi, M. Gabiccini, M. Santello, Modelling natural and artificial hands with synergies. Philos. Trans. R. Soc. B 366, 3153–3161 (2011). doi: 10.1098/rstb.2011.0152; pmid: 21969697 40. C. Y. Brown, H. H. Asada, “Inter-finger coordination and postural synergies in robot hands via mechanical implementation of principal components analysis” in Proceedings of the 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2007), pp. 2877–2882. 41. M. Ciocarlie, C. Goldfeder, P. Allen, “Dexterous grasping via eigengrasps: A low-dimensional approach to a high- complexity problem” in Robotics: Science and Systems (RSS), Manipulation Workshop - Sensing and Adapting to the Real World (2007); http://projects.csail.mit.edu/ manipulation/rss07/paper__dexterous_grasping_via_ eigengrasps_a_low_dimensional_approach_to_a_ high_complexity_problem__ciocarlie.pdf. 42. M. G. Catalano et al., Adaptive synergies for the design and control of the Pisa/IIT softhand. Int. J. Robot. Res. 33, 768–782 (2014). doi: 10.1177/0278364913518998 43. L. Birglen, T. Laliberté, C. M. Gosselin, Underactuated Robotic Hands (Springer, ed. 1, 2007), vol. 40. 44. M. Ciocarlie et al., The velo gripper: A versatile single- actuator design for enveloping, parallel and fingertip grasps. Int. J. Robot. Res. 33, 753–767 (2014). doi: 10.1177/ 0278364913519148 45. R. R. Ma, W. G. Bircher, A. M. Dollar, “Toward robust, whole-hand caging manipulation with underactuated hands” in Proceedings of the 2017 IEEE International Conference on Robotics and Automation (IEEE, 2017), pp. 1336–1342. 46. P. Kyberd, A. Clawson, B. Jones, The use of underactuation in prosthetic grasping. Mech. Sci. 2,27–32 (2011). doi: 10.5194/ms-2-27-2011 47. C. Meijneke, G. A. Kragten, M. Wisse, Design and performance assessment of an underactuated hand for industrial applications. Mech. Sci. 2,9–15 (2011). doi: 10.5194/ms-2-9-2011 48. A. M. Dollar, R. D. Howe, The highly adaptive SDM hand: Design and performance evaluation. Int. J. Robot. Res. 29, 585–597 (2010). doi: 10.1177/0278364909360852 49. J. Borràs, A. M. Dollar, Dimensional synthesis of three- fingered robot hands for maximal precision manipulation workspace. Int. J. Robot. Res. 34, 1731–1746 (2015). doi: 10.1177/0278364915608250 50. R. Ma, A. Dollar, Yale openhand project: Optimizing open- source hand designs for ease of fabrication and adoption. IEEE Robot. Autom. Mag. 24,32–40 (2017). doi: 10.1109/ MRA.2016.2639034 51. K. Ghazi-Zahedi, R. Deimel, G. Montúfar, V. Wall, O. Brock, “Morphological computation: the good, the bad, and the ugly” in Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2017), pp. 464–469. 52. R. Pfeifer, J. C. Bongard, How the Body Shapes the Way We Think: A New View of Intelligence (MIT Press, 2006). 53. R. de Souza, S. El-Khoury, J. Santos-Victor, A. Billard, Recognizing the grasp intention from human demonstration. Robot. Auton. Syst. 74, 108–121 (2015). doi: 10.1016/ j.robot.2015.07.006 54. M. Madry, D. Song, D. Kragic, “From object categories to grasp transfer using probabilistic reasoning” in Proceedings of the 2012 IEEE International Conference on Robotics and Automation (IEEE, 2012), pp. 1716–1723. 55. R. Detry, J. Papon, L. Matthies, “Task-oriented grasping with semantic and geometric scene understanding” in Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2017), pp. 3266–3273. 56. M. Kokic, J. A. Stork, J. A. Haustein, D. Kragic, “Affordance detection for task-specific grasping using deep learning” in Proceedings of the 2017 IEEE International Conference on Humanoid Robots (IEEE, 2017), pp. 91–98. 57. A. Nguyen, D. Kanoulas, D. G. Caldwell, N. G. Tsagarakis, “Detecting object affordances with convolutional neural networks” in Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2016), pp. 2765–2770. 58. A. Ghadirzadeh, A. Maki, D. Kragic, M. Björkman, “Deep predictive policy training using reinforcement learning” in Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2017), pp. 2351–2358. 59. J. Shi, J. Z. Woodruff, K. M. Lynch, “Dynamic in-hand sliding manipulation” in Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2015), pp. 870–877. 60. P. Agrawal, A. Nair, P. Abbeel, J. Malik, S. Levine, “Learning to poke by poking: Experiential learning of intuitive physics” in Proceedings of IEEE Conference on Neural Information Processing Systems (IEEE, 2016), pp. 5074–5082. 61. R. Antonova, S. Cruciani, C. Smith, D. Kragic, Reinforcement learning for pivoting task. arxiv:1703.00472 [cs.RO] (1 March 2017). 62. J. Z. Woodruff, K. M. Lynch, “Planning and control for dynamic, non-prehensile, and hybrid manipulation tasks” in Proceedings of the 2017 IEEE International Conference on Robotics and Automation (IEEE, 2017), pp. 4066–4073. 63. J. Bohg et al., Interactive perception: Leveraging action in perception and perception in action. IEEE Trans. Robot. 33, 1273–1291 (2017). doi: 10.1109/TRO.2017.2721939 64. M. Gemici, A. Saxena, “Learning haptic representation for manipulating Deformable food objects” in Proceedings of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2014), pp. 638–645. 65. L. P. Ureche, A. Billard, Constraints extraction from asymmetrical bimanual tasks and their use in coordinated behavior. Robot. Auton. Syst. 103, 222–235 (2018). doi: 10.1016/j.robot.2017.12.011 66. R. Ozawa, K. Tahara, Grasp and dexterous manipulation of multifingered robotic hands: A review from a control view point. Adv. Robot. 31, 1030–1050 (2017). doi: 10.1080/ 01691864.2017.1365011 67. N. Rahman, L. Carbonari, M. D’Imperio, C. Canali, D. G. Caldwell, F. Cannella, “A dexterous gripper for in-hand manipulation” in Proceedings of the 2016 IEEE International Conference on Advanced Intelligent Mechatronics (IEEE, 2016), pp. 377–382. 68. W. G. Bircher, A. M. Dollar, N. Rojas, “A two-fingered robot gripper with large object reorientation range” in Proceedings of the 2017 IEEE International Conference on Robotics and Automation (IEEE, 2017), pp. 3453–3460. 69. N. C. Dafle, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa, M. Erdmann, M. T. Mason, I. Lundberg, H. Staab, T. Fuhlbrigge, “Extrinsic dexterity: In-hand manipulation with external forces” in Proceedings of the 2014 IEEE International Conference on Robotics and Automation (IEEE, 2014), pp. 1578–1585. 70. N. Vahrenkamp, M. Przybylski, T. Asfour, R. Dillmann, “Bimanual grasp planning” in Proceedings of the 2011 IEEE/ RAS International Conference on Humanoid Robots (IEEE, 2011), pp. 493–499. 71. C. Smith et al., Dual arm manipulation-A survey. Robot. Auton. Syst. 60, 1340–1353 (2012). doi: 10.1016/ j.robot.2012.07.005 72. S. Cruciani, C. Smith, D. Kragic, K. Hang, “Dexterous manipulation graphs” in Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2018), pp. 2040–2047. 73. M. T. Mason, Toward robotic manipulation. Annu. Rev. Control Rob. Auton. Syst 1,1–28 (2018). doi: 10.1146/annurev- control-060117-104848 74. K. Bouyarmane, K. Chappellet, J. Vaillant, A. Kheddar, Quadratic programming for multirobot and task-space force control. IEEE Trans. Robot. 35,64–77 (2018). doi: 10.1109/ TRO.2018.2876782 75. O. B. Kroemer, R. Detry, J. Piater, J. Peters, Combining active learning and reactive control for robot grasping. Robot. Auton. Syst. 58, 1105–1116 (2010). doi: 10.1016/ j.robot.2010.06.001 76. Y. Bekiroglu, J. Laaksonen, J. A. Jørgensen, V. Kyrki, D. Kragic, Assessing grasp stability based on learning and haptic data. IEEE Trans. Robot. 27, 616–629 (2011). doi: 10.1109/TRO.2011.2132870 77. R. Detry, C. H. Ek, M. Madry, J. Piater, D. Kragic, “Learning a dictionary of prototypical grasp-predicting parts from grasping experience” in Proceedings of the 2013 IEEE International Conference on Robotics and Automation (IEEE, 2013), pp. 601–608. Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 7of 8 RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from 78. A. Boularias, O. Kroemer, J. Peters, “Learning robot grasping from 3-D images with markov random fields” in Proceedings of the 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2011), pp. 1548–1553. 79. S. Kim, A. Shukla, A. Billard, Catching Objects in Flight. IEEE Trans. Robot. 30, 1049–1065 (2014). doi: 10.1109/ TRO.2014.2316022 80. D. Nguyen-Tuong, J. Peters, Model learning for robot control: A survey. Cogn. Process. 12, 319–340 (2011). doi: 10.1007/ s10339-011-0404-1; pmid: 21487784 81. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, W. Zaremba, Learning dexterous in-hand manipulation. arxiv:1808.00177 [cs.LG] (1 August 2018). 82. H. van Hoof, T. Hermans, G. Neumann, J. Peters, “Learning robot in-hand manipulation with tactile features” in Proceedings of the 2015 IEEE/RAS International Conference on Humanoid Robots (IEEE, 2015), pp 121–127. 83. V. Kumar, E. Todorov, S. Levine, “Optimal control with learned local models: Application to dexterous manipulation” in Proceedings of the 2016 IEEE International Conference on Robotics and Automation (IEEE, 2016), pp. 378–383. 84. Y. Yang, Y. Li, C. Fermüller, Y. Aloimonos, “Robot learning manipulation action plans by “watching” unconstrained videos from the World Wide Web” in Proceedings of the 29th Conference on Artificial Intelligence (AAAI Press, 2015), pp. 3686–3693. 85. J. Krüger, T. K. Lien, A. Verl, Cooperation of human and machines in assembly lines. CIRP Ann. Manuf. Technol. 58, 628–646 (2009). doi: 10.1016/j.cirp.2009.09.009 86. A. Cherubini, R. Passama, A. Crosnier, A. Lasnier, P. Fraisse, Collaborative manufacturing with physical human-robot interaction. Robot. Comput.-Integr. Manuf. 40,1–13 (2016). doi: 10.1016/j.rcim.2015.12.007 87. A. Ajoudani et al., Progress and prospects of the human- robot collaboration. Auton. Robots 42, 957–975 (2018). doi: 10.1007/s10514-017-9677-2 88. M. Huber, A. Knoll, T. Brandt, S. Glasauer, Handing over a cube: Spatial features of physical joint-action. Ann. N. Y. Acad. Sci. 1164, 380–382 (2009). doi: 10.1111/ j.1749-6632.2008.03743.x; pmid: 19645931 89. J. Mainprice, M. Gharbi, T. Simeon, R. Alami, “Sharing effort in planning human-robot handover tasks” in Proceedings of the 21st IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2012), pp. 764–770. 90. C.-M. Huang, M. Cakmak, B. Mutlu, “Adaptive coordination strategies for human-robot handovers” in Proceedings of the 2015 Conference on Robotics: Science and Systems (2015); www.roboticsproceedings.org/rss11/p31.html. 91. M. Cakmak, S. S. Srinivasa, M. K. Lee, S. Kiesler, J. Forlizzi, “Using spatial and temporal contrast for fluent robot- human handovers” in Proceedings of the International Conference on Human-Robot Interaction (IEEE, 2011), p. 489. 92. P. Basili, M. Huber, T. Brandt, S. Hirche, S. Glasauer, “Investigating human-human approach and hand-over” in Human Centered Robot Systems (Cognitive Systems Monographs, Springer, 2009), vol. 6, pp. 151–160. 93. A. Edsinger, C. C. Kemp, “Human-robot interaction for cooperative manipulation: Handing objects to one another” in Proceedings of the 16th IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2007), pp. 1167–1172. 94. J. Aleotti, V. Micelli, S. Caselli, “Comfortable robot to human object hand-over” in Proceedings of the 21st IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2012), pp. 771–776. 95. M. Pan, E. Croft, G. Niemeyer, “Exploration of geometry and forces occurring within human-to-robot handovers” in 2018 IEEE Haptics Symposium (IEEE, 2018), pp. 327–333. 96. K. Yokoyama, H. Handa, T. Isozumi, Y. Fukase, K. Kaneko, F. Kanehiro, Y. Kawai, F. Tomita, H. Hirukawa, “Cooperative works by a human and a humanoid robot” in Proceedings of the 2003 IEEE International Conference on Robotics and Automation (IEEE, 2003), vol. 3, pp. 2985–2991. 97. E. Berger, D. Vogt, N. Haji-Ghassemi, B. Jung, H. B. Amor, “Inferring guidance information in cooperative human-robot tasks” in Proceedings of the 2013 IEEE/RAS International Conference on Humanoid Robots (IEEE, 2013), pp. 124–129. 98. M. Bellaccini, L. Lanari, A. Paolillo, M. Vendittelli, “Manual guidance of humanoid robots without force sensors: Preliminary experiments with NAO” in Proceedings of the 2014 IEEE International Conference on Robotics and Automation (IEEE, 2014), pp. 1184–1189. 99. A. Bussy, P. Gergondet, A. Kheddar, F. Keith, A. Crosnier, “Proactive behavior of a humanoid robot in a haptic transportation task with a human partner” in Proceedings of the 21st IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2012), pp. 962–967. 100. K. Kosuge, M. Sato, N. Kazamura, “Mobile robot helper” in Proceedings of the 2000 IEEE International Conference on Robotics and Automation (IEEE, 2000), vol. 1, pp. 583–588. 101. J. Stuckler, S. Behnke, “Following human guidance to cooperatively carry a large object” in Proceedings of the 2011 IEEE/RAS International Conference on Humanoid Robots (IEEE, 2011), pp. 218–223. 102. M. Lawitzky, A. Mortl, S. Hirche, “Load sharing in human- robot cooperative manipulation” in Proceedings of the 19th IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2010), pp. 185–191. 103. Y. Karayiannidis, C. Smith, D. Kragic, “Mapping human intentions to robot motions via physical interaction through a jointly-held object” in Proceedings of the 23rd IEEE International Symposium on Robot and Human Interactive Communication (IEEE, 2014), pp. 391–397. 104. D. Kruse, R. J. Radke, J. T. Wen, “Collaborative human-robot manipulation of highly deformable materials” in Proceedings of the 2015 IEEE International Conference on Robotics and Automation (IEEE, 2015), pp. 3782–3787. 105. Y. Gao, H. J. Chang, Y. Demiris, “Iterative path optimization for personalized dressing assistance using vision and force information” in Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE, 2016), pp. 4398–4403. 106. L. Lu, J. T. Wen, Human-directed coordinated control of an assistive mobile manipulator. Int. J. Intell. Robot. Appl. 1, 104–120 (2017). doi: 10.1007/s41315-016-0005-3 AC KNOWLED GME NTS We thank the reviewers for many helpful comments to improve the article, A. Kheddar and J. Paik for providing images of their research, and L. Cohen for hand-drawn illustrations. Funding: We acknowledge funding from the European Research Council, the Knut and Alice Wallenberg Foundation, and the Swedish Foundation for Strategic Research. Competing interests: The authors declare no competing interests. 10.1126/science.aat8414 Billard and Kragic, Science 364, eaat8414 (2019) 21 June 2019 8of 8 RESEARCH | REV IEWon August 8, 2020 http://science.sciencemag.org/Downloaded from Trends and challenges in robot manipulation Aude Billard and Danica Kragic DOI: 10.1126/science.aat8414 (6446), eaat8414.364Science , this issue p. eaat8414Science feedback to the robot. are emerging from advances in computer vision, computer processing capabilities, and tactile materials that give environment, to robots that can identify, select, and manipulate objects from a random collection. Further developments robotics to emulate these functions. Systems have developed from simple, pinching grippers operating in a fully defined from our eyes and muscles that allows us to maintain a controlled grip. Billard and Kragic review the progress made in Our ability to grab, hold, and manipulate objects involves our dexterous hands, our sense of touch, and feedback Hand it to you ARTICLE TOOLS http://science.sciencemag.org/content/364/6446/eaat8414 REFERENCES http://science.sciencemag.org/content/364/6446/eaat8414#BIBL This article cites 45 articles, 1 of which you can access for free PERMISSIONS http://www.sciencemag.org/help/reprints-and-permissions Terms of ServiceUse of this article is subject to the is a registered trademark of AAAS.ScienceScience, 1200 New York Avenue NW, Washington, DC 20005. The title (print ISSN 0036-8075; online ISSN 1095-9203) is published by the American Association for the Advancement ofScience Science. No claim to original U.S. Government Works Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement ofon August 8, 2020 http://science.sciencemag.org/Downloaded from","libVersion":"0.3.2","langs":""}