{"path":"_aula_virtual/SJK003/12-artificialNeuralNetwork.pdf","text":"Ramón A. Mollineda Cárdenas artificial neural networks MACHINE LEARNING University Master's Degree in Intelligent Systems fuel of the future “AN OIL refinery... Data centres... the two have much in common. For one thing, both are stuffed with pipes. In refineries these collect petrol, propane and other components of crude oil ... In big data centres ... tens of thousands of computers ... extract value —patterns, predictions and other insights— from raw digital information.” The Economist, May 6th 2017 https://www.economist.com/briefing/2017/05/06/data-is-giving-rise-to-a-new-economy. 2 deep learning deep learning (refinery, distillery) massive raw data (big data, raw material) concepts, patterns, predictions, manifolds, etc. (product with practical value) 010010110101001010101001010001001100010011 100100100100110101101001001001110010101000 110010000011010101110100100110010001010001 111011001001110011001100100010010110010001 001100010000100110100010100101111001001010 111000001000010000000100101111111111000101 110101000101010001000001010101000101000100 111010010100101111001001001100101001101000 001001101001010100010000100011111000101010 101010101001001110001000011110010101010101 𝑓𝜃 𝑥 3Fuente: ARGILITY. https://www.argility.com/argility-ecosystem-solutions/industry-4-0/machine-learning-deep-learning/. deep learning some context 4 deep learning artificial neural networks outperform humans o object and image recognition (ILSVRC, ¿aceptas el reto?) o imitation of art styles (DeepArt.io, transferencia de estilo) o medical diagnosis (DL 95% - senior doctors 87% in the diagnosis of malignant melanomas -distinguishing them from benign moles-) o computer gaming (modelos que aprenden, AlphaGo vs. Lee Sedol) o lipreading (LipNet 95% - humanos 52% according to grammar “command(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4)”) o big data analytics (demografía a partir de Google Street View). “Using deep learning …, we determined the make, model, and year of all motor vehicles encountered .... Data from this census of motor vehicles, which enumerated 22M automobiles in total (8% of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns …” (Gebru et al. 2017) Ver más en Roman Steinberg (2017), 6 areas where artificial neural networks outperform humans, VentureBeat (ir). 5applications to summarize texts, generate responses (chatbots), write stories, etc. news deep learning natural language generation models OpenAI GPT-3 175 billion parameters 6 GAURAV CHAUHAN, Iris Dataset Project from UCI Machine Learning Repository, machinelearninghd.com, 2021. (enlace) IRIS Data Set classes 7 • most popular database in machine learning • 150 examples distributed among 3 classes : – Iris Versicolor (50) – Iris Setosa (50) – Iris Virginica (50) • examples described by 4 measurements (in cm): – sepal length – sepal width – petal length – petal width IRIS Data Set description 8https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Iris_dataset_scatterplot.svg/2000px-Iris_dataset_scatterplot.svg.png IRIS Data Set distributions by classes in 2D subspaces 9 Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Annual Eugenics, 7, Part II, 179-188 (acceder). IRIS Data Set original paper 10 linear model a fully connected neural network 𝑓 is a composition of nonlinear transformations of linear models (linear combinations of features) 𝑓 𝑥 𝑤𝑘, … 𝑤2, 𝑤1 = 𝑔∗ 𝑤𝑘 𝑡𝑔𝑘−1 ⋯ 𝑔2 𝑤2 𝑡𝑔1 𝑤1 𝑡𝑥 being... • 𝑥, the input vector to the network • 𝑘, the number of layers (or transformations) • 𝑤𝑖, weight matrix of the layer 𝑖, 1 ≤ 𝑖 ≤ 𝑘 • 𝑔𝑖, nonlinear activation function of the layer 𝑖 (they are generally the same) • 𝑔∗, output layer activation function (can be the identity function) • 𝑓, network function composed from chains of linear and nonlinear functions 11 linear model a fully connected neural network 𝑓 is a composition of nonlinear transformations of linear models (linear combinations of features) 𝑓 𝑥 𝑤𝑘, … 𝑤2, 𝑤1 = 𝑔∗ 𝑤𝑘 𝑡𝑔𝑘−1 ⋯ 𝑔2 𝑤2 𝑡𝑔1 𝑤1 𝑡𝑥 linear model being... • 𝑥, the input vector to the network • 𝑘, the number of layers (or transformations) • 𝑤𝑖, weight matrix of the layer 𝑖, 1 ≤ 𝑖 ≤ 𝑘 • 𝑔𝑖, nonlinear activation function of the layer 𝑖 (they are generally the same) • 𝑔∗, output layer activation function (can be the identity function) • 𝑓, network function composed from chains of linear and nonlinear functions 12 linear model classes Setosa (red) and Versicolor (gray) 𝑧 = 3𝑥1 − 5𝑥2 − 0,6 𝑥1 𝑥2 𝑧 < 0 𝑧 > 0 error 𝑑 = 0.25 𝑧 = 0 𝑑 = 1.21 𝑑 = 0.09 𝑑 = −0.92 𝑑 = 𝑧 𝑤 2 𝑑 , distance to the plane 13 𝑧 = 𝑤𝑇𝑥 + 𝑏 where: w, weight vector x, input data vector b, scalar; bias, threshold z, scalar; affine transformation output linear model affine/linear transformation Σ... 𝑧 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑏 𝑧 = σ𝑖=1 𝑑 𝑤𝑖𝑥𝑖 + 𝑏 -6 -4 -2 0 2 4 6 -3 -2 -1 0 1 2 3 𝑧 < 0 𝑧 > 0 𝑧 = 0 𝑧 = 2𝑥1 − 𝑥2 + 1 linear division of space 14 𝑧 = 𝑤𝑇𝑥 where: w, weight vector (𝑤0, bias) x, input data vector z, scalar; affine transformation output 𝑧 = σ𝑖=0 𝑑 𝑤𝑖𝑥𝑖 Σ... 𝑧 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 linear model affine/linear transformation: compact notation 15 where: z, scalar; affine transformation output g, nonlinear activation function h, scalar; nonlinear function output 𝑔 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 nonlinear activation function linear function linear model activation function 16 where: z, affine transformation output g, step function (derivative 0 at all points) ℎ ∈ 0,1 linear model Perceptron (Frank Rosenblatt, 1958) ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 step function 0 0,5 1 -3 -2 -1 0 1 2 3 𝑔 𝑧 = ቊ 1, si 𝑧 ≥ 0 0, si 𝑧 < 0 17 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 𝑧 < 0 𝑔 𝑧 = 0 𝑧 = 2𝑥1 − 𝑥2 + 1 𝑧 ≥ 0 𝑔 𝑧 = 1 linear model Perceptron (Frank Rosenblatt, 1958) where: z, affine transformation output g, step function (derivative 0 at all points) ℎ ∈ 0,1 step function 18 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 sigmoid 𝑔 𝑧 = 1 1 + 𝑒−𝑧 0 0,5 1 -3 -2 -1 0 1 2 3 linear model logistic regression: a linear decision boundary logistic regression where: z, affine transformation output g, sigmoid function (derivative ≠ 0 at all points) ℎ ∈ 0,1 ℎ = 𝑃𝑟 𝑦 = 1 𝑥, 𝑤 Why is logistic regression a linear classifier? (link) 19 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 sigmoid 𝑧 < 0 0 < 𝑔 𝑧 < 0.5 𝑧 = 2𝑥1 − 𝑥2 + 1 𝑧 > 0 0.5 < 𝑔 𝑧 < 1 linear model logistic regression: a linear decision boundary where: z, affine transformation output g, sigmoid function (derivative ≠ 0 at all points) ℎ ∈ 0,1 ℎ = 𝑃𝑟 𝑦 = 1 𝑥, 𝑤 20 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 𝑔 𝑧 = 𝑚𝑎𝑥 0, 𝑚𝑖𝑛 1, 𝑥 + 1 2 0 0,5 1 -3 -2 -1 0 1 2 3 rigid sigmoid linear model approximate (or rigid) logistic regression where: z, affine transformation output g, rigid sigmoid function ℎ ∈ 0,1 ℎ = 𝑃𝑟 𝑦 = 1 𝑥, 𝑤 21 ℎΣ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 ℎ = 𝑔 𝑧 = 𝑔 𝑤𝑇𝑥 𝑧 < 0 0 < 𝑔 𝑧 < 0.5 𝑧 = 2𝑥1 − 𝑥2 + 1 𝑧 > 0 0.5 < 𝑔 𝑧 < 1 rigid sigmoid linear model approximate (or rigid) logistic regression where: z, affine transformation output g, rigid sigmoid function ℎ ∈ 0,1 ℎ = 𝑃𝑟 𝑦 = 1 𝑥, 𝑤 22 iris = sklearn.datasets.load_iris() X = iris.data[:, :2] # first two features (sepal width, sepal length) y = iris.target X = (X-numpy.mean(X, axis=0))/numpy.std(X, axis=0) # data standardization model = keras.models.Sequential() model.add(keras.layers.Dense(1, input_shape=(2,), activation=keras.activations.hard_sigmoid)) opt = keras.optimizers.SGD(learning_rate = 0.1, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) history = model.fit(X, y, epochs=10, batch_size=10, verbose=1, validation_split=0.2) script simplificado usando numpy, sklearn y keras: linear model approximate perceptron on Setosa and Versicolor classes See: Understanding binary cross-entropy / log loss: a visual explanation (link). Published in Towards Data Science. 23 linear model approximate perceptron on Setosa and Versicolor classes 24 linear model approximate perceptron on Setosa and Versicolor classes 25 linear model limitations linear classifier logistic regression => “sigmoid” activation function Aryan Kashyap (2017), Neural Networks for Decision Boundary in Python!, Medium. 26 fully connected multilayer networks Σ Σ Σ 𝑔 𝑔 𝑔 Σ Σ 𝑔 𝑔 Σ 𝑥1 𝑥2 ത𝑦 𝑔: función de activación (e.g. sigmoid, tanh, softmax, relu, …) 1 1 1 input layer hidden layer output layerhidden layer 27 Σ Σ Σ 𝑔 𝑔 𝑔 1 … … • cada capa consiste en: función lineal + activación no lineal • función de transformación en capa 𝑙: 𝑧𝑙 = 𝑊𝑙 · 𝐼𝑙 𝐼𝑙+1 = 𝑔 𝑧𝑙 • sea 𝐻𝑙 el número de unidades de la capa 𝑙; el tamaño de 𝑊𝑙 sería … 𝐻𝑙 × 𝐻𝑙−1 • 𝐼𝑙 es de tamaño 𝐻𝑙−1 × 1 capa 𝑙 fully connected multilayer networks 28 multilayer network 3 layers of 12+6+1 units, “relu” and “sigmoid” activation functions fully connected multilayer networks 29 multilayer network 3 layers of 12+6+1 units, “relu” and “sigmoid” activation functions fully connected multilayer networks 30 backpropagation learning goal progressive adjustment (iterative optimization) of weights that minimize the error/loss of the network on a representative set of training samples. potentially, a network with sufficient learning capacity could achieve zero error on the training set; however, it is not usually a desirable goal (overfitting risk). 31 initialize network weights w(0) with small arbitrary values for epoch = 1...K, do for batch = 1...N/batch_size, do batch <– randomly choose batch_size instances X, y <– preprocess(batch) z <– network(X) (forward execution) ℓ <– loss(z, y) g <- gradients(ℓ, w) (backward execution) w(t+1) <- w(t) – γ · g (weight optimization/fitting) end for end for backpropagation training a neural network (mini-batches) backpropagation 32 backpropagation fundamentals: chain rule some examples of composite of two (differentiable) functions 𝑓 𝑔 𝑥 : o 𝑓 𝑔 𝑥 = 2𝑥 + 1 3 o 𝑓 𝑔 𝑥 = sin 𝑥2 o 𝑓 𝑔 𝑥 = sin 𝑥 2 their derivative functions 𝑓′ = 𝑓′ 𝑔 𝑥 · 𝑔′ 𝑥 , or 𝜕𝑓 𝜕𝑥 = 𝜕𝑓 𝜕𝑔 · 𝜕𝑔 𝜕𝑥 o 𝜕𝑓 𝜕𝑥 = 3 2𝑥 + 1 2 · 2 o 𝜕𝑓 𝜕𝑥 = cos 𝑥2 · 2𝑥 o 𝜕𝑓 𝜕𝑥 = 2 · sin 𝑥 · cos 𝑥 33 gradient descent (steepest descent) is an iterative optimization algorithm for finding a local minimum of a differentiable function. strategy • starting from an initial value of the parameter • move “downhill” along the surface of the function, in the direction of the negative gradient, looking for a parameter value that minimizes it • stop when the minimum of the function is reached https://www.jeremyjordan.me/gradient-descent/ https://youtu.be/CsKCT5ezVdI backpropagation fundamentals: gradient descent 34 goal to find 𝜃 = 𝜃∗ such that 𝐿 𝜃∗ is the minimum value of 𝐿 method • let 𝐿 𝜃 be a function defined by the parameter 𝜃 • initialization : 𝜃0 = 𝜃𝑖𝑛𝑖𝑐𝑖𝑎𝑙 • updating: 𝜃𝑖+1 = 𝜃𝑖 − 𝛾 𝜕𝐿 𝜕𝜃 𝜃𝑖 , ─ 𝛾 ≪ 1 is the learning rate; it determines the magnitude of change • repeat as long as 𝐿 𝜃𝑖+1 < 𝐿 𝜃𝑖 • solution: 𝜃∗ = 𝜃𝑖 backpropagation fundamentals: gradient descent 35source: https://www.jeremyjordan.me/nn-learning-rate/ effect of the learning coefficient 𝛾 backpropagation fundamentals: gradient descent 36 𝑤𝑖𝑗 𝐿−2𝑤𝑖𝑗 1 𝑤𝑖𝑗 𝐿−1 𝑤𝑖𝑗 𝐿…𝑥 𝑦 𝐸 ො𝑦, 𝑦 1 𝐿 − 2 𝐿 − 1 𝐿 layer layer layer layer 𝐸 error/loss function ො𝑦 backpropagation solution scheme 37 𝑤𝑖𝑗 𝐿−2𝑤𝑖𝑗 1 𝑤𝑖𝑗 𝐿−1 𝑤𝑖𝑗 𝐿…𝑥 𝑦 𝐸 ො𝑦, 𝑦 1 𝐿 − 2 𝐿 − 1 𝐿 layer layer layer layer 𝐸 ො𝑦 goal: 𝜕𝐸 𝜕𝑤𝑖𝑗 𝑙 for all 𝑤𝑖𝑗 𝑙 backpropagation solution scheme error/loss function 38 1. initialize weights, choose learning coefficient, choose stop criterion 2. create an arbitrary batch (X, y) --> (subset of instances, expected output). 3. forward execution: walk X through the network and get output z. 4. compute loss(y, z) 5. backward execution (backpropagation) o compute sensitivity coefficient 𝛿𝐿 = 𝑓 𝛿𝐿+1 from input to layer L o compute gradients 𝜕𝐸 𝜕𝑤𝑖𝑗 𝐿−1 = 𝑓 𝛿𝐿 6. weight optimization 7. check stopping criteria; if it is fulfilled, then finish; otherwise, go to step 2. backpropagation solution scheme 39 𝑤𝑖𝑗 𝐿−2𝑤𝑖𝑗 1 𝑤𝑖𝑗 𝐿−1 𝑤𝑖𝑗 𝐿…𝑥 𝑦 1 𝐿 − 2 𝐿 − 1 𝐿 layer layer layer layer 𝐸 𝐼𝐿−2 𝐼𝐿−1 𝐼𝐿 forward execution the following pieces of information are calculated: • input 𝐼𝑙 to each layer 𝑙 (matches the output of the layer 𝑙 − 1), • actual network output ො𝑦 • value of the error or loss function 𝐸 ො𝑦, 𝑦 ො𝑦 𝐸 ො𝑦, 𝑦 backpropagation solution scheme 40 𝑤𝑖𝑗 𝐿−2𝑤𝑖𝑗 1 𝑤𝑖𝑗 𝐿−1 𝑤𝑖𝑗 𝐿 𝐸…𝑥 𝑦 𝐼𝐿−2 𝐼𝐿−1 𝐼𝐿 𝛿𝐿−2 𝛿𝐿−1 𝛿𝐿 𝛿𝐿+1 𝛿𝐿+1 = 𝜕𝐸 𝜕 ො𝑦 1 𝐿 − 2 𝐿 − 1 𝐿 layer layer layer layer 𝛿𝐿 = 𝑓 𝛿𝐿+1𝛿𝐿−1 = 𝑓 𝛿𝐿 𝛿𝐿−1 = 𝜕𝐸 𝜕𝐼𝐿−1 𝛿𝐿 = 𝜕𝐸 𝜕𝐼𝐿 𝛿𝐿+1 = 𝜕𝐸 𝜕𝐼𝐿+1 backward execution (calculation of sensitivities) definition recursive computing 𝛿𝑙 measures the sensitivity of 𝐸 to changes in 𝐼𝑙 𝛿𝑙 is computed recursively from 𝛿𝑙+1 𝛿𝑙 allows efficient computation of 𝜕𝐸 𝜕𝑤𝑖𝑗 𝑙−1 ො𝑦 𝐸 ො𝑦, 𝑦 backpropagation solution scheme 41 𝑤𝑖𝑗 𝐿−2𝑤𝑖𝑗 1 𝑤𝑖𝑗 𝐿−1 𝑤𝑖𝑗 𝐿 𝐸…𝑥 𝑦 𝐼𝐿−2 𝐼𝐿−1 𝐼𝐿 𝛿𝐿−2 𝛿𝐿−1 𝛿𝐿 𝜕𝐸 𝜕𝑤𝑖𝑗 𝐿 = 𝑓 𝛿𝐿+1 𝛿𝐿+1 1 𝐿 − 2 𝐿 − 1 𝐿 layer layer layer layer 𝜕𝐸 𝜕𝑤𝑖𝑗 𝐿−1 = 𝑓 𝛿𝐿 𝜕𝐸 𝜕𝑤𝑖𝑗 𝐿−2 = 𝑓 𝛿𝐿−1 𝜕𝐸 𝜕𝑤𝑖𝑗 1 = 𝑓 𝛿2 𝐸 ො𝑦, 𝑦 ො𝑦 backpropagation solution scheme backward execution (calculation of sensitivities) 42 𝜎: activation function (e.g. sigmoid, tanh, relu, etc.) Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝑥1 𝑥2 𝐸 𝑦 ത𝑦 𝐸 ത𝑦, 𝑦 error/loss computation backpropagation case study: fully connected network with 3 layers 43 2 x 3 + 3 + 3 x 2 + 2 + 2 x 1 + 1 = 9 + 8 + 3 = 20 trainable parameters/weights Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝑥1 𝑥2 𝐸 𝑦 ത𝑦 𝐸 ത𝑦, 𝑦 error/loss computation backpropagation case study: fully connected network with 3 layers 44 𝑥1 𝑥2 𝑦 ത𝑦 𝐼1 3 𝐼2 3 ത𝑦 − 𝑦 2 forward execution: introduce 𝑥, 𝑦 and compute 𝐼𝑖 𝑙 and ത𝑦 𝐼1 2 𝐼2 2 𝐼3 2 Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝐸 backpropagation case study: fully connected network with 3 layers error/loss computation 𝐼1 4 45 𝑤11 3 𝑤21 3 𝜕𝐸 𝜕𝑤𝑖𝑗 3 = 𝜕𝐼1 4 𝜕𝑤𝑖𝑗 3 · 𝛿1 4 = 𝜎 𝐼𝑖 3 · 2 ത𝑦 − 𝑦 𝑥1 𝑥2 𝑦 ത𝑦 𝐼1 3 𝐼2 3 ത𝑦 − 𝑦 2 𝐼1 2 𝐼2 2 𝐼3 2 𝐼1 4 Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝐸 backward execution: compute gradients 𝜕𝐸 𝜕𝑤𝑖𝑗 𝑙 error/loss computation backpropagation case study: fully connected network with 3 layers 46 𝜕𝐸 𝜕𝑤𝑖𝑗 2 = 𝜕𝐼𝑗 3 𝜕𝑤𝑖𝑗 2 · 𝛿𝑗 3 = 𝜎 𝐼𝑖 2 · 𝜎′ 𝐼𝑗 3 · 𝑤𝑗1 3 · 𝛿1 4 𝑤𝑖𝑗 2 𝑥1 𝑥2 𝑦 ത𝑦 𝐼1 3 𝐼2 3 ത𝑦 − 𝑦 2 𝐼1 2 𝐼2 2 𝐼3 2 𝐼1 4 Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝐸 𝑤11 3 𝑤21 3 backward execution: compute gradients 𝜕𝐸 𝜕𝑤𝑖𝑗 𝑙 error/loss computation backpropagation case study: fully connected network with 3 layers 47 𝜕𝐸 𝜕𝑤𝑖𝑗 1 = 𝜕𝐼𝑗 2 𝜕𝑤𝑖𝑗 1 · 𝛿𝑗 2 = 𝜎 𝐼𝑖 1 · 𝜎′ 𝐼𝑗 2 σ𝑘=1 2 𝑤𝑗𝑘 2 · 𝛿𝑘 3 𝑤𝑖𝑗 1 𝐼1 1 𝐼2 1 𝐼𝑖 1 = 𝑥𝑖 𝑤𝑖𝑗 2 𝑥1 𝑥2 𝑦 ത𝑦 𝐼1 3 𝐼2 3 ത𝑦 − 𝑦 2 𝐼1 2 𝐼2 2 𝐼3 2 𝐼1 4 Σ Σ Σ σ σ σ Σ Σ σ σ Σ 𝐸 𝑤11 3 𝑤21 3 backward execution: compute gradients 𝜕𝐸 𝜕𝑤𝑖𝑗 𝑙 error/loss computation backpropagation case study: fully connected network with 3 layers 48 case study fully connected neural network for the MNIST digits task implementation in Keras • deep learning framework: Python library for creating neural networks • high-level interface based on Tensorflow, Theano, or the Microsoft Cognitive Toolkit • it allows defining and assembling pieces in neural networks such as layers, objective functions, activation, optimizers, etc. 49 MNIST (Modified National Institute of Standards and Technology database) handwritten digits image collection (official website) Source: Wikipedia (+) • 60,000 training images • 10,000 test images • 10 classes • image size: 28x28 • grey images Source: Medium (+) case study fully connected neural network for the MNIST digits task 50 Source: Michael Nielsen (2019). Neural Networks and Deep Learning, online book (+). 33 test errors with CNN • 33 errors + 9,967 hits • true class at top right • estimated class at bottom right • error rate = 0.33% MNIST (Modified National Institute of Standards and Technology database) • relatively simple task • although it includes complex cases that are difficult to read (noise) case study fully connected neural network for the MNIST digits task 51 (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784).astype('float32')/255 x_val = x_test.reshape(10000, 784).astype('float32')/255 y_train = keras.utils.to_categorical(y_train, num_classes) y_val = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='softmax')) sgd=SGD(lr=0.01, decay=1e-6, momentum=0.9) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) history = model.fit( x_train, y_train, batch_size=100, epochs=10, validation_data=(x_val, y_val)) simplified script case study fully connected neural network for the MNIST digits task 52 case study fully connected neural network for the MNIST digits task 53 How many parameters does this fully connected network have? case study fully connected neural network for the MNIST task 54 layers input hidden 1 hidden 2 hidden 3 output TOTAL units 784 64 64 64 10 202 weights 50,176 4,096 4,096 640 59,008 bias 64 64 64 10 202 TOTAL 50,240 4,160 4,160 650 59,210 case study fully connected neural network for the MNIST task 55 (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784).astype('float32')/255 x_test = x_test.reshape(10000, 784).astype('float32')/255 y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(10, activation='softmax')) sgd=SGD(lr=0.01, momentum=0.9) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy’]) history = model.fit( x_train, y_train, batch_size=100, epochs=10, validation_data=(x_val, y_val)) hyperparameters case study fully connected neural network for the MNIST task 56 activation 'relu' • Rectified Linear Unit (ReLU) • a piecewise linear activation function (hidden u) activation 'softmax' • output layer activation function • probability distribution over K outputs loss 'categorical_crossentropy' • one-hot vector + softmax + cross entropy • loss function • measures discrepancy between two distributions optimizer 'sgd' • stochastic gradient descent • basic optimization method batch_size 100 • mini-batch based training • stochastic method (random subsets) • weights update after each batch epochs 5 • learning loop over all training samples case study fully connected neural network for the MNIST task 57 activation function relu, sigmoid, tanh -1,5 -1 -0,5 0 0,5 1 1,5 -3 -2 -1 0 1 2 3 relu sigmoid tanh 𝑔 𝑧 = 𝑚𝑎𝑥 0, 𝑧 relu 𝑔 𝑧 = 1 1 + 𝑒−𝑧 sigmoid 𝑔 𝑧 = 2𝜎 2𝑧 − 1 tanh 58 activation function softmax (normalized exponential function) given a vector 𝑧 ∈ ℝ𝐾 softmax projects a vector of real data onto a “probability distribution” 𝑠: ℝ𝐾 → 0,1 𝐾, σ𝑖 𝑠 𝑧 𝑖 = 1 formulation: 𝑠 𝑧 𝑖 = 𝑒𝑧𝑖 σ𝑘=1 𝐾 𝑒𝑧𝑘 0 2 4 6 8 10 12 -3 -2 -1 0 1 2 3 𝑒 𝑥 59 … Σ Σ Σ Σ ... 𝑤1 𝑤2 𝑤3 𝑤𝐾 𝑠 𝑠 𝑠 𝑠 ... softmax 𝑠 𝑧 1 = 𝑒 𝑧1 σ𝑘=1 𝐾 𝑒𝑧𝑘 𝑠 𝑧 2 = 𝑒 𝑧2 σ𝑘=1 𝐾 𝑒𝑧𝑘 𝑠 𝑧 3 = 𝑒 𝑧3 σ𝑘=1 𝐾 𝑒𝑧𝑘 𝑠 𝑧 𝐾 = 𝑒 𝑧𝐾 σ𝑘=1 𝐾 𝑒𝑧𝑘 𝑧1 𝑧2 𝑧3 𝑧𝐾 activation function softmax (normalized exponential function) 60 softmax is increasing: if 𝑧𝑖 < 𝑧𝑗, then 𝑠 𝑧𝑖 < 𝑠 𝑧𝑗 𝑧𝑖 𝑒 𝑧𝑖 𝑠 𝑧 𝑖 -1 0,37 0,01 0 1,00 0,02 1 2,72 0,04 2 7,39 0,11 4 54,60 0,83 66,07 1,00 reinforces the highest activation value penalizes non-maximum activation values example activation function softmax (normalized exponential function) 61 softmax saturates if 𝑚𝑖𝑛𝑖𝑧𝑖 ≪ 𝑚𝑎𝑥𝑖𝑧𝑖 𝑧𝑖 𝑒 𝑧𝑖 𝑠 𝑧 𝑖 -1 0,37 0,00 0 1,00 0,00 1 2,72 0,00 2 7,39 0,00 8 2.980,96 1,00 2.992,43 1,00 the winner takes it all example activation function softmax (normalized exponential function) 62 loss function operating principle given • 𝑦, expectation (ground truth) • ො𝑦, prediction, estimation a los/cost/error function 𝐿 𝑦, ො𝑦 • measures the distance, difference, or discrepancy between 𝑦 e ො𝑦 • when 𝑦 e ො𝑦 are very different, then 𝐿 is large (high loss) • when 𝑦 e ො𝑦 are very similar, then 𝐿 is small (low loss) • when 𝑦 e ො𝑦 are equal, then 𝐿 = 0 learning objective • find parameters of the model that minimize 𝐿 over the validation set (part of the training data, not the test data!) 63 loss function one-hot encoding (output encoding) categorical/nominal variable: takes symbolic values, not numeric ones. examples: • pet: “dog”, “cat”, “bird” • model: “sedan”, “minivan”, “bus”, “truck” • dígitos: ‘0’, ‘1’, ‘2’, …‘9’ limitation: do not support numerical comparisons/operations one-hot encoding: clase sedan minivan bus truck 𝑥𝑠𝑒𝑑 1 0 0 0 𝑥𝑚𝑖𝑛 0 1 0 0 𝑥𝑏𝑢𝑠 0 0 1 0 𝑥𝑡𝑟𝑢 0 0 0 1 64 … Σ Σ Σ Σ ... 𝑤1 𝑤2 𝑤3 𝑤𝐾 𝑠 𝑠 𝑠 𝑠 ... MNIST 𝐾 = 10 0 0 1 . . . 0 0,1 0 0,6 . . . 0,2 ? ≈ expected probab. 𝑦 𝑥 estimated probab. ො𝑦 𝑥 one-hot encoding loss function how to measure the difference between two distributions? softmax 𝑥 𝑧1 𝑧2 𝑧3 𝑧𝐾 65 loss function categorical cross entropy given two probability distributions: • 𝑦 (e.g. expected, ground truth) • ො𝑦 (e.g. predicted, observed) cross entropy (single sample): 𝐸 𝑦, ො𝑦 = − σ𝑖 𝑦𝑖 log ෝ𝑦𝑖 = − log ෝ𝑦𝑐 effect: • ෝ𝑦𝑐 is the model’s prediction for the correct class 𝑐 • if 𝑦 and ො𝑦 are similar, then 𝐸 𝑦, ො𝑦 is small • if 𝑦 and ො𝑦 are different, then 𝐸 𝑦, ො𝑦 is large 0 0,5 1 1,5 2 0 0,2 0,4 0,6 0,8 1 cross entropy for 𝑝 = 1 𝑞 66 favorable scenario unfavorable scenario 𝑦 ො𝑦 −𝑦𝑖 log ෝ𝑦𝑖 𝑦 ො𝑦 −𝑦𝑖 log ෝ𝑦𝑖 0 0,05 0,00 0 0,20 0,00 0 0,05 0,00 0 0,20 0,00 1 0,80 0,10 1 0,20 0,70 0 0,05 0,00 0 0,20 0,00 0 0,05 0,00 0 0,20 0,00 loss 0,10 loss 0,70 two opposite scenarios only the ෝ𝑦𝑖 associated with 𝑦𝑖 = 1 contributes (-log 1 is the smallest loss) loss function categorical cross entropy 67 loss function can we pay more attention to minority classes? what if classes are imbalanced? • many more loss terms from the majority class than from the rest • all the loss terms matter (wheigh) the same • the majority class examples dominate the loss function • the majority class examples dominate gradient propagation • more model weight updates to favor the majority class • the model will be more confident in predicting the majority class • little emphasis on minority classes • summary: biased classifier learning 68 loss function balanced cross entropy given two probability distributions and… • 𝑦 (e.g. expected, ground truth) • ො𝑦 (e.g. predicted, observed) balanced cross entropy (single sample): 𝐸 𝑦, ො𝑦 = −𝛼𝑐 log ෝ𝑦𝑐 comments: • ෝ𝑦𝑐 is the model’s prediction for the correct class 𝑐 • 𝛼𝑐 is a class weight related to class 𝑐 • 𝛼𝑐 is inversely proportional to the frequency of class 𝑐 𝑞 balanced cross entropy 69 loss function balanced cross entropy how to compute class weights? • by hyperparameter tuning • by compute_class_weight from sklearn.utils ∝𝑖= 𝑛 𝑘 · 𝑛𝑖 where: − 𝑛 is the total number of training simples − 𝑛𝑖 is the number of training samples of class 𝑖 − 𝑘 is the number of classes 70 loss function paying more attention to hard-to-classify examples how to improve predictions on hard examples • hard examples = samples classified with less confidence • strategy: guide learning to focus more on hard examples • side effect: natural mitigation of biases from imbalanced classes − examples from the majority class are usually easy to predict − examples from the minority class are usually hard to predict − examples from the majority class dominate loss & gradients 71 loss function focal cross entropy given two probability distributions • 𝑦 (e.g. expected, ground truth) • ො𝑦 (e.g. predicted, observed) focal cross entropy (for a single sample): 𝐸 𝑦, ො𝑦 = − 1 − ෝ𝑦𝑐 𝛾 log ෝ𝑦𝑐 where: • ෝ𝑦𝑐 is the model’s prediction for the correct class 𝑐 • 𝛾: focal factor (hyperparameter) • 𝛾 reduces the contribution of easy examples to the total loss • typical values for 𝛾 range from 1 to 5 𝑞 focal cross entropy 72 how focal loss works? • when a sample is misclassified (hard examples)… − ෝ𝑦𝑐 is small => the modulating factor 1 − ෝ𝑦𝑐 𝛾 is close to 1 − the loss term keeps unaffected (it behaves as in cross entropy loss) • when a sample is correctly classified (easy examples)… − ෝ𝑦𝑐 is close to 1 => the modulating factor 1 − ෝ𝑦𝑐 𝛾 is close to 0 − the loss term is down weighted, reducing its impact on the loss function • 𝛾 adjusts the rate at which easy examples are down-weighted • 𝛾 = 0 reduces focal loss to standard cross entropy • higher values of 𝛾 encourage the model to focus on harder examples loss function focal cross entropy 73 loss function focal cross entropy 𝛼-balanced focal loss (single sample): 𝐸 𝑦, ො𝑦 = −𝛼𝑐 1 − ෝ𝑦𝑐 𝛾 log ෝ𝑦𝑐 comments: • typical implementation of focal loss • it usually leads to better results than the unbalanced version. 74 optimization problem MNIST dataset given 60.000 training digit images, with their classes annotated as one-hot vectors… 𝑇 = 𝑥𝑖, 𝑦𝑖 𝑖=1,60.000 such that 𝑥𝑖 ∈ 0,1 784, 𝑦𝑖 ∈ 0,1 10 goal: to find optimal values for the 2.913.920 model parameters 𝑊∗ = arg min 𝑊 𝐿 𝑊 , 𝑊 ∈ ℝ2.913.920 with 𝐿 𝑊 being the loss function defined as follows: 𝐿 𝑊 = 1 60.000 σ𝑖=1 60.000 𝐸 𝑦𝑖, ො𝑦𝑖 = − 1 60.000 σ𝑖=1 60.000 σ𝑗=1 10 𝑦𝑖𝑗 log ො𝑦𝑖𝑗 75 optimizer sgd + momentum + weight decay stochastic gradient descent (sgd): 𝜃𝑡+1 = 𝜃𝑡 − 𝛾 𝜕𝐿 𝜕𝜃 𝜃𝑡 sgd + momentum + weight decay (𝛾 denotes the learning rate): 𝜃𝑡+1 = 𝜃𝑡 + 𝑣𝑡 𝑣𝑡 = 𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚 · 𝑣𝑡−1 − 𝛾 𝜕𝐿 𝜕𝜃 𝜃𝑡 𝛾 = 𝛾 · 1 1+𝑑𝑒𝑐𝑎𝑦·𝑖𝑡𝑒𝑟𝑎𝑐𝑖ó𝑛 76 hyperparameters overview hyperparameters are parameters needed to generate the model or to define the training process hyperparameters determine the structure or configuration of the model or characteristics of the learning process; their values are chosen before each training session. their optimal values depend on the complexity of the task, the nature of the data (dimensionality, distribution, quantity, etc.), and the interdependence with other hyperparameters. 77 hyperparameters overview hyper- parameter choice hyperparameter values model generation model model training optimized model 78 hyperparameters of the network architecture defined for MNIST: • 4 trainable layers • layer 1: 1024 units, ReLU activation • layer 1: 1024 units, ReLU activation • layer 1: 1024 units, ReLU activation • layer 4: softmax activation hyperparameters fully connected neural network for the MNIST task 79 hyperparameters of the learning process defined for MNIST: • optimizer: SGD(lr=0.01, decay=1e-6, momentum=0.9) • loss = 'categorical_crossentropy' • batch_size = 100 • epochs = 5 hyperparameters fully connected neural network for the MNIST task 80 summary • a machine learning algorithm optimizes models from data • a linear model solves only tasks with linear boundaries • Perceptron is a linear model of binary classification • a fully connected multilayer network is composed of a sequence of fully connected layers. • each unit consists of a linear function + nonlinear activation • a multilayer network could learn any decision boundary • backpropagation is an algorithm for training feedforward networks in supervised learning; includes calculation of gradients, not how to use them. 81","libVersion":"0.3.2","langs":""}