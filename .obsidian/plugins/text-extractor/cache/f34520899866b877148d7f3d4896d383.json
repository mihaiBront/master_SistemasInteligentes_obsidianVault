{"path":"_aula_virtual/SJK003/12-artificialNeuralNetwork.pdf","text":"RamÃ³n A. Mollineda CÃ¡rdenas artificial neural networks MACHINE LEARNING University Master's Degree in Intelligent Systems fuel of the future â€œAN OIL refinery... Data centres... the two have much in common. For one thing, both are stuffed with pipes. In refineries these collect petrol, propane and other components of crude oil ... In big data centres ... tens of thousands of computers ... extract value â€”patterns, predictions and other insightsâ€” from raw digital information.â€ The Economist, May 6th 2017 https://www.economist.com/briefing/2017/05/06/data-is-giving-rise-to-a-new-economy. 2 deep learning deep learning (refinery, distillery) massive raw data (big data, raw material) concepts, patterns, predictions, manifolds, etc. (product with practical value) 010010110101001010101001010001001100010011 100100100100110101101001001001110010101000 110010000011010101110100100110010001010001 111011001001110011001100100010010110010001 001100010000100110100010100101111001001010 111000001000010000000100101111111111000101 110101000101010001000001010101000101000100 111010010100101111001001001100101001101000 001001101001010100010000100011111000101010 101010101001001110001000011110010101010101 ğ‘“ğœƒ ğ‘¥ 3Fuente: ARGILITY. https://www.argility.com/argility-ecosystem-solutions/industry-4-0/machine-learning-deep-learning/. deep learning some context 4 deep learning artificial neural networks outperform humans o object and image recognition (ILSVRC, Â¿aceptas el reto?) o imitation of art styles (DeepArt.io, transferencia de estilo) o medical diagnosis (DL 95% - senior doctors 87% in the diagnosis of malignant melanomas -distinguishing them from benign moles-) o computer gaming (modelos que aprenden, AlphaGo vs. Lee Sedol) o lipreading (LipNet 95% - humanos 52% according to grammar â€œcommand(4) + color(4) + preposition(4) + letter(25) + digit(10) + adverb(4)â€) o big data analytics (demografÃ­a a partir de Google Street View). â€œUsing deep learning â€¦, we determined the make, model, and year of all motor vehicles encountered .... Data from this census of motor vehicles, which enumerated 22M automobiles in total (8% of all automobiles in the US), was used to accurately estimate income, race, education, and voting patterns â€¦â€ (Gebru et al. 2017) Ver mÃ¡s en Roman Steinberg (2017), 6 areas where artificial neural networks outperform humans, VentureBeat (ir). 5applications to summarize texts, generate responses (chatbots), write stories, etc. news deep learning natural language generation models OpenAI GPT-3 175 billion parameters 6 GAURAV CHAUHAN, Iris Dataset Project from UCI Machine Learning Repository, machinelearninghd.com, 2021. (enlace) IRIS Data Set classes 7 â€¢ most popular database in machine learning â€¢ 150 examples distributed among 3 classes : â€“ Iris Versicolor (50) â€“ Iris Setosa (50) â€“ Iris Virginica (50) â€¢ examples described by 4 measurements (in cm): â€“ sepal length â€“ sepal width â€“ petal length â€“ petal width IRIS Data Set description 8https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Iris_dataset_scatterplot.svg/2000px-Iris_dataset_scatterplot.svg.png IRIS Data Set distributions by classes in 2D subspaces 9 Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. Annual Eugenics, 7, Part II, 179-188 (acceder). IRIS Data Set original paper 10 linear model a fully connected neural network ğ‘“ is a composition of nonlinear transformations of linear models (linear combinations of features) ğ‘“ ğ‘¥ ğ‘¤ğ‘˜, â€¦ ğ‘¤2, ğ‘¤1 = ğ‘”âˆ— ğ‘¤ğ‘˜ ğ‘¡ğ‘”ğ‘˜âˆ’1 â‹¯ ğ‘”2 ğ‘¤2 ğ‘¡ğ‘”1 ğ‘¤1 ğ‘¡ğ‘¥ being... â€¢ ğ‘¥, the input vector to the network â€¢ ğ‘˜, the number of layers (or transformations) â€¢ ğ‘¤ğ‘–, weight matrix of the layer ğ‘–, 1 â‰¤ ğ‘– â‰¤ ğ‘˜ â€¢ ğ‘”ğ‘–, nonlinear activation function of the layer ğ‘– (they are generally the same) â€¢ ğ‘”âˆ—, output layer activation function (can be the identity function) â€¢ ğ‘“, network function composed from chains of linear and nonlinear functions 11 linear model a fully connected neural network ğ‘“ is a composition of nonlinear transformations of linear models (linear combinations of features) ğ‘“ ğ‘¥ ğ‘¤ğ‘˜, â€¦ ğ‘¤2, ğ‘¤1 = ğ‘”âˆ— ğ‘¤ğ‘˜ ğ‘¡ğ‘”ğ‘˜âˆ’1 â‹¯ ğ‘”2 ğ‘¤2 ğ‘¡ğ‘”1 ğ‘¤1 ğ‘¡ğ‘¥ linear model being... â€¢ ğ‘¥, the input vector to the network â€¢ ğ‘˜, the number of layers (or transformations) â€¢ ğ‘¤ğ‘–, weight matrix of the layer ğ‘–, 1 â‰¤ ğ‘– â‰¤ ğ‘˜ â€¢ ğ‘”ğ‘–, nonlinear activation function of the layer ğ‘– (they are generally the same) â€¢ ğ‘”âˆ—, output layer activation function (can be the identity function) â€¢ ğ‘“, network function composed from chains of linear and nonlinear functions 12 linear model classes Setosa (red) and Versicolor (gray) ğ‘§ = 3ğ‘¥1 âˆ’ 5ğ‘¥2 âˆ’ 0,6 ğ‘¥1 ğ‘¥2 ğ‘§ < 0 ğ‘§ > 0 error ğ‘‘ = 0.25 ğ‘§ = 0 ğ‘‘ = 1.21 ğ‘‘ = 0.09 ğ‘‘ = âˆ’0.92 ğ‘‘ = ğ‘§ ğ‘¤ 2 ğ‘‘ , distance to the plane 13 ğ‘§ = ğ‘¤ğ‘‡ğ‘¥ + ğ‘ where: w, weight vector x, input data vector b, scalar; bias, threshold z, scalar; affine transformation output linear model affine/linear transformation Î£... ğ‘§ ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘ ğ‘§ = Ïƒğ‘–=1 ğ‘‘ ğ‘¤ğ‘–ğ‘¥ğ‘– + ğ‘ -6 -4 -2 0 2 4 6 -3 -2 -1 0 1 2 3 ğ‘§ < 0 ğ‘§ > 0 ğ‘§ = 0 ğ‘§ = 2ğ‘¥1 âˆ’ ğ‘¥2 + 1 linear division of space 14 ğ‘§ = ğ‘¤ğ‘‡ğ‘¥ where: w, weight vector (ğ‘¤0, bias) x, input data vector z, scalar; affine transformation output ğ‘§ = Ïƒğ‘–=0 ğ‘‘ ğ‘¤ğ‘–ğ‘¥ğ‘– Î£... ğ‘§ ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 linear model affine/linear transformation: compact notation 15 where: z, scalar; affine transformation output g, nonlinear activation function h, scalar; nonlinear function output ğ‘” â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ nonlinear activation function linear function linear model activation function 16 where: z, affine transformation output g, step function (derivative 0 at all points) â„ âˆˆ 0,1 linear model Perceptron (Frank Rosenblatt, 1958) â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ step function 0 0,5 1 -3 -2 -1 0 1 2 3 ğ‘” ğ‘§ = á‰Š 1, si ğ‘§ â‰¥ 0 0, si ğ‘§ < 0 17 â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ ğ‘§ < 0 ğ‘” ğ‘§ = 0 ğ‘§ = 2ğ‘¥1 âˆ’ ğ‘¥2 + 1 ğ‘§ â‰¥ 0 ğ‘” ğ‘§ = 1 linear model Perceptron (Frank Rosenblatt, 1958) where: z, affine transformation output g, step function (derivative 0 at all points) â„ âˆˆ 0,1 step function 18 â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ sigmoid ğ‘” ğ‘§ = 1 1 + ğ‘’âˆ’ğ‘§ 0 0,5 1 -3 -2 -1 0 1 2 3 linear model logistic regression: a linear decision boundary logistic regression where: z, affine transformation output g, sigmoid function (derivative â‰  0 at all points) â„ âˆˆ 0,1 â„ = ğ‘ƒğ‘Ÿ ğ‘¦ = 1 ğ‘¥, ğ‘¤ Why is logistic regression a linear classifier? (link) 19 â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ sigmoid ğ‘§ < 0 0 < ğ‘” ğ‘§ < 0.5 ğ‘§ = 2ğ‘¥1 âˆ’ ğ‘¥2 + 1 ğ‘§ > 0 0.5 < ğ‘” ğ‘§ < 1 linear model logistic regression: a linear decision boundary where: z, affine transformation output g, sigmoid function (derivative â‰  0 at all points) â„ âˆˆ 0,1 â„ = ğ‘ƒğ‘Ÿ ğ‘¦ = 1 ğ‘¥, ğ‘¤ 20 â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ ğ‘” ğ‘§ = ğ‘šğ‘ğ‘¥ 0, ğ‘šğ‘–ğ‘› 1, ğ‘¥ + 1 2 0 0,5 1 -3 -2 -1 0 1 2 3 rigid sigmoid linear model approximate (or rigid) logistic regression where: z, affine transformation output g, rigid sigmoid function â„ âˆˆ 0,1 â„ = ğ‘ƒğ‘Ÿ ğ‘¦ = 1 ğ‘¥, ğ‘¤ 21 â„Î£... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘‘ ğ‘¥1 ğ‘¥2 ğ‘¥3 ğ‘¥ğ‘‘ ğ‘¥0 = 1 ğ‘¤0 â„ = ğ‘” ğ‘§ = ğ‘” ğ‘¤ğ‘‡ğ‘¥ ğ‘§ < 0 0 < ğ‘” ğ‘§ < 0.5 ğ‘§ = 2ğ‘¥1 âˆ’ ğ‘¥2 + 1 ğ‘§ > 0 0.5 < ğ‘” ğ‘§ < 1 rigid sigmoid linear model approximate (or rigid) logistic regression where: z, affine transformation output g, rigid sigmoid function â„ âˆˆ 0,1 â„ = ğ‘ƒğ‘Ÿ ğ‘¦ = 1 ğ‘¥, ğ‘¤ 22 iris = sklearn.datasets.load_iris() X = iris.data[:, :2] # first two features (sepal width, sepal length) y = iris.target X = (X-numpy.mean(X, axis=0))/numpy.std(X, axis=0) # data standardization model = keras.models.Sequential() model.add(keras.layers.Dense(1, input_shape=(2,), activation=keras.activations.hard_sigmoid)) opt = keras.optimizers.SGD(learning_rate = 0.1, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) history = model.fit(X, y, epochs=10, batch_size=10, verbose=1, validation_split=0.2) script simplificado usando numpy, sklearn y keras: linear model approximate perceptron on Setosa and Versicolor classes See: Understanding binary cross-entropy / log loss: a visual explanation (link). Published in Towards Data Science. 23 linear model approximate perceptron on Setosa and Versicolor classes 24 linear model approximate perceptron on Setosa and Versicolor classes 25 linear model limitations linear classifier logistic regression => â€œsigmoidâ€ activation function Aryan Kashyap (2017), Neural Networks for Decision Boundary in Python!, Medium. 26 fully connected multilayer networks Î£ Î£ Î£ ğ‘” ğ‘” ğ‘” Î£ Î£ ğ‘” ğ‘” Î£ ğ‘¥1 ğ‘¥2 à´¤ğ‘¦ ğ‘”: funciÃ³n de activaciÃ³n (e.g. sigmoid, tanh, softmax, relu, â€¦) 1 1 1 input layer hidden layer output layerhidden layer 27 Î£ Î£ Î£ ğ‘” ğ‘” ğ‘” 1 â€¦ â€¦ â€¢ cada capa consiste en: funciÃ³n lineal + activaciÃ³n no lineal â€¢ funciÃ³n de transformaciÃ³n en capa ğ‘™: ğ‘§ğ‘™ = ğ‘Šğ‘™ Â· ğ¼ğ‘™ ğ¼ğ‘™+1 = ğ‘” ğ‘§ğ‘™ â€¢ sea ğ»ğ‘™ el nÃºmero de unidades de la capa ğ‘™; el tamaÃ±o de ğ‘Šğ‘™ serÃ­a â€¦ ğ»ğ‘™ Ã— ğ»ğ‘™âˆ’1 â€¢ ğ¼ğ‘™ es de tamaÃ±o ğ»ğ‘™âˆ’1 Ã— 1 capa ğ‘™ fully connected multilayer networks 28 multilayer network 3 layers of 12+6+1 units, â€œreluâ€ and â€œsigmoidâ€ activation functions fully connected multilayer networks 29 multilayer network 3 layers of 12+6+1 units, â€œreluâ€ and â€œsigmoidâ€ activation functions fully connected multilayer networks 30 backpropagation learning goal progressive adjustment (iterative optimization) of weights that minimize the error/loss of the network on a representative set of training samples. potentially, a network with sufficient learning capacity could achieve zero error on the training set; however, it is not usually a desirable goal (overfitting risk). 31 initialize network weights w(0) with small arbitrary values for epoch = 1...K, do for batch = 1...N/batch_size, do batch <â€“ randomly choose batch_size instances X, y <â€“ preprocess(batch) z <â€“ network(X) (forward execution) â„“ <â€“ loss(z, y) g <- gradients(â„“, w) (backward execution) w(t+1) <- w(t) â€“ Î³ Â· g (weight optimization/fitting) end for end for backpropagation training a neural network (mini-batches) backpropagation 32 backpropagation fundamentals: chain rule some examples of composite of two (differentiable) functions ğ‘“ ğ‘” ğ‘¥ : o ğ‘“ ğ‘” ğ‘¥ = 2ğ‘¥ + 1 3 o ğ‘“ ğ‘” ğ‘¥ = sin ğ‘¥2 o ğ‘“ ğ‘” ğ‘¥ = sin ğ‘¥ 2 their derivative functions ğ‘“â€² = ğ‘“â€² ğ‘” ğ‘¥ Â· ğ‘”â€² ğ‘¥ , or ğœ•ğ‘“ ğœ•ğ‘¥ = ğœ•ğ‘“ ğœ•ğ‘” Â· ğœ•ğ‘” ğœ•ğ‘¥ o ğœ•ğ‘“ ğœ•ğ‘¥ = 3 2ğ‘¥ + 1 2 Â· 2 o ğœ•ğ‘“ ğœ•ğ‘¥ = cos ğ‘¥2 Â· 2ğ‘¥ o ğœ•ğ‘“ ğœ•ğ‘¥ = 2 Â· sin ğ‘¥ Â· cos ğ‘¥ 33 gradient descent (steepest descent) is an iterative optimization algorithm for finding a local minimum of a differentiable function. strategy â€¢ starting from an initial value of the parameter â€¢ move â€œdownhillâ€ along the surface of the function, in the direction of the negative gradient, looking for a parameter value that minimizes it â€¢ stop when the minimum of the function is reached https://www.jeremyjordan.me/gradient-descent/ https://youtu.be/CsKCT5ezVdI backpropagation fundamentals: gradient descent 34 goal to find ğœƒ = ğœƒâˆ— such that ğ¿ ğœƒâˆ— is the minimum value of ğ¿ method â€¢ let ğ¿ ğœƒ be a function defined by the parameter ğœƒ â€¢ initialization : ğœƒ0 = ğœƒğ‘–ğ‘›ğ‘–ğ‘ğ‘–ğ‘ğ‘™ â€¢ updating: ğœƒğ‘–+1 = ğœƒğ‘– âˆ’ ğ›¾ ğœ•ğ¿ ğœ•ğœƒ ğœƒğ‘– , â”€ ğ›¾ â‰ª 1 is the learning rate; it determines the magnitude of change â€¢ repeat as long as ğ¿ ğœƒğ‘–+1 < ğ¿ ğœƒğ‘– â€¢ solution: ğœƒâˆ— = ğœƒğ‘– backpropagation fundamentals: gradient descent 35source: https://www.jeremyjordan.me/nn-learning-rate/ effect of the learning coefficient ğ›¾ backpropagation fundamentals: gradient descent 36 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2ğ‘¤ğ‘–ğ‘— 1 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 ğ‘¤ğ‘–ğ‘— ğ¿â€¦ğ‘¥ ğ‘¦ ğ¸ à·œğ‘¦, ğ‘¦ 1 ğ¿ âˆ’ 2 ğ¿ âˆ’ 1 ğ¿ layer layer layer layer ğ¸ error/loss function à·œğ‘¦ backpropagation solution scheme 37 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2ğ‘¤ğ‘–ğ‘— 1 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 ğ‘¤ğ‘–ğ‘— ğ¿â€¦ğ‘¥ ğ‘¦ ğ¸ à·œğ‘¦, ğ‘¦ 1 ğ¿ âˆ’ 2 ğ¿ âˆ’ 1 ğ¿ layer layer layer layer ğ¸ à·œğ‘¦ goal: ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ‘™ for all ğ‘¤ğ‘–ğ‘— ğ‘™ backpropagation solution scheme error/loss function 38 1. initialize weights, choose learning coefficient, choose stop criterion 2. create an arbitrary batch (X, y) --> (subset of instances, expected output). 3. forward execution: walk X through the network and get output z. 4. compute loss(y, z) 5. backward execution (backpropagation) o compute sensitivity coefficient ğ›¿ğ¿ = ğ‘“ ğ›¿ğ¿+1 from input to layer L o compute gradients ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 = ğ‘“ ğ›¿ğ¿ 6. weight optimization 7. check stopping criteria; if it is fulfilled, then finish; otherwise, go to step 2. backpropagation solution scheme 39 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2ğ‘¤ğ‘–ğ‘— 1 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 ğ‘¤ğ‘–ğ‘— ğ¿â€¦ğ‘¥ ğ‘¦ 1 ğ¿ âˆ’ 2 ğ¿ âˆ’ 1 ğ¿ layer layer layer layer ğ¸ ğ¼ğ¿âˆ’2 ğ¼ğ¿âˆ’1 ğ¼ğ¿ forward execution the following pieces of information are calculated: â€¢ input ğ¼ğ‘™ to each layer ğ‘™ (matches the output of the layer ğ‘™ âˆ’ 1), â€¢ actual network output à·œğ‘¦ â€¢ value of the error or loss function ğ¸ à·œğ‘¦, ğ‘¦ à·œğ‘¦ ğ¸ à·œğ‘¦, ğ‘¦ backpropagation solution scheme 40 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2ğ‘¤ğ‘–ğ‘— 1 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 ğ‘¤ğ‘–ğ‘— ğ¿ ğ¸â€¦ğ‘¥ ğ‘¦ ğ¼ğ¿âˆ’2 ğ¼ğ¿âˆ’1 ğ¼ğ¿ ğ›¿ğ¿âˆ’2 ğ›¿ğ¿âˆ’1 ğ›¿ğ¿ ğ›¿ğ¿+1 ğ›¿ğ¿+1 = ğœ•ğ¸ ğœ• à·œğ‘¦ 1 ğ¿ âˆ’ 2 ğ¿ âˆ’ 1 ğ¿ layer layer layer layer ğ›¿ğ¿ = ğ‘“ ğ›¿ğ¿+1ğ›¿ğ¿âˆ’1 = ğ‘“ ğ›¿ğ¿ ğ›¿ğ¿âˆ’1 = ğœ•ğ¸ ğœ•ğ¼ğ¿âˆ’1 ğ›¿ğ¿ = ğœ•ğ¸ ğœ•ğ¼ğ¿ ğ›¿ğ¿+1 = ğœ•ğ¸ ğœ•ğ¼ğ¿+1 backward execution (calculation of sensitivities) definition recursive computing ğ›¿ğ‘™ measures the sensitivity of ğ¸ to changes in ğ¼ğ‘™ ğ›¿ğ‘™ is computed recursively from ğ›¿ğ‘™+1 ğ›¿ğ‘™ allows efficient computation of ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ‘™âˆ’1 à·œğ‘¦ ğ¸ à·œğ‘¦, ğ‘¦ backpropagation solution scheme 41 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2ğ‘¤ğ‘–ğ‘— 1 ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 ğ‘¤ğ‘–ğ‘— ğ¿ ğ¸â€¦ğ‘¥ ğ‘¦ ğ¼ğ¿âˆ’2 ğ¼ğ¿âˆ’1 ğ¼ğ¿ ğ›¿ğ¿âˆ’2 ğ›¿ğ¿âˆ’1 ğ›¿ğ¿ ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ¿ = ğ‘“ ğ›¿ğ¿+1 ğ›¿ğ¿+1 1 ğ¿ âˆ’ 2 ğ¿ âˆ’ 1 ğ¿ layer layer layer layer ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ¿âˆ’1 = ğ‘“ ğ›¿ğ¿ ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ¿âˆ’2 = ğ‘“ ğ›¿ğ¿âˆ’1 ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— 1 = ğ‘“ ğ›¿2 ğ¸ à·œğ‘¦, ğ‘¦ à·œğ‘¦ backpropagation solution scheme backward execution (calculation of sensitivities) 42 ğœ: activation function (e.g. sigmoid, tanh, relu, etc.) Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ‘¥1 ğ‘¥2 ğ¸ ğ‘¦ à´¤ğ‘¦ ğ¸ à´¤ğ‘¦, ğ‘¦ error/loss computation backpropagation case study: fully connected network with 3 layers 43 2 x 3 + 3 + 3 x 2 + 2 + 2 x 1 + 1 = 9 + 8 + 3 = 20 trainable parameters/weights Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ‘¥1 ğ‘¥2 ğ¸ ğ‘¦ à´¤ğ‘¦ ğ¸ à´¤ğ‘¦, ğ‘¦ error/loss computation backpropagation case study: fully connected network with 3 layers 44 ğ‘¥1 ğ‘¥2 ğ‘¦ à´¤ğ‘¦ ğ¼1 3 ğ¼2 3 à´¤ğ‘¦ âˆ’ ğ‘¦ 2 forward execution: introduce ğ‘¥, ğ‘¦ and compute ğ¼ğ‘– ğ‘™ and à´¤ğ‘¦ ğ¼1 2 ğ¼2 2 ğ¼3 2 Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ¸ backpropagation case study: fully connected network with 3 layers error/loss computation ğ¼1 4 45 ğ‘¤11 3 ğ‘¤21 3 ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— 3 = ğœ•ğ¼1 4 ğœ•ğ‘¤ğ‘–ğ‘— 3 Â· ğ›¿1 4 = ğœ ğ¼ğ‘– 3 Â· 2 à´¤ğ‘¦ âˆ’ ğ‘¦ ğ‘¥1 ğ‘¥2 ğ‘¦ à´¤ğ‘¦ ğ¼1 3 ğ¼2 3 à´¤ğ‘¦ âˆ’ ğ‘¦ 2 ğ¼1 2 ğ¼2 2 ğ¼3 2 ğ¼1 4 Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ¸ backward execution: compute gradients ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ‘™ error/loss computation backpropagation case study: fully connected network with 3 layers 46 ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— 2 = ğœ•ğ¼ğ‘— 3 ğœ•ğ‘¤ğ‘–ğ‘— 2 Â· ğ›¿ğ‘— 3 = ğœ ğ¼ğ‘– 2 Â· ğœâ€² ğ¼ğ‘— 3 Â· ğ‘¤ğ‘—1 3 Â· ğ›¿1 4 ğ‘¤ğ‘–ğ‘— 2 ğ‘¥1 ğ‘¥2 ğ‘¦ à´¤ğ‘¦ ğ¼1 3 ğ¼2 3 à´¤ğ‘¦ âˆ’ ğ‘¦ 2 ğ¼1 2 ğ¼2 2 ğ¼3 2 ğ¼1 4 Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ¸ ğ‘¤11 3 ğ‘¤21 3 backward execution: compute gradients ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ‘™ error/loss computation backpropagation case study: fully connected network with 3 layers 47 ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— 1 = ğœ•ğ¼ğ‘— 2 ğœ•ğ‘¤ğ‘–ğ‘— 1 Â· ğ›¿ğ‘— 2 = ğœ ğ¼ğ‘– 1 Â· ğœâ€² ğ¼ğ‘— 2 Ïƒğ‘˜=1 2 ğ‘¤ğ‘—ğ‘˜ 2 Â· ğ›¿ğ‘˜ 3 ğ‘¤ğ‘–ğ‘— 1 ğ¼1 1 ğ¼2 1 ğ¼ğ‘– 1 = ğ‘¥ğ‘– ğ‘¤ğ‘–ğ‘— 2 ğ‘¥1 ğ‘¥2 ğ‘¦ à´¤ğ‘¦ ğ¼1 3 ğ¼2 3 à´¤ğ‘¦ âˆ’ ğ‘¦ 2 ğ¼1 2 ğ¼2 2 ğ¼3 2 ğ¼1 4 Î£ Î£ Î£ Ïƒ Ïƒ Ïƒ Î£ Î£ Ïƒ Ïƒ Î£ ğ¸ ğ‘¤11 3 ğ‘¤21 3 backward execution: compute gradients ğœ•ğ¸ ğœ•ğ‘¤ğ‘–ğ‘— ğ‘™ error/loss computation backpropagation case study: fully connected network with 3 layers 48 case study fully connected neural network for the MNIST digits task implementation in Keras â€¢ deep learning framework: Python library for creating neural networks â€¢ high-level interface based on Tensorflow, Theano, or the Microsoft Cognitive Toolkit â€¢ it allows defining and assembling pieces in neural networks such as layers, objective functions, activation, optimizers, etc. 49 MNIST (Modified National Institute of Standards and Technology database) handwritten digits image collection (official website) Source: Wikipedia (+) â€¢ 60,000 training images â€¢ 10,000 test images â€¢ 10 classes â€¢ image size: 28x28 â€¢ grey images Source: Medium (+) case study fully connected neural network for the MNIST digits task 50 Source: Michael Nielsen (2019). Neural Networks and Deep Learning, online book (+). 33 test errors with CNN â€¢ 33 errors + 9,967 hits â€¢ true class at top right â€¢ estimated class at bottom right â€¢ error rate = 0.33% MNIST (Modified National Institute of Standards and Technology database) â€¢ relatively simple task â€¢ although it includes complex cases that are difficult to read (noise) case study fully connected neural network for the MNIST digits task 51 (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784).astype('float32')/255 x_val = x_test.reshape(10000, 784).astype('float32')/255 y_train = keras.utils.to_categorical(y_train, num_classes) y_val = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='softmax')) sgd=SGD(lr=0.01, decay=1e-6, momentum=0.9) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) history = model.fit( x_train, y_train, batch_size=100, epochs=10, validation_data=(x_val, y_val)) simplified script case study fully connected neural network for the MNIST digits task 52 case study fully connected neural network for the MNIST digits task 53 How many parameters does this fully connected network have? case study fully connected neural network for the MNIST task 54 layers input hidden 1 hidden 2 hidden 3 output TOTAL units 784 64 64 64 10 202 weights 50,176 4,096 4,096 640 59,008 bias 64 64 64 10 202 TOTAL 50,240 4,160 4,160 650 59,210 case study fully connected neural network for the MNIST task 55 (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(60000, 784).astype('float32')/255 x_test = x_test.reshape(10000, 784).astype('float32')/255 y_train = keras.utils.to_categorical(y_train, num_classes) y_test = keras.utils.to_categorical(y_test, num_classes) model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(10, activation='softmax')) sgd=SGD(lr=0.01, momentum=0.9) model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracyâ€™]) history = model.fit( x_train, y_train, batch_size=100, epochs=10, validation_data=(x_val, y_val)) hyperparameters case study fully connected neural network for the MNIST task 56 activation 'relu' â€¢ Rectified Linear Unit (ReLU) â€¢ a piecewise linear activation function (hidden u) activation 'softmax' â€¢ output layer activation function â€¢ probability distribution over K outputs loss 'categorical_crossentropy' â€¢ one-hot vector + softmax + cross entropy â€¢ loss function â€¢ measures discrepancy between two distributions optimizer 'sgd' â€¢ stochastic gradient descent â€¢ basic optimization method batch_size 100 â€¢ mini-batch based training â€¢ stochastic method (random subsets) â€¢ weights update after each batch epochs 5 â€¢ learning loop over all training samples case study fully connected neural network for the MNIST task 57 activation function relu, sigmoid, tanh -1,5 -1 -0,5 0 0,5 1 1,5 -3 -2 -1 0 1 2 3 relu sigmoid tanh ğ‘” ğ‘§ = ğ‘šğ‘ğ‘¥ 0, ğ‘§ relu ğ‘” ğ‘§ = 1 1 + ğ‘’âˆ’ğ‘§ sigmoid ğ‘” ğ‘§ = 2ğœ 2ğ‘§ âˆ’ 1 tanh 58 activation function softmax (normalized exponential function) given a vector ğ‘§ âˆˆ â„ğ¾ softmax projects a vector of real data onto a â€œprobability distributionâ€ ğ‘ : â„ğ¾ â†’ 0,1 ğ¾, Ïƒğ‘– ğ‘  ğ‘§ ğ‘– = 1 formulation: ğ‘  ğ‘§ ğ‘– = ğ‘’ğ‘§ğ‘– Ïƒğ‘˜=1 ğ¾ ğ‘’ğ‘§ğ‘˜ 0 2 4 6 8 10 12 -3 -2 -1 0 1 2 3 ğ‘’ ğ‘¥ 59 â€¦ Î£ Î£ Î£ Î£ ... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ¾ ğ‘  ğ‘  ğ‘  ğ‘  ... softmax ğ‘  ğ‘§ 1 = ğ‘’ ğ‘§1 Ïƒğ‘˜=1 ğ¾ ğ‘’ğ‘§ğ‘˜ ğ‘  ğ‘§ 2 = ğ‘’ ğ‘§2 Ïƒğ‘˜=1 ğ¾ ğ‘’ğ‘§ğ‘˜ ğ‘  ğ‘§ 3 = ğ‘’ ğ‘§3 Ïƒğ‘˜=1 ğ¾ ğ‘’ğ‘§ğ‘˜ ğ‘  ğ‘§ ğ¾ = ğ‘’ ğ‘§ğ¾ Ïƒğ‘˜=1 ğ¾ ğ‘’ğ‘§ğ‘˜ ğ‘§1 ğ‘§2 ğ‘§3 ğ‘§ğ¾ activation function softmax (normalized exponential function) 60 softmax is increasing: if ğ‘§ğ‘– < ğ‘§ğ‘—, then ğ‘  ğ‘§ğ‘– < ğ‘  ğ‘§ğ‘— ğ‘§ğ‘– ğ‘’ ğ‘§ğ‘– ğ‘  ğ‘§ ğ‘– -1 0,37 0,01 0 1,00 0,02 1 2,72 0,04 2 7,39 0,11 4 54,60 0,83 66,07 1,00 reinforces the highest activation value penalizes non-maximum activation values example activation function softmax (normalized exponential function) 61 softmax saturates if ğ‘šğ‘–ğ‘›ğ‘–ğ‘§ğ‘– â‰ª ğ‘šğ‘ğ‘¥ğ‘–ğ‘§ğ‘– ğ‘§ğ‘– ğ‘’ ğ‘§ğ‘– ğ‘  ğ‘§ ğ‘– -1 0,37 0,00 0 1,00 0,00 1 2,72 0,00 2 7,39 0,00 8 2.980,96 1,00 2.992,43 1,00 the winner takes it all example activation function softmax (normalized exponential function) 62 loss function operating principle given â€¢ ğ‘¦, expectation (ground truth) â€¢ à·œğ‘¦, prediction, estimation a los/cost/error function ğ¿ ğ‘¦, à·œğ‘¦ â€¢ measures the distance, difference, or discrepancy between ğ‘¦ e à·œğ‘¦ â€¢ when ğ‘¦ e à·œğ‘¦ are very different, then ğ¿ is large (high loss) â€¢ when ğ‘¦ e à·œğ‘¦ are very similar, then ğ¿ is small (low loss) â€¢ when ğ‘¦ e à·œğ‘¦ are equal, then ğ¿ = 0 learning objective â€¢ find parameters of the model that minimize ğ¿ over the validation set (part of the training data, not the test data!) 63 loss function one-hot encoding (output encoding) categorical/nominal variable: takes symbolic values, not numeric ones. examples: â€¢ pet: â€œdogâ€, â€œcatâ€, â€œbirdâ€ â€¢ model: â€œsedanâ€, â€œminivanâ€, â€œbusâ€, â€œtruckâ€ â€¢ dÃ­gitos: â€˜0â€™, â€˜1â€™, â€˜2â€™, â€¦â€˜9â€™ limitation: do not support numerical comparisons/operations one-hot encoding: clase sedan minivan bus truck ğ‘¥ğ‘ ğ‘’ğ‘‘ 1 0 0 0 ğ‘¥ğ‘šğ‘–ğ‘› 0 1 0 0 ğ‘¥ğ‘ğ‘¢ğ‘  0 0 1 0 ğ‘¥ğ‘¡ğ‘Ÿğ‘¢ 0 0 0 1 64 â€¦ Î£ Î£ Î£ Î£ ... ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤ğ¾ ğ‘  ğ‘  ğ‘  ğ‘  ... MNIST ğ¾ = 10 0 0 1 . . . 0 0,1 0 0,6 . . . 0,2 ? â‰ˆ expected probab. ğ‘¦ ğ‘¥ estimated probab. à·œğ‘¦ ğ‘¥ one-hot encoding loss function how to measure the difference between two distributions? softmax ğ‘¥ ğ‘§1 ğ‘§2 ğ‘§3 ğ‘§ğ¾ 65 loss function categorical cross entropy given two probability distributions: â€¢ ğ‘¦ (e.g. expected, ground truth) â€¢ à·œğ‘¦ (e.g. predicted, observed) cross entropy (single sample): ğ¸ ğ‘¦, à·œğ‘¦ = âˆ’ Ïƒğ‘– ğ‘¦ğ‘– log à·ğ‘¦ğ‘– = âˆ’ log à·ğ‘¦ğ‘ effect: â€¢ à·ğ‘¦ğ‘ is the modelâ€™s prediction for the correct class ğ‘ â€¢ if ğ‘¦ and à·œğ‘¦ are similar, then ğ¸ ğ‘¦, à·œğ‘¦ is small â€¢ if ğ‘¦ and à·œğ‘¦ are different, then ğ¸ ğ‘¦, à·œğ‘¦ is large 0 0,5 1 1,5 2 0 0,2 0,4 0,6 0,8 1 cross entropy for ğ‘ = 1 ğ‘ 66 favorable scenario unfavorable scenario ğ‘¦ à·œğ‘¦ âˆ’ğ‘¦ğ‘– log à·ğ‘¦ğ‘– ğ‘¦ à·œğ‘¦ âˆ’ğ‘¦ğ‘– log à·ğ‘¦ğ‘– 0 0,05 0,00 0 0,20 0,00 0 0,05 0,00 0 0,20 0,00 1 0,80 0,10 1 0,20 0,70 0 0,05 0,00 0 0,20 0,00 0 0,05 0,00 0 0,20 0,00 loss 0,10 loss 0,70 two opposite scenarios only the à·ğ‘¦ğ‘– associated with ğ‘¦ğ‘– = 1 contributes (-log 1 is the smallest loss) loss function categorical cross entropy 67 loss function can we pay more attention to minority classes? what if classes are imbalanced? â€¢ many more loss terms from the majority class than from the rest â€¢ all the loss terms matter (wheigh) the same â€¢ the majority class examples dominate the loss function â€¢ the majority class examples dominate gradient propagation â€¢ more model weight updates to favor the majority class â€¢ the model will be more confident in predicting the majority class â€¢ little emphasis on minority classes â€¢ summary: biased classifier learning 68 loss function balanced cross entropy given two probability distributions andâ€¦ â€¢ ğ‘¦ (e.g. expected, ground truth) â€¢ à·œğ‘¦ (e.g. predicted, observed) balanced cross entropy (single sample): ğ¸ ğ‘¦, à·œğ‘¦ = âˆ’ğ›¼ğ‘ log à·ğ‘¦ğ‘ comments: â€¢ à·ğ‘¦ğ‘ is the modelâ€™s prediction for the correct class ğ‘ â€¢ ğ›¼ğ‘ is a class weight related to class ğ‘ â€¢ ğ›¼ğ‘ is inversely proportional to the frequency of class ğ‘ ğ‘ balanced cross entropy 69 loss function balanced cross entropy how to compute class weights? â€¢ by hyperparameter tuning â€¢ by compute_class_weight from sklearn.utils âˆğ‘–= ğ‘› ğ‘˜ Â· ğ‘›ğ‘– where: âˆ’ ğ‘› is the total number of training simples âˆ’ ğ‘›ğ‘– is the number of training samples of class ğ‘– âˆ’ ğ‘˜ is the number of classes 70 loss function paying more attention to hard-to-classify examples how to improve predictions on hard examples â€¢ hard examples = samples classified with less confidence â€¢ strategy: guide learning to focus more on hard examples â€¢ side effect: natural mitigation of biases from imbalanced classes âˆ’ examples from the majority class are usually easy to predict âˆ’ examples from the minority class are usually hard to predict âˆ’ examples from the majority class dominate loss & gradients 71 loss function focal cross entropy given two probability distributions â€¢ ğ‘¦ (e.g. expected, ground truth) â€¢ à·œğ‘¦ (e.g. predicted, observed) focal cross entropy (for a single sample): ğ¸ ğ‘¦, à·œğ‘¦ = âˆ’ 1 âˆ’ à·ğ‘¦ğ‘ ğ›¾ log à·ğ‘¦ğ‘ where: â€¢ à·ğ‘¦ğ‘ is the modelâ€™s prediction for the correct class ğ‘ â€¢ ğ›¾: focal factor (hyperparameter) â€¢ ğ›¾ reduces the contribution of easy examples to the total loss â€¢ typical values for ğ›¾ range from 1 to 5 ğ‘ focal cross entropy 72 how focal loss works? â€¢ when a sample is misclassified (hard examples)â€¦ âˆ’ à·ğ‘¦ğ‘ is small => the modulating factor 1 âˆ’ à·ğ‘¦ğ‘ ğ›¾ is close to 1 âˆ’ the loss term keeps unaffected (it behaves as in cross entropy loss) â€¢ when a sample is correctly classified (easy examples)â€¦ âˆ’ à·ğ‘¦ğ‘ is close to 1 => the modulating factor 1 âˆ’ à·ğ‘¦ğ‘ ğ›¾ is close to 0 âˆ’ the loss term is down weighted, reducing its impact on the loss function â€¢ ğ›¾ adjusts the rate at which easy examples are down-weighted â€¢ ğ›¾ = 0 reduces focal loss to standard cross entropy â€¢ higher values of ğ›¾ encourage the model to focus on harder examples loss function focal cross entropy 73 loss function focal cross entropy ğ›¼-balanced focal loss (single sample): ğ¸ ğ‘¦, à·œğ‘¦ = âˆ’ğ›¼ğ‘ 1 âˆ’ à·ğ‘¦ğ‘ ğ›¾ log à·ğ‘¦ğ‘ comments: â€¢ typical implementation of focal loss â€¢ it usually leads to better results than the unbalanced version. 74 optimization problem MNIST dataset given 60.000 training digit images, with their classes annotated as one-hot vectorsâ€¦ ğ‘‡ = ğ‘¥ğ‘–, ğ‘¦ğ‘– ğ‘–=1,60.000 such that ğ‘¥ğ‘– âˆˆ 0,1 784, ğ‘¦ğ‘– âˆˆ 0,1 10 goal: to find optimal values for the 2.913.920 model parameters ğ‘Šâˆ— = arg min ğ‘Š ğ¿ ğ‘Š , ğ‘Š âˆˆ â„2.913.920 with ğ¿ ğ‘Š being the loss function defined as follows: ğ¿ ğ‘Š = 1 60.000 Ïƒğ‘–=1 60.000 ğ¸ ğ‘¦ğ‘–, à·œğ‘¦ğ‘– = âˆ’ 1 60.000 Ïƒğ‘–=1 60.000 Ïƒğ‘—=1 10 ğ‘¦ğ‘–ğ‘— log à·œğ‘¦ğ‘–ğ‘— 75 optimizer sgd + momentum + weight decay stochastic gradient descent (sgd): ğœƒğ‘¡+1 = ğœƒğ‘¡ âˆ’ ğ›¾ ğœ•ğ¿ ğœ•ğœƒ ğœƒğ‘¡ sgd + momentum + weight decay (ğ›¾ denotes the learning rate): ğœƒğ‘¡+1 = ğœƒğ‘¡ + ğ‘£ğ‘¡ ğ‘£ğ‘¡ = ğ‘šğ‘œğ‘šğ‘’ğ‘›ğ‘¡ğ‘¢ğ‘š Â· ğ‘£ğ‘¡âˆ’1 âˆ’ ğ›¾ ğœ•ğ¿ ğœ•ğœƒ ğœƒğ‘¡ ğ›¾ = ğ›¾ Â· 1 1+ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦Â·ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘ğ‘–Ã³ğ‘› 76 hyperparameters overview hyperparameters are parameters needed to generate the model or to define the training process hyperparameters determine the structure or configuration of the model or characteristics of the learning process; their values are chosen before each training session. their optimal values depend on the complexity of the task, the nature of the data (dimensionality, distribution, quantity, etc.), and the interdependence with other hyperparameters. 77 hyperparameters overview hyper- parameter choice hyperparameter values model generation model model training optimized model 78 hyperparameters of the network architecture defined for MNIST: â€¢ 4 trainable layers â€¢ layer 1: 1024 units, ReLU activation â€¢ layer 1: 1024 units, ReLU activation â€¢ layer 1: 1024 units, ReLU activation â€¢ layer 4: softmax activation hyperparameters fully connected neural network for the MNIST task 79 hyperparameters of the learning process defined for MNIST: â€¢ optimizer: SGD(lr=0.01, decay=1e-6, momentum=0.9) â€¢ loss = 'categorical_crossentropy' â€¢ batch_size = 100 â€¢ epochs = 5 hyperparameters fully connected neural network for the MNIST task 80 summary â€¢ a machine learning algorithm optimizes models from data â€¢ a linear model solves only tasks with linear boundaries â€¢ Perceptron is a linear model of binary classification â€¢ a fully connected multilayer network is composed of a sequence of fully connected layers. â€¢ each unit consists of a linear function + nonlinear activation â€¢ a multilayer network could learn any decision boundary â€¢ backpropagation is an algorithm for training feedforward networks in supervised learning; includes calculation of gradients, not how to use them. 81","libVersion":"0.3.2","langs":""}