{"path":"_aula_virtual/SJK001/T1C5 - Intro NN.pdf","text":"Introduction to Neural Networks 1 Neural Networks ! A mathematical model to solve engineering problems \" Group of connected neurons to realize compositions of non linear functions ! Tasks \" Classification \" Discrimination \" Estimation ! 2 types of networks \" Feed forward Neural Networks \" Recurrent Neural Networks 2 Feed Forward Neural Networks ! The information is propagated from the inputs to the outputs ! Computations of functions from n input variables by compositions of functions ! Time has no role (NO cycle between outputs and inputs) x1 x2 xn….. 1st hidden layer 2nd hidden layer Output layer 3 Recurrent Neural Networks ! Can have arbitrary topologies. i.e. connections between units form a cycle. ! Can model systems with internal states (dynamic ones) ! Delays are associated to a specific weight ! Training is more difficult ! Performance may be problematic \" Stable Outputs may be more difficult to evaluate \" Unexpected behavior (oscillation, chaos, …) x1 x2 1 0 10 1 0 0 0 A graph with a cycle 4 Properties of Neural Networks ! Supervised networks are universal approximators ! Theorem : Any limited function can be approximated by a neural network with a finite number of hidden neurons to an arbitrary precision 5 Supervised Learning ! The desired response of the neural network in function of particular inputs is well known. ! A “teacher” may provide examples and teach the neural network how to fulfill a certain task 6 Unsupervised learning ! Idea : group typical input data in function of resemblance criteria un-known a priori ! Data clustering ! No need of a “teacher” \" The network finds itself the correlations between the data \" Examples of such networks : ! Self-Organizing Feature Maps (SOM) 7 Semi-Supervised Learning ! Halfway between supervised and unsupervised ! The desired response of the neural network is known only for a subset of the inputs, together with unlabeled data. 8 Reinforcement Learning ! The desired response of the neural network in function of particular inputs is not known. ! Only a reward or penalty value is provided with the input examples ! Q-learning: delayed rewards 9 Example Examples of handwritten postal codes drawn from a database available from the US Postal service 10 What is needed to create a NN ? ! Determination of relevant inputs ! Collection of data for the learning and testing phases of the neural network ! Finding the optimum number of hidden nodes ! Learning the parameters ! Evaluate the performances of the network ! If performances are not satisfactory then review all the precedent points 11 Popular neural architectures ! Perceptron ! Multi-Layer Perceptron (MLP) ! Radial Basis Function Network (RBFN) ! Self-Organizing Feature Maps (SOM) ! Other architectures 12 Perceptron ! Rosenblatt (1962) ! Linear separation ! Inputs :Vector of real values* ! Outputs :1 or -1 022110 =++ xcxcc + ++ + + + + + + + + + + + + + + ++ + + + + + + + + + + + + + + + + + 1=y y = −1 0c 1c 2c å 1x 2x1 22110 xcxccv ++= y = step(v) 13 * In the original definition of perceptron inputs typically had only two states: ON and OFF ! The perceptron algorithm converges if examples are linearly separable 14 Perceptron with several outputs A perceptron network with several outputs (from Marsland Fig. 3.3) Perceptron with several outputsMulti-Layer Perceptron ! One or more hidden layers 1st hidden layer 2nd hidden layer Output layer Input data 17 Multi-Layer Perceptron 18 Backpropagation 19 ! Backpropagation is a process involved in training a neural network. ! It involves taking the error rate of a forward propagation and feeding this loss backward through the neural network layers to fine-tune the weights. ! Neural Networks online course ! A radial basis function (RBF) is a real-valued function whose value depends only on the distance from some other point c, called a center, φ(x) = f(||x-c||) ! Any function φ that satisfies the property φ(x) = f(||x-c||) is a radial function. ! The distance is usually the Euclidean distance Radial Basis Functions ( )å = -=- N i ii cxcx 1 22|||| 20 Distance in 2D space: Normalized Gaussian curves with expected value μ and variance σ2. The corresponding parameters are a = 1/(σ√(2π)), b = μ, c = σ 21 ! The popular output of radial basis functions is the Gaussian function: ( ) )exp( 2 ÷÷ ÷ ø ö çç ç è æ - -=-F j j j cx acx s a=1, c1=0.75, c2=3.25 Radial Basis Functions 22 ! The popular output of radial basis functions is the Gaussian function: ( ) )exp( 2 ÷÷ ÷ ø ö çç ç è æ - -=-F j j j cx acx s a=1, c1=0.75, c2=3.25 Radial Basis Functions 23Weight Space = Input Space Sometimes the centers are called weights Radial Basis Functions Network (RBFN) ! Features \" One hidden layer \" The activation of a hidden unit is determined by a radial basis function Radial units Outputs Inputs 2425 Sigmoidal vs. Gaussian Units 26 Tiling the Input Space Receptive fields in Neuroscience! Generally, the hidden unit function is the Gaussian function ! The output Layer is linear: ( )å = -F= K j jj cxWxs 1)( ( ) )exp( 2 ÷÷ ÷ ø ö çç ç è æ - -=-F j j j cx wcx j s 27 RBFN Learning ! The training is performed by deciding on \" How many hidden nodes there should be \" The centers and the sharpness of the Gaussians ! 2 steps (first unsupervised, second supervised) \" In the 1st stage, the input data set is used to determine the parameters of the RBF \" In the 2nd stage, RBFs are kept fixed while the second layer weights are learned ( Simple BP algorithm like for MLPs or Perceptron) 28 Summary ! Neural networks are utilized as statistical tools \" Adjust non linear functions to fulfill a task \" Need of multiple and representative examples but fewer than in other methods ! Neural networks enable to model complex static phenomena (Feed- Forward) as well as dynamic ones (Recurent NN) ! NN are good classifiers BUT \" Good representations of data have to be formulated \" Training vectors must be statistically representative of the entire input space \" Unsupervised techniques can help ! The use of NN needs a good comprehension of the problem 29 - Used for Unsupervised Learning - Weights in neurons must represent a class of pattern one neuron, one class Self Organising Feature Maps (SOM) 30 Four requirements for SOM ! Input pattern presented to all neurons and each produces an output. ! Output: measure of the match between input pattern and pattern stored by neuron. ! A competitive learning strategy selects neuron with largest response. ! A method of reinforcing the largest response 31 Architecture ! The Kohonen network (named after Teuvo Kohonen) is a self-organising network proposed in the 1980s ! Neurons are usually arranged on a 2- dimensional grid ! Inputs are sent to all neurons ! There are no connections between neurons 32 Architecture Kohonen network X 33 Output value ! The output of each neuron is the weighted sum ! There is no threshold or bias ! Input values and weights are normalized 34 “Winner takes all” ! Initially the weights in each neuron are random ! Input values are sent to all the neurons ! The outputs of each neuron are compared ! The “winner” is the neuron with the largest output value 3536Fig. 2.1 Machine Learning, 2nd ed. By S. Marsland Weight Space = Input Space 37 The linear combination is a dot product Training ! Having found the winner, the weights of the winning neuron are adjusted ! Weights of neurons in a surrounding neighbourhood are also adjusted 38 Neighbourhood X Kohonen network neighbourhood 39 Training ! As training progresses the neighbourhood gets smaller ! Weights are adjusted according to the following formula: 40 W X X - W Weight adjustment ! The learning coefficient (alpha) starts with a value of 1 and gradually reduces to 0 ! This has the effect of making big changes to the weights initially, but no changes at the end ! The weights are adjusted so that they more closely resemble the input patterns 41 Weight adjustment details Wv(s + 1) = Wv(s) + Θ(u, v, s) α(s)(D(t) - Wv(s)) Where: •s is the step index, •t an index into the training sample, •u is the index of the best matching unit for D(t), •α(s) is a monotonically decreasing learning coefficient • D(t) is the input vector •Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s 42 Example ! A Kohonen network receives the input pattern 0.6 0.6 0.6. ! Two neurons in the network have weights 0.5 0.3 0.8 and -0.6 –0.5 0.6. ! Which neuron will have its weights adjusted and what will the new values of the weights be if the learning coefficient is 0.4? 43 Answer The weighted sums are 0.96 and –0.3 so the first neuron wins. (0.6 X 0.3 + 0.6 X 0.5 + 0.6 X 0.8 = 0.18 + 0.30 + 0.48 = 0.96) The weights become: w1 = 0.5 + 0.4 *(0.6 – 0.5) w1 = 0.5 + 0.4 * 0.1 = 0.5 + 0.04 = 0.54 w2 = 0.3 + 0.4 *(0.6 – 0.3) w2 = 0.3 + 0.4 * 0.3 = 0.3 + 0.12 = 0.42 w3 = 0.8 + 0.4 *(0.6 – 0.8) w3= 0.8 - 0.4 * 0.2 = 0.8 - 0.08 = 0.72 4445 Visualizing a SOM 46 Kohonen network representing ‘Normal’ space Fault data falling outside ‘Normal’ space Summary ! The Kohonen network is self-organising ! It uses unsupervised training ! All the neurons are connected to the input ! A winner takes all mechanism determines which neuron gets its weights adjusted ! Neurons in a neighbourhood also get adjusted 4748 https://youtu.be/IixbH1gDhsg Visualizing a SOM 49 https://youtu.be/dASyjPQtbS8 Visualizing a SOMProf. Kohonen explains SOMs 50 Prof. Teuvo Kohonen explains Self-Organizing Maps in May 2014 Watch from 12:05 to 16:05, he still uses his classical slides! https://youtu.be/iWPhGKniTew","libVersion":"0.3.2","langs":""}