{"path":"_aula_virtual/SJK002/SJK002-U11-01-ClassificationAndSegmentation.pdf","text":"Deep Learning for various CV tasks Computer Vision (SJK02) Universitat Jaume I Part A: Classification Segmentation Object detection (Image-based) biometrics Part B: Sequence processing Optical flow Action Recognition Self-supervised learning Transformers Convolutional neural networks (CNNs) ~A quick review~ Deep Convolutional Neural Networks [Lecture Notes] Inductive bias • Beliefs/assumptions made by ML models • Constraints in hypotheses space • May help generalise to unseen data https://www.baeldung.com/cs/ml-inductive-bias • Priors • Structure Similar data are closer Linear dependence Inductive bias in CNNs • 2D spatial relations (locality) • Translation invariance (convolution + max pooling) https://samiraabnar.github.io/articles/2020-05/indist Are CNNs rotation invariant? Picasso effect Combining neural and symbolic approaches to solve the Picasso problem: A first step (Displays 2022) Classification: AlexNet, VGG, Inception, ResNet https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5 11 LeNet (1998) AlexNet (2012) • 5 conv, 3 fc, 60 M params GoogLeNet / Inception (2014) VGG (2014) • 16 layers, 96 M params ResNet (2015) • 152 layers, 1 M params MobileNet (2017) • A few hundred layers, fit on smart phones EfficientNet (2019) • Systematic balance between net depth, width, and resolution GoogLeNet (Inception v1) Simple stacking layers is costly and may not be so effective What about regions of interest of very different sizes? https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202 Inception module1x1 convolutions Computation grow with # channels 1x1 filters can change (reduce) # channels 1x1 filter changes #channels, but... How can width x height of activation maps be changed? ~22 layers How many inception modules does it have? www.socrative.com Room 219986 Other Inception versions Inception v2: factorize convolutions (3x3 conv = 1x3 conv, 3x1 conv) Inception v3: batchnorm, label smoothing Inception v5: change of stem part VGG https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96 Very Deep Convolutional Networks for Large-Scale Image Recognition (ICLR 2015) www.socrative.com Room 219986 How many layers (with trainable weights) does VGGNet have? How many object categories can it classify? Kernel of size 3x3 for all conv layers • Unlike previous nets with 5x5, 7x7, 11x11 How many trainable weights are required...? • Input feature map of 100x100x1 • conv layer with 1 filter of size 5x5 And with two conv layers, each with 3x3 filters? The same effective \"receptive field\" can be achieved with more layers of smaller filters, resulting in less parameters What's the benefit of less parameters? • Faster training • Less prone to overfitting • Less memory footprint ResNet Many layers --> vanishing gradient in backpropagation Innovative solution: skip connections https://datagen.tech/guides/computer-vision/resnet/ Residual blockDenseNet (variant of ResNet)EfficientNet https://medium.com/mlearning-ai/understanding-efficientnet-the-most-powerful-cnn-architecture-eaeb40386fad How to scale up models? • Depth-wise? • Width-wise? • Higher resolution? How to decide? • Much human effort • Lot of manual tunning Can we do better? EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019) Different scaling methods Balancing the scale in all the three dimensions improves the overall model performance General idea Architecture search (AutoML) Optimize for... • Max accuracy • Penalize computational requirements • Penalize slow inference Family of EfficientNets Performance comparison Segmentation: UNet 29 U-Net: Convolutional Networks for Biomedical Image Segmentation (MICCAI 2015) CNN vs Fully Convolutional Network (FCN) CNN for global classification: one value per input image FCN: one value per pixel (e.g. pixel-level classification) Fully Convolutional Networks for Semantic Segmentation (PAMI 2017) Architecture contracting part expansive part How to upsample? Interpolation • Nearest neighbor • Bi-linear • Bi-cubic These are fixed, not learnable! Can we do better? Transposed convolutions https://github.com/naokishibuya/deep-learning/blob/master/python/transposed_convolution.ipynb =xO =·=· Skip connections: why? Combines • Rich semantic features of deep layers • Finer localisation of contracting layers Scarce training data Data augmentation Shifts and rotations Gray-level transformations Elastic deformations Region borders are tricky to segment Weighted loss Give higher importance to borders Loss function Per channel soft-max (one channel per label) Cross entropy: penalize wrong labels Is it a correct loss like that? Weight map: compensate class imbalance and \"highlight\" bordersSome results Ground truth Prediction Ground truth Prediction Intersection over Union (IoU)","libVersion":"0.3.2","langs":""}