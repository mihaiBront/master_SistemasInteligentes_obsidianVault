{"path":"_aula_virtual/SJK003/09-clustering.pdf","text":"Department of Computer Languages and Systems Clustering General context Unsupervised learning can be applied if ‚Ä¶ ‚Ä¢ there are no labeled data ‚Ä¢ possibly there are no predefined classes ‚Ä¢ The objective is to divide the set of objects into a number of groups such that objects in the same group are more similar than those in other groups ‚Ä¢ A cluster is, therefore, a collection of objects that are ‚Äúsimilar‚Äù between them and are ‚Äúdissimilar‚Äù to the objects belonging to other clusters Introduction The results of clustering depend on: ‚Ä¢ the clustering algorithm used ‚Ä¢ the available data set ‚Ä¢ the similarity measure used to compare the objects However, the clustering is, in many cases, quite subjective! Introduction (ii): clustering is subjective How many clusters? Two? Six? Four? Introduction (iii): clustering is subjective What is the natural way to group the characters? Women vs. Men Simpsons vs. Springfield School Employees Applications ‚Ä¢ Image processing: automatic segmentation of images, pixel classification in remote sensing imagery, MRI quantization, etc. ‚Ä¢ Bioinformatics: gene-expression analysis to identify groups of genes, or to discover new subgroups of pathologies, etc. ‚Ä¢ Marketing: market segmentation, customer segmentation, etc. ‚Ä¢ Finance: to group either customer or company profiles, fraud detection, etc. ‚Ä¢ Social networking: content understanding, language translation based on the location of the user, etc. ‚Ä¢ Many others: document (or news) categorization, city planning, insurance, spam filter, identification of criminal behaviors, etc. Similarity measures See the slides in Unit 4 (Distance-based Classifiers) Types of clustering algorithms Distance-based methods Grid-based methods Density-based methods Probabilistic methods Partitioning algorithms Hierarchical algorithms C-means, C-medians, C-medoids, C-modes DBSCAN, OPTICS, DENCLUE Agglomerative, Divisive E-M algorithm, AutoClass Other methods Genetic algorithms, Self- organizing maps, Gaussian mixture, Graph- based algorithms, etc. OptiGrid, STING, CLIQUE, WaveCluster Partitioning algorithms ‚Ä¢ these algorithms use prototypes (central points or centroids) to characterize clusters and assign each object to the cluster whose centroid is the closest ‚Ä¢ they require prior knowledge about the number of clusters (classes, C) to divide the data ‚Ä¢ the best known algorithm and one of the most widely used within this group is C-means (some authors call it K-means) The C-means algorithm ‚Ä¢ it partitions the data points into C clusters based upon a similarity measure ‚Ä¢ the object that is closest to the cluster centroid is assigned to that cluster ‚Ä¢ after an iteration, it recalculates the centroids of those clusters, and the process continues until a predefined number of iterations is reached or the cluster centroids do not change after an iteration ‚Ä¢ hyperparameters of the algorithm: ‚Äì initial values of clusters ‚Äì distance measures ‚Äì number of clusters (C) The C-means algorithm (ii) ‚Ä¢ the centroid of a cluster corresponds to the mean (or the central point) of the cluster ‚Ä¢ random initialization of centroids at the start of the algorithm may lead to different clusters when running the algorithm multiple times ‚Ä¢ it computes the dissimilarity between data points using the Euclidean distance metric ‚Ä¢ time complexity is linear O(n) The C-means algorithm (ii) Variations ‚Ä¢ C-medians calculates the median of each cluster to determine the centroids ‚Äì this computes the dissimilarity between data points using the Manhattan distance ‚Äì it is less sensitive to outliers in the data set ‚Ä¢ C-medoids (or PAM, Partitioning Around Medoids), the medoid of a cluster has to be an input object of the data set ‚Äì CLARA (Clustering Large Applications) is an extension to PAM to perform better for large data sets by applying PAM to multiple random samples of the original data and computing the best medoids in each sample ‚Ä¢ C-modes is used with categorical data The C-means algorithm (iv) Randomly select C cluster centroids repeat Calculate the distance between each object and each cluster centroid Assign each object to the nearest cluster centroid Calculate the new centroid of each cluster until the centroids stop moving (i.e. they do not change their positions) The C-means algorithm (v)The optimal value of C ‚Ä¢ a fundamental step for any unsupervised algorithm is to determine the optimal number of clusters C into which the data should be clustered ‚Ä¢ the Elbow method is one of the most popular heuristics to determine this optimal value of C ‚Ä¢ a better method than Elbow is the Silhouette method, which can be used to study the separation distance between the resulting clusters The Elbow method ‚Ä¢ it consists of plotting the sum of squared errors (SSE) as a function of the number of clusters C and picking the elbow (inflection point) of the curve as the optimal value of C ‚Ä¢ Steps: 1. Run the clustering for a range of values for C 2. After each clustering, compute the sum of squared error (SSE) of each cluster 3. Create a line plot of the SSE for each value of C. This plot is called the elbow plot 4. Look at the elbow plot and find the value of C where the SSE begins to decrease linearly. That value is the optimal number of clusters The Elbow method (ii) ‚Ä¢ the SSE is the average distance between each object and the centroid ‚Ä¢ Steps: 1. For each cluster 1. For each object in the cluster 1. Calculate the difference between each object and its cluster centroid 2. Square the difference 2. Compute the sum of all the differences 2. Add up the total distances computed for each cluster to give the sum of squared error (SSE) The Elbow method (iii) ‚Ä¢ with an increase in C, the SSE decreases ‚Ä¢ the inflection point is the point from where the decrease in SSE starts looking linear (there is a sharp and steep fall of the distance) ‚Üí optimal value of C C (number of clusters) Inflection point The Elbow method (iv) ‚Ä¢ what happens when you have a plot with a fairly smooth or linear curve and no obvious elbow point? C (number of clusters) The Silhouette method ‚Ä¢ a method of interpretation and validation of consistency within clusters of data ‚Äì it consists of computing coefficients for each object ‚Äì the silhouette coefficients measure how similar an object is to its own cluster (cohesion) compared to other clusters (separation) ‚Äì the average of the silhouette coefficients for all the objects gives the silhouette score ‚Äì the silhouette value ranges between [-1,1], where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters The Silhouette method (ii) ‚Ä¢ if most objects have a high value, then the clustering is appropriate. If many points have a low or negative value, then the clustering may have too many or too few clusters ‚Äì the silhouette coefficient of +1 indicates that the object is far away from the neighboring clusters ‚Äì the silhouette coefficient of 0 indicates overlapping clusters ‚Äì the silhouette coefficient of <0 indicates that those objects might have been assigned to the wrong cluster or are outliers The Silhouette method (iii) Steps: 1. For each i‚Äôth object in the data set 1. Compute the average distance of i‚Äôth object from all other objects in the same cluster, a(i) 2. Compute the average distance of i‚Äôth object from all the objects in the closest cluster to its own cluster, b(i) 3. Compute the silhouette coefficient of i‚Äôth object, s(i) ùë† ùëñ = ùëè ùëñ ‚àí ùëé(ùëñ) max(ùëè ùëñ , ùëé ùëñ ) 2. Average the silhouette coefficients to get the silhouette score The Silhouette method (iv) ‚Ä¢ similar to the Elbow method, we can pick a range of candidate values of C, run the clustering algorithm for each C, plot the silhouette score vs the number of clusters and take the clustering with the highest silhouette score as the optimal value of C The Silhouette method (v) ‚Ä¢ another way to determine the optimal value of C consists of analyzing a silhouette plot: ‚Äì the x-axis displays the silhouette coefficients (sorted) ‚Äì the y-axis displays the labels of each cluster ‚Ä¢ if all the cluster plots are beyond the average silhouette score, have mostly uniform thickness, and do not have wide fluctuation in size, the number of clusters used is optimal ‚Ä¢ the number of clusters is suboptimal if: ‚Äì the plot for a cluster falls below the average coefficient, or ‚Äì there are wide fluctuations in the size and thickness of the cluster plots The Silhouette method (vi) The silhouette plot for C = 2 shows that the cluster with label 1 is bigger in size due to the grouping of the 3 sub-clusters into one big cluster Average silhouette score = 0.704978749608 The Silhouette method (vii) The silhouette plot shows that C = 3 is bad, as all the points in the cluster with label 1 are below-average silhouette scores Average silhouette score = 0.588200401213 The Silhouette method (viii) For C = 4, all the plots are more or less of similar thickness and hence are of similar sizes, as can be considered as the best C Average silhouette score = 0.650518663273 The Silhouette method (ix) ‚Ä¢ The silhouette plot shows that C = 5 is bad, as all the points in the clusters 2 and 4 are below-average silhouette scores Average silhouette score = 0.563764690262 The Silhouette method (x) ‚Ä¢ The silhouette plot shows that C = 6 is bad, as all the points in the clusters 1, 2, 4 and 5 are below-average silhouette scores Average silhouette score = 0.450466629437 Hierarchical algorithms ‚Ä¢ unsupervised learning algorithms that seek to build a hierarchy between objects while clustering them ‚Ä¢ these are incremental algorithms ‚Ä¢ they do not require the number of clusters to be specified initially ‚Ä¢ it can create very complex shaped clusters ‚Ä¢ time complexity is quadratic O(n2) Hierarchical algorithms (ii) ‚Ä¢ they can be agglomerative or divisive ‚Äì the agglomerative methods are bottom-up methods. Starting from a cluster for each object, they merge the most similar clusters into a new cluster until some stopping criterion is met ‚Äì the divisive methods are top-down. Starting with a single cluster containing all the objects, this cluster is divided until the stopping criterion of the algorithm is verified Hierarchical algorithms (iii) ‚Ä¢ these algorithms create a distance matrix of all the existing clusters and perform the linkage between the clusters depending on the criteria of the linkage: ‚Äì Single Linkage: the distance between the two clusters is the shortest distance between objects in those two clusters ‚Äì Complete Linkage: the distance between the two clusters is the farthest distance (diameter) between objects in those two clusters ‚Äì Average Linkage: the distance between the two clusters is the average distance of every object in the cluster with every object in the other cluster ‚Äì Centroid linkage: the distance between the two clusters is the distance between their centroids Hierarchical algorithms (iv) Single linkage Complete linkage Average linkage Centroid linkage Hierarchical algorithms (v) ‚Ä¢ the output is represented by a dendrogram, which shows the hierarchical relationship between the clusters ‚Ä¢ the similarity between two objects is represented by the height of the node in the dendrogram Hierarchical algorithms (vi) ‚Ä¢ it is not sensitive to outliers Hierarchical algorithms (vii) The agglomerative hierarchical algorithm Compute the distance (similarity) matrix Initialization: each object is a separate cluster repeat Merge the two closest clusters Update the distance matrix until reaching only one cluster An example with 25 objects: ‚Ä¢ we start by assigning each object to separate clusters ‚Ä¢ two closest clusters are then merged till we have just one cluster at the top ‚Ä¢ the height in the dendrogram at which two clusters are merged represents the similarity between two clusters Hierarchical algorithms (viii) An example with 25 objects: ‚Ä¢ the decision of the optimal number of clusters can be chosen by observing the dendrogram ‚Ä¢ the optimal number is the number of vertical lines in the dendrogram cut by a horizontal line that covers the maximum vertical distance (AB) without intersecting a cluster Here the best choice will be 4 clusters Hierarchical algorithms (ix) ‚Ä¢ An example with 4 objects described by the following distance matrix: Hierarchical algorithms (x) Single linkage Hierarchical algorithms (xi) Complete linkage The smallest distance is between 3 and 5 and they get merged into a first cluster '35' Now we replace the 3 and 5 entries by an entry \"35‚Äú: the distance between \"35\" and every other item is the maximum of the distance between this item and 3 and this item and 5. The items with the smallest distance get clustered next ‚Äô24‚Äô Density-based clustering ‚Ä¢ they assume that a cluster is a dense region of points, separated by sparse regions of other dense regions ‚Ä¢ useful when clusters can be of arbitrary shapes, are interleaved or there are noise/outliers in the data ‚Ä¢ the best known and most widely used algorithm within this group is DBSCAN (Density-Based Spatial Clustering of Applications with Noise) DBSCAN ‚Ä¢ clusters are formed by connecting the objects that are densely located in a region ‚Ä¢ it uses concepts such as core point, border point and noise point to come up with clusters ‚Ä¢ time complexity is logarithmic O(n¬∑log n) DBSCAN (ii) ‚Ä¢ Two important parameters: ‚Äì Epsilon (ÔÅ•): the maximum distance between a pair of objects ‚Äì Minimum points (MinPts): minimum number of objects required to form a dense region (cluster) ‚Ä¢ ÔÅ• decides the size of a circle and MinPts decides the minimum number of objects that require being in that circle to consider it a cluster DBSCAN (iii) Meaning of ÔÅ•: ‚Äì two points are considered as neighbors if and only if they are separated by a distance less than or equal to epsilon DBSCAN (iv) ‚Ä¢ objects are categorized into three types: ‚Äì an object is a core point if it has at least MinPts including itself within its ÔÅ•-neighbourhood ‚Äì an object that is within a neighborhood of a core point but it itself cannot be a core point is a border point ‚Äì an object is a noise point if it is neither the a core point nor a border point DBSCAN (v) ‚Ä¢ Initially, the algorithm begins by selecting an object randomly from the data set, and checks if the selected point is a core point ‚Ä¢ For each core point, find all the connected objects ‚Ä¢ Assign each non-core object to the nearest cluster if the cluster is its ÔÅ•- neighbor. Otherwise, assign it to noise ‚Ä¢ The algorithm stops when it explores all the objects one by one and classifies them as either core, border or noise point Summary: Pros ‚Ä¢ C-means ‚Äì can handle large amounts of data ‚Äì simple to implement ‚Ä¢ Hierarchical clustering ‚Äì does not need to specify the initial value ‚Äì easy to implement, scalable and easy to understand ‚Ä¢ DBSCAN ‚Äì does not need to specify the initial value ‚Äì is robust to outliers (noise points) Summary: Cons ‚Ä¢ C-means ‚Äì needs to manually choose the C value ‚Äì sensitive to outliers in the data set ‚Äì dependent on starting point ‚Ä¢ Hierarchical clustering ‚Äì difficulty to handle a large amount of data ‚Äì no backtracking ‚Äì more space and time complexity ‚Ä¢ DBSCAN ‚Äì dependent on the values of ÔÅ• and MinPts ‚Äì struggles to work with high dimensionality data","libVersion":"0.3.2","langs":""}