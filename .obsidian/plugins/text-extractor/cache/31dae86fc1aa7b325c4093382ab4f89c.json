{"path":"_aula_virtual/SJK002/SJK002-U9-2-3D-Vision-ToF.pdf","text":"U9.2. 3D Vision. ToFSystemsSJK002 ComputerVisionMaster in IntelligentSystems2U9.2. 3D Vision. ToFÍndiceStructured light:•Structured light patterns•Correspondence•LimitationsTime of Flight, ToF:•Continuous Wave modulation (CW-modulation)•Pulsed Modulation (PM)LIDAR: Light Detection and Ranging3KinectU9.2. 3D Vision. ToFKinect v1Kinect v24StructuredlightU9.2. 3D Vision. ToF5StructuredlightU9.2. 3D Vision. ToF6StructuredlightU9.2. 3D Vision. ToFWave patternimageforstructuredlight basedonphaseshiftTarget object7StructuredlightU9.2. 3D Vision. ToF3D reconstructionbasedonphaseshift wave patternsof structuredlight8StructuredlightU9.2. 3D Vision. ToFOne component is always null, and the pattern intensity is uniformRedGreenBlueIntensityColumnRandompatternColor pattern9SystemcalibrationCalibrate intrinsic parameters of the IR camera.Calibrate extrinsic parameters with respect to a single calibration grid.Transform the calibration plane in the camera coordinates system.Calibrate the projector in the camera reference system.Move the calibration plane to several scene positions.Rectify the camera-projector system to align the image of the camera with the projector plane.Calibration software, for example: https://github.com/jakobwilm/slstudio/U9.2. 3D Vision. ToF10CorrespondenceAs a stereo vision system, the problem is to find the correspondence between a point in the image of the camera and the corresponding point in the projector pattern.Each pixel of the projector can be signaled to be recognized by:•Temporal signal.•Spatial signal.•Combination of both.Use cross correlation: define a window around the target pixel and use correlation techinques between the window image values and the calibration pattern values.U9.2. 3D Vision. ToF11LimitationsStructured light system calibration needs projector information (pattern, etc.):•Kinect does not provide this information.As a stereo system, it provides limited range estimation.Very sensitive to light saturation:•Bad behaviour in outdoor scenarios, with natural illumination.Problems with partial occlusion and light scattering (hair, wool fabrics, …).Advantage:•Good spatial resolution, usually better than ToF.U9.2. 3D Vision. ToF12Time ofFlight, ToFPrinciple:•Measure the time a light pulse lasts going from the camera light source to an object and back to the sensor.Light travels at a speed 300.000 Km/sToF system:•Pulsed modulation (PM)•Continuous Wave modulation (CW-modulation)U9.2. 3D Vision. ToF13ToFcamerasU9.2. 3D Vision. ToF14ToF. CW-modulationToFdistance:In practiceismeasuredin anindirectway.ContinuousWave modulation(CW-modulation)•Measuresthephasedifferencebetweentheemittedand receivedsignal.•Differentsignalshapes: sinusoidal, square, …•Base oncrosscorrelationbetweensignals. Signalfrequencyisknown.U9.2. 3D Vision. ToF15ToF. CW-modulationAmplitudemodulatedfrequenciesusedbetween10-100 MHz.Relationbetweensignalphaseand ToF:Phaseisdefinedup to : phasewarping.Amplitudeof thereceivedsignaldependsonobjectreflectivityand sensor sensitivity.Amplitudedecreasesas , mainlydueto dispersion.Ambientilluminationisa constantU9.2. 3D Vision. ToF16ToF. CW-modulationEmittedsignal:Receivedsignal:Cross correlation between emitted and received signals:U9.2. 3D Vision. ToF17ToF. CW-modulation4 cubes method:•Estimate values ofin four phases U9.2. 3D Vision. ToF18ToF. CW-modulationU9.2. 3D Vision. ToF19ToF. CW-modulationU9.2. 3D Vision. ToFIn practice:20ToF. CW-modulationU9.2. 3D Vision. ToFE.g. for a frequency f = 30MHz•Unambiguous range from dmin = 0 to dmax = 5 m.For larger distance we must apply “phase unwrapping” techniques.Accuracy increases with modulationfrequency. Several and different ToF frequencies can be combined.Needs significant integration times, over several periods, in order to increase SNR:Blurring effects because of motion.21ToF. CW-modulationhttps://www.analog.com/en/applications/technology/3d-time-of-flight.htmlU9.2. 3D Vision. ToF22ToF. PulsedModulationLaser light pulse of nanoseconds order.Distance is estimated by directly measuring the delaybetween the emitted pulse and received signal.Very short pulses of high optical power can be used:•Pulse irradiance should be much higher than the background illumination.Total emitted energy should be low (class 1).Does not suffer from ambiguous range, as in CW.Can be usedoutdoors.U9.2. 3D Vision. ToF23ToF. PulsedModulationElkhalili et al. (2004). IEEE Transactions on Solid-State Circuits, Vol. 39. No 7, pages 1208-1212U9.2. 3D Vision. ToF24ToF. PulsedModulationU9.2. 3D Vision. ToFDuring the first integrationDuring the second integration, total reflected pulse energy is measured25ToF. PulsedModulationU9.2. 3D Vision. ToFPulsed modulationContinuous Wave modulation CW26ToF. PulsedModulationPrecision:•Based on estimating •Note that it is zero if•Maximum rangeExample: maximum distance 4,5 m.Maximum range can be extended by:•Increment light pulse duration.•Introduce a delay between the emitted pulse start and the beginning of integration time (shutter):Produces a minimum distanceU9.2. 3D Vision. ToF27ToF. PulsedModulationExample of sensor parameters for a ToF system:U9.2. 3D Vision. ToF28ToF. PulsedModulationTiger Eye 3D video cameraU9.2. 3D Vision. ToF","libVersion":"0.3.2","langs":""}