{"path":"_aula_virtual/SJK003/14-overfitting-regularization.pdf","text":"Ramón A. Mollineda Cárdenas overfitting & regularization MACHINE LEARNING University Master's Degree in Intelligent Systems index • overfitting • regularization • L1, L2 • dropout • pooling • batch normalization • data augmentation • case study: CIFAR10 overfitting SHUBHAM JAIN, An Overview of Regularization Techniques in Deep Learning (with Python code). ANALYTICS VIDHYA. 2018. overfitting (model useful ONLY on training data) underfitting (useless model) natural fitting (general purpose model) CIFAR10 overfitting CNN = 2xCon2D(32) + 2xCon2D(64) + 2xCon2D(128) + GlobalMaxPooling + Dense(64) + Dense(10, softmax) • parameters: 295,914 • epochs: 40 • AdamW optimizer • batch size: 200 CIFAR10, 50.000 training samples, 10.000 validation samples excellent performance on training data (50.000) poor performance on validation data (10.000) 295,914 • 60.000 images • image size: 32x32x3 • number of classes: 10 • 6.000 images per class • 50.000 training samples • 10.000 test samples source: https://www.cs.toronto.edu/~kriz/cifar.html CIFAR10regularization definition regularization: introduction of constraints in the model optimization to: o promote simpler useful models o discourage (avoid) complex and excessively flexible models that tend to overfit the training data (overfitting) o help balance the model's ability to fit the training data and generalize to new data. o reduce generalization error (over unseen test data) regularization L1, L2 hypothesis: smaller weights promote simpler models (and simpler models tend to generalize better) p-norm of a vector 𝜃: 𝜃 𝑝 = 𝑝 𝜃1 𝑝 + 𝜃2 𝑝 + ⋯ + 𝜃𝑛 𝑝 regularization L1, L2 – adding penalty to the loss function 𝐿 𝜃 , loss function with 𝜃 being the model parameter set L1 regularization (promotes sparse parameter vectors; compact models) 𝐿′ 𝜃 = 𝐿 𝜃 + 𝜆 𝜃 1 L2 regularization (promotes dense parameter vectors, with small values) 𝐿 ′ 𝜃 = 𝐿 𝜃 + 𝜆 𝜃 2 2 optimization 𝜃∗ = arg min 𝜃 𝐿 ′ 𝜃 𝜆 ≪ 1 hyperparameter that weights the impact of the regularizer in the loss function regularization ≈ constrained optimization from tensorflow.keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.L2(1e-3))) in Keras… o regularization term (penalty) is defined layer by layer o in the example, 𝜆 = 10−3 (equivalent to 1e-3) o 0,1 % of the · 2 2 of the kernel weights is added to the loss function o 𝜆 is a hyperparameter that must/should be adjusted/optimized regularization L1, L2 – adding penalty to the loss function (pooling) pooling map subsampling Illustration of LeCun et al. 1998 from CS231n 2017 Lecture 1 pooling after a convolution operation… 4 1 6 7 5 4 8 6 3 1 3 0 1 2 6 4 2 0 1 4 0 7 0 0 3 4 4 5 5 3 5 4 4 8 2 4 3 2 0 3 3 6 0 3 0 pooling max pooling max pooling alternative: average pooling pool size = 2x2 stride = 2 6x6 3x3 pooling regular setting: o pool size = 2x2 o stride = 2 o padding = 0 Arden Dertat. Applied Deep Learning - Part 4: Convolutional Neural Networks. Towards Data Science. 2017. o applies to individual maps o depth does not change pooling summary • goal: model simplification, better generalization, overfitting reduction • method: propagate local representative activation values • hyperparameters: pool size, stride • trainable parameters: 0 • effects: o if stride == pool size, then no overlap o quadratic size reduction of feature maps o reduces the number of parameters and training effort (simpler models) o applies to individual maps o depth does not change size of the input volume: 𝑊 × 𝐻 × 𝐷 pooling hyperparameters • pool size 𝑃 × 𝑃 • stride 𝑆 size of the output volume: 𝑊′ × 𝐻′ × 𝐷′ • 𝑊′ = 𝑊−𝑃 𝑆 + 1 • 𝐻′ = 𝐻−𝑃 𝑆 + 1 • 𝐷′ = 𝐷 pooling size of the output volume model = Sequential() model.add(Conv2D(32, (3, 3), padding='same', input_shape=train_X.shape[1:])) # input_shape=(32,32,3) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3), padding='same')) model.add(Activation('relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3), padding='same')) model.add(Activation('relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) … max pooling Keras max pooling Keras summary max pooling effect on model generalization model regularized with MaxPooling non-regularized model image: https://www.stanforddaily.com/category/sports/spring-sports/mens-rowing/ rowers develop dependencies (co-adaptation); this limits individual ability e.g. collective performance could “hide” individual limitations dropout metaphor what if during training we asked certain randomly chosen rowers to stop rowing for short periods of time? Do active rowers develop better individual abilities? X X X dropout metaphor image: https://www.stanforddaily.com/category/sports/spring-sports/mens-rowing/ It would be like training multiple sub-teams (active rowers in each period), which we would combine during the competition (test). …and perhaps each rower would be less dependent on his fellow rowers. X X X dropout metaphor image: https://www.stanforddaily.com/category/sports/spring-sports/mens-rowing/ Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" The Journal of Machine Learning Research 15.1 (2014): 1929-1958. dropout (in neural networks) means deactivate/cancel units in training phase, both in forward and backward executions, to prevent co-adaptation of units and to force the network to learn more robust features dropout neural networks dropout (in neural networks) is equivalent to a combination of subnets (models) with shared parameters. dropout neural networks original network in training o in each network update (one batch processing), in the affected layer, each unit can be disabled with probability 𝑝 (along with its connections) o therefore 𝑝 would be the expected fraction of units disabled (on average) o disabled units are not involved in gradient computation in test o all units and connections are involved in data processing o activations 𝑦 scale according dropout rate 𝑝: 𝑦 = 1 − 𝑝 · 𝑦 goal: to adapt each unit’s activation to the percentage of times (on average) the unit was involved in updating the weights (mini-batch) dropout how it works: training and test phases o dropout forces the network to learn more robust features, obtained from multiple network configurations o configuration ≡ subnet with a particular combination of units o given 𝐻 hidden units, there are 2𝐻 possible subnets/models o the result is equivalent to a combination of subnets (ensemble) o an ensemble is generally more effective than individual models o dropout requires more iterations to converge (more unstable process), but each one requires less computational time dropout summary o not recommended for use in convolutional networks: batch normalization has been shown to regularize better (see details) o less need: CNNs have fewer parameters than fully connected networks o inappropriate: a feature map contains information with strong spatial dependencies; dropout affects these relationships o however, it sometimes helps improve a model CNN o recommended in fully connected networks o dropout rates are recommended between 0,2 and 0,5 o the larger the network, the more effective the use of dropout o the original article recommends learning rates higher than usual dropout guidelines … model.add(Conv2D(128, (3, 3), padding='same')) model.add(Activation('relu')) model.add(Conv2D(128, (3, 3), padding='same')) model.add(Activation('relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) model.add(GlobalMaxPooling2D()) model.add(keras.layers.Dropout(0.3)) model.add(Dense(64, activation='relu')) model.add(keras.layers.Dropout(0.3)) model.add(Dense(num_classes, activation='softmax')) dropout rate (Keras): 0.3 in the example rate: float between 0 and 1. Fraction of the input units to drop. dropout Keras dropout Keras summary dropout effect on model generalization non-regularized model model regularized with MaxPooling model regularized with MaxPooling + Dropout batch normalization what we already know input data normalization is a good practice (generally a need) ... (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') x_test = x_test.astype('float32') x_train /= 255 # normalizes [0..255] --> [0..1] x_test /= 255 # normalizes [0..255] --> [0..1] benefits of normalizing input data o promotes loss functions with simpler surfaces (gradient descent is a search process over this surface) o promotes similar parameter distributions o speeds up parameter optimization o default hyperparameters make sense (e.g. learning rate) batch normalization what we already know input data normalization is a good practice (generally a need) benefits of normalizing input data o promotes loss functions with simpler surfaces (gradient descent is a search process over this surface) o promotes similar parameter distributions o speeds up parameter optimization o default hyperparameters make sense (e.g. learning rate) (train_X, train_Y), (val_X, val_Y) = cifar10.load_data() train_X = train_X.astype('float32’) val_X = val_X.astype('float32’) train_X /= 255 val_X /= 255 Σ Σ Σ 𝑔 𝑔 𝑔 Σ Σ 𝑔 𝑔 Σ 𝑥1 𝑥2 ത𝑦 batch normalization what we already know parameters benefited input layer 1 hidden layer 2 output layer 4 hidden layer 3 𝑔: activation function Σ Σ Σ 𝑔 𝑔 𝑔 Σ Σ 𝑔 𝑔 Σ 𝑥1 𝑥2 ത𝑦 batch normalization rationale what if we normalize the input to layer 3? could these weights be more easily learned? input layer 1 hidden layer 2 output layer 4 hidden layer 3 𝑔: activation function Σ Σ Σ 𝑔 𝑔 𝑔 Σ Σ 𝑔 𝑔 Σ 𝑥1 𝑥2 ത𝑦 batch normalization during training normalization stable distribution of normalized activations, independent of previous activations (more stable parameter optimization) input layer 1 hidden layer 2 output layer 4 hidden layer 3 𝑔: activation function Σ Σ Σ 𝑔 𝑔 𝑔 Σ Σ 𝑔 𝑔 Σ 𝑥1 𝑥2 ത𝑦 batch normalization during training common practice: normalize here (see original paper [1]) there is an open debate on whether to normalize: before or after nonlinear activation [1] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015). input layer 1 hidden layer 2 output layer 4 hidden layer 3 batch normalization during training ℬ denotes the current batch non-normalized output batch normalization during training 1. Let ℬ = 𝑥𝑖 𝑖=1 𝑛 be the activations resulting from processing a current batch 𝜇ℬ = 1 𝑛 σ𝑖=1 𝑛 𝑥𝑖 𝜎ℬ 2 = 1 𝑛 σ𝑖=1 𝑛 𝑥𝑖 − 𝜇ℬ 2 2. Let 𝑥𝑖 be a 𝑑-dimensional vector 𝑥𝑖 = 𝑥𝑖 (1), … , 𝑥𝑖 (𝑑) ො𝑥𝑖 𝑘 = 𝑥𝑖 (𝑘)− 𝜇ℬ (𝑘) 𝜎ℬ (𝑘)2+𝜀 , with 𝜀 being a small constant to avoid division by zero. 3. Standardization: ො𝑥𝑖 𝑘 ~𝒩 0,1 , too restrictive!! batch normalization during training 4. Batch Normalization (BN) output: scaling and shifting distributions of standardized activations: 𝑦𝑖 (𝑘) = 𝛾(𝑘) · ො𝑥𝑖 (𝑘) + 𝛽(𝑘) details: • 𝑦𝑖 𝑘 ~𝒩 𝛽(𝑘), 𝛾(𝑘)2 , BN output distribution (flexible domain) • 𝛽(𝑘) and 𝛾(𝑘) are trainable parameters via backpropagation • separate process for each activation batch normalization during inference output in inference (in tf.keras): 𝑦 = 𝛾 · (𝑥 − 𝜇)/ 𝜎 + 𝜀 + 𝛽 details: • 𝜇 = 𝜇 · 𝑚𝑜𝑚𝑒𝑚𝑡𝑢𝑚 + 𝜇ℬ · (1 − 𝑚𝑜𝑚𝑒𝑚𝑡𝑢𝑚), ℬ is the current batch • 𝜎 = 𝜎 · 𝑚𝑜𝑚𝑒𝑚𝑡𝑢𝑚 + 𝜎ℬ 2 · (1 − 𝑚𝑜𝑚𝑒𝑚𝑡𝑢𝑚) • 𝜇 and 𝜎 are moving averages of mean and variance on training batches • 𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚 is a parameter of BN (default value 0.99) • 𝜀 is a parameter of BN (default value 0.001) • 𝛾 and 𝛽 are the parameters learned at training batch normalization summary • mean 𝜇ℬ and variance 𝜎ℬ are parameters calculated for each batch • scale 𝛾 and shift 𝛽 are parameters learned by the network • a layer with batch normalization might not need a bias vector, since the displacement of the activations can be achieved through 𝛽 • activation distribution independent of previous layer activations • abrupt changes in activation distributions are avoided (internal covariate shift) => stable learning of weights • deep network training time is reduced • higher learning rates can be used (less risk of high activations) Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015). model = Sequential() model.add(Conv2D(32, (3, 3), padding='same', input_shape=train_X.shape[1:])) # input_shape=(32,32,3) model.add(keras.layers.BatchNormalization()) model.add(Activation('relu')) model.add(Conv2D(32, (3, 3), padding='same')) model.add(keras.layers.BatchNormalization()) model.add(Activation('relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(64, (3, 3), padding='same')) model.add(keras.layers.BatchNormalization()) model.add(Activation('relu')) model.add(Conv2D(64, (3, 3), padding='same')) model.add(keras.layers.BatchNormalization()) model.add(Activation('relu')) model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) … batch normalization Keras typical pattern: between the convolution and the nonlinear activation function batch normalization Keras summary remarks o a batch normalization per convolution (output map) o 4 parameters per function (2 calculated + 2 learned) o total number of BN functions: 448 o total number of BN parameters: 1,792 o number of calculated (non- trainable): 896 (50%) o number of learned (trainable): 896 (50%) ... batch normalization effect on model generalization non-regularized model model regularized with MaxPooling + Dropout + Batch Normalization +MP +DO +MP data augmentation motivation the generated images retain the nature of the original object: a pen remains pen regardless of size, orientation or position. original image randomly generated artificial images (fakes) data augmentation motivation o the invariance to scale, orientation and shift of an image is a desirable property in any automatic vision method (e.g. classification, detection, localization, interpretation, etc.) o neural networks have an innate ability to learn invariant models, given suitable examples o small rotations or shifts of an image do not (generally) change its nature, nor its class o adding rescaled, rotated, or shifted versions of actual images is one way of adding prior knowledge data augmentation what is it about data augmentation stochastic process of generating new artificial images, which recreate different scales, orientations and shifts of real images, without losing their nature data augmentation negative example o some images (red frame) do NOT retain the nature of the original '3'; this design does not admit flips or large rotations o if we used them, the model would learn unrealistic patterns of '3' original image randomly generated artificial images (fakes) data augmentation positive example generator used in this example (only shifts and rotation ≤ 20º): datagen = ImageDataGenerator( width_shift_range=0.2, height_shift_range=0.2, rotation_range=20) original image randomly generated artificial images (fakes) from tensorflow.keras.preprocessing.image import ImageDataGenerator … # image generator with which the previous examples were created # includes 'flips' and rotations up to 90º, inappropriate for '3'! datagen = ImageDataGenerator( rotation_range=90, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, vertical_flip=True, fill_mode='nearest') … # training the model from batches of artificial samples generated # by 'flow', taking as reference real samples of (train_X, train_Y) history = model.fit(datagen.flow(train_X, train_Y, batch_size = batch_size), steps_per_epoch= len(train_X) / batch_size, epochs=epochs, validation_data=(val_X, val_Y), verbose=1) data augmentation Keras model_augmentation = keras.Sequential( [ keras.layers.Rescaling(1 / 255.0), keras.layers.RandomRotation(0.2), keras.layers.RandomZoom(height_factor=0.1, width_factor=0.1), keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1), keras.layers.RandomFlip(\"horizontal\"), ], name=\"augmentation_model\", ) data augmentation Keras data augmentation benefits o avoid underfitting: • augment small data sets o avoid overfitting: • increment diversity of training data (intraclass variance) • unique (artificial) images are generated in each training batch (they are not repeated in any other batch or epoch) data augmentation effect on model generalization non-regularized model model regularized with MaxPooling + Dropout + Batch Normalization + Data Augmentation +MP +DO +MP +MP +DO +BN overfitting MP: Max Pooling BN: Batch Normalization DA: Data Augmentation CIFAR10 two opposite scenarios healthy learning pattern CNN + MP + BN + DA (1.838.346 parámetros) training validation ConvNetJS CIFAR-10 demo https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html","libVersion":"0.3.2","langs":""}