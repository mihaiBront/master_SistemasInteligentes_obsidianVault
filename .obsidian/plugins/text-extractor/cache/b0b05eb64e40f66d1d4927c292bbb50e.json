{"path":"_aula_virtual/SJK003/07-linearRegression.pdf","text":"Department of Computer Languages and Systems Linear Regression Introduction • Linear regressions tries to approximate a relationship between one dependent (response, outcome) and one or more independent (predictor, explanatory) variable(s): – Simple linear regression: it concerns the study of only one independent variable – Multiple linear regression: it concerns the study of two or more independent variables Introduction Purposes of regression analysis – Explanatory: A regression analysis explains the relationship between the response and predictor variables – Predictive: A regression model can give a point estimate of the response variable based on the value of the predictors Simple linear regression • We want to find the linear relationship between a dependent variable 𝑦 and an independent variable 𝑥 by fitting a linear function to our observed data (𝑥𝑖, 𝑦𝑖): 𝑦 = 𝑏0 + 𝑏1𝑥 + 𝜀 – This is a line where 𝑦 is the variable we want to predict, 𝑥 is the input variable we know and 𝑏0 and 𝑏1are the regression coefficients that we need to estimate – 𝑏0 is called the intercept (or bias) because it determines where the line intercepts the y-axis. The 𝑏1term is called the slope because it defines the slope of the line. 𝜀 is the residual error. Multiple linear regression • The equation of a multiple linear regression model is: 𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + ⋯ + 𝛽𝑚𝑥𝑚 + 𝜀 where 𝑦 is the dependent variable, 𝑥𝑖 is the independent variable 𝑖, 𝛽0 is the constant of the equation, 𝛽𝑖 is the regression coefficient associated to the variable 𝑥𝑖, and 𝜀 is the residual error Multiple linear regression (ii) • If we have a sample with a total of 𝑛 observations, the multiple linear regression model can be expressed in matrix form: 𝑦1 ⋮ 𝑦𝑛 = 1 ⋮ 1 𝑥11 ⋯ 𝑥1𝑚 ⋮ ⋱ ⋮ 𝑥𝑛1 ⋯ 𝑥𝑛𝑚 ∙ 𝛽1 ⋮ 𝛽𝑚 + 𝜀1 ⋮ 𝜀𝑛 that is, 𝑌 = 𝑋𝛽 + 𝜀 Types of relationships Deterministic relationship Statistical relationship when there is a mathematical formula that allows the values ​​of one of the variables to be calculated from the values ​​of the other 𝐹𝑎ℎ𝑟 = 32 + 1.8 𝐶𝑒𝑙𝑠 when there is no mathematical expression that relates the variables exactly 𝑤𝑒𝑖𝑔ℎ𝑡 = 6.1376 ℎ𝑒𝑖𝑔ℎ𝑡 − 266.53 y = 1.8x + 32 0 20 40 60 80 100 120 140 0 10 20 30 40 50 60Fahrenheit Celsius y = 6.1376x - 266.53 100 120 140 160 180 200 220 62 64 66 68 70 72 74 76weight height Fundamentals of simple linear regression • Hypothesis (assumptions): – Linearity: the response variable is a linear combination of parameters (regression coefficients) and the predictor variable – Normality: the variables follow a symmetric and Gaussian distribution • Preliminary assessment of the strength of the hypothesis: – Linearity: linear correlation coefficient – Normality: regression plot (scatterplot), histogram, Q-Q plot Linear correlation coefficient Pearson’s correlation coefficient: a measure of linear correlation between two variables: 𝑟 = 𝑆𝑥𝑦 𝑆𝑥𝑆𝑦 = σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 𝑦𝑖− ത𝑦 σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 2 σ𝑖=1 𝑛 𝑦𝑖− ത𝑦 2 −1 ≤ 𝑟 ≤ 1 where is the sample covariance 𝑆𝑥𝑦 and 𝑆𝑥 and 𝑆𝑦 are the standard deviations of the variables 𝑥 and 𝑦 Pearson’s correlation coefficient • If 𝒓 = −𝟏, then there is a perfect negative linear relationship between 𝑥 and 𝑦 • If 𝒓 = 𝟏, then there is a perfect positive linear relationship between 𝑥 and 𝑦 • If 𝒓 = 𝟎, then there is no linear relationship between 𝑥 and 𝑦 All other values of 𝑟 tell us that the relationship between 𝑥 and 𝑦 is not perfect. A reasonable rule is to say that: • the relationship is weak if 𝟎 < 𝒓 < 𝟎. 𝟓 • the relationship is strong if 𝟎. 𝟖 < 𝒓 < 𝟏 Pearson’s correlation coefficient By DenisBoigelot, https://commons.wikimedia.org/w/index.php?curid=15165296 Pearson’s correlation coefficient X Y X Y 0.1 0.2 0.5 0.2 0.2 0.4 0.4 0.4 0.3 0.6 0.3 0.6 0.4 0.8 0.2 0.8 0.5 1 0.1 1 r 1.00 r -1.00 X Y X Y 0.5 0.2 1 0.2 0.4 0.4 100 0.4 0.5 0.6 1000 0.6 0.4 0.8 10000 0.8 0.5 1 100000 1 r 0.00 r 0.76 Regression plot (scatterplot)Regression plot (scatterplot) • (a) and (b): the points fit perfectly on the straight line, so that we have a linear relationship between the two variables: – (a): with negative slope, which indicates that as X increases, Y becomes smaller and smaller. – (b) with positive slope. • (c): it is possible to ensure the existence of a strong relationship between the two variables, but it is not a linear relationship. • (d): the points are completely dispersed, so that there is no type of relationship between the variables. • (e) and (f): there is some kind of linear relationship between the two variables: – (e): a type of linear dependence with a negative slope. – (f): linear relationship with positive slope, but not as strong as the previous case. Q-Q plot It plots the cumulative distribution functions. If they come from the same distribution (in this case, the normal distribution), the points should fall approximately on a straight line Parameter estimation After making the scatter diagram and observing a possible linear relationship between the two variables, the next step will be to find the equation of the line that best fits the cloud of points: the regression line. The equation of the regression line will be defined by determining the values ​​of the intercept (𝑏0) and slope (𝑏1): 𝑦 = 𝑏0 + 𝑏1𝑥 Parameter estimation When we use ෝ𝑦𝑖 = 𝑏0 + 𝑏1𝑥𝑖 to predict the actual response 𝑦𝑖, we make a prediction error (or residual error) of size: 𝑒𝑖 = 𝑦𝑖 − ෝ𝑦𝑖 = 𝑦𝑖 − 𝑏0 + 𝑏1𝑥𝑖 The \"best fitting line\" will be the one that minimizes differences between observed and predicted data (ordinary least squares criterion): 𝐿 = ෍ 𝑖=1 𝑛 (𝑦𝑖 − ො𝑦𝑖)2 = ෍ 𝑖=1 𝑛 (𝑦𝑖 − 𝑏0 + 𝑏1𝑥𝑖 )2 Parameter estimation We have to calculate 𝑏0 and 𝑏1 for the equation of the line that minimizes the sum of the squared prediction errors: by applying derivatives with respect to 𝑏0 and 𝑏1, and setting them equal to 0 𝜕𝐿 𝜕𝑏0 = 0 and 𝜕𝐿 𝜕𝑏1 = 0 we obtain 𝑏1 = 𝑆𝑥𝑦 𝑆𝑥 2 = ൗ σ𝑖=1 𝑛 𝑥𝑖−ഥ𝑥 𝑦𝑖−ഥ𝑦 𝑛−1 ൘σ 𝑖=1 𝑛 𝑥𝑖−ഥ𝑥 2 𝑛−1 = σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 𝑦𝑖− ത𝑦 σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 2 𝑏0 = ത𝑦 − 𝑏1 ҧ𝑥 𝑆𝑥𝑦 is the covariance of observations 𝑥𝑖, 𝑦𝑖 𝑆𝑥 2 is the variance of observation 𝑥𝑖 Parameter estimation Because the formulas for 𝑏0 and 𝑏1 are derived using the least squares criterion, the resulting equation ෝ𝑦𝑖 = 𝑏0 + 𝑏1𝑥𝑖 is referred to as the least squares regression line (or least squares line) Note that the least squares line passes through the point ( ҧ𝑥, ത𝑦), since when 𝑥 = ҧ𝑥, then 𝑦 = 𝑏0 + 𝑏1 ҧ𝑥 = ത𝑦 − 𝑏1 ҧ𝑥 + 𝑏1 ҧ𝑥= ത𝑦 𝑏0 = ത𝑦 − 𝑏1 ҧ𝑥 Parameter estimation From now on, we will write the regression line as follows: ො𝑦 = 𝛽0 + 𝛽1𝑥 where the parameters of the line 𝛽0 and 𝛽1 are given by: 𝛽1 = σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 𝑦𝑖− ത𝑦 σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 2 and 𝛽0 = ത𝑦 − 𝛽1 ҧ𝑥 Interpreting the slope Meaning: the slope 𝛽1 represents the expected mean change in the response variable for each unit of change in the predictor variable An example: A positive slope value indicating that weight increases with height at a rate of 0.979 kg per centimeter height1 = 1 → weight1 = –95.133091 height2 = 2 → weight2 = –94.154082 weight2 – weight1 = –94.154082 – (–95.133091) = 0.979009 w𝑒𝑖𝑔ℎ𝑡 = 0.979009 ℎ𝑒𝑖𝑔ℎ𝑡 − 96.1121 Interpreting the slope • if 𝛽1 = 0 , then there is no relationship between the variables ො𝑦 = 𝛽0 + 𝛽1𝑥 = 𝛽0 + 0 ∙ 𝑥 = 𝛽0 (horizontal “no relationship” line in the regression plot) Interpreting the intercept Meaning: the intercept 𝛽0 only makes sense when the predictor variable can equals 0, Then, it is simply the expected value of the response variable at that value An example where the intercept has no intrinsic meaning: a person who is 0 cm tall is predicted to weigh -96.1121 kg! 𝑤𝑒𝑖𝑔ℎ𝑡 = 0.979009 ℎ𝑒𝑖𝑔ℎ𝑡 − 96.1121 Interpreting the residual error X Y xi yi b0 + b1xi y = b0 + b1x (xi , yi) Predicted value Observed value Residual An example latitude predicts mortality from skin cancer Regression line: an example Linear function: 𝑴𝒐𝒓𝒕 = 𝟑𝟖𝟗. 𝟐 − 𝟓. 𝟗𝟕𝟖 𝑳𝒂𝒕 Lat: independent (predictor) variable Mort: dependent (response) variable Regression coefficients: • −5.978: slope of the line • 389.2: intercept of the line Making predictions: an example Once we have obtained the \"estimated regression coefficients\" 𝛽0 and 𝛽1, we can predict future responses • a common use of the estimated regression line: ෝ𝑦𝑖 = 389.2 − 5.978𝑥𝑖 • predict (mean) mortality of a state at 38 degrees north latitude: ෝ𝑦𝑖 = 389.2 − 5.978 × 38 = 132.2 Making predictions: an example 𝑴𝒐𝒓𝒕 = 𝟑𝟖𝟗. 𝟐 − 𝟓. 𝟗𝟕𝟖 𝑳𝒂𝒕 State latitude (predictor var.) mortality (response var.) mortality’ (prediction) residual error Florida 28,0 197 221,8 -24,8 Texas 31,5 229 200,9 28,1 California 37,5 182 165,0 17,0 Washington, DC 39,0 177 156,1 21,0 New York 43,0 152 132,2 19,8 South Dakota 44,8 86 121,4 -35,4 Minnesota 46,0 116 114,2 1,8 Making predictions: an example Mort = 382.8 - 5.8525 Lat 75 95 115 135 155 175 195 215 235 Mort = 389.2 - 5,978 Lat 75 95 115 135 155 175 195 215 235 25 30 35 40 45 50 regression equation considering the data of all available states regression equation considering the first 39 states in alphabetical order + prediction (•) over the remaining 10 states + observed data (○) Residual error: an example residual error (or prediction error): 𝑒𝑖 = 𝑦𝑖 − ෝ𝑦𝑖 y = -5.8525x + 382,8 75 95 115 135 155 175 195 215 235 residual error for Texas (prediction error) 𝑥𝑖 = 31,5, 𝑦𝑖 = 229 ෝ𝑦𝑖 = 198.45 𝑒𝑖 = 229 − 198.45 = 30.55 Validation: the quality of the fit given a linear function inferred from observed data (a sample) ... • is there a good fit to the observed data? – residual errors → residual plot – coefficient of determination (or R-squared value or R2) – observed data vs. predicted data • is the inferred model adequate for the general problem? – hypothesis test for the population correlation coefficient Validation: the quality of the fit Does the linear function fit the data well? Is it suitable for the observed distribution? y = 30.527x + 338.47 400 500 600 700 800 900 1000 1100 1200 1300 1400 0 5 10 15 20 25 30 data generation: 𝑦 = 𝑥2 + 500 + 𝑚 where 𝑚 is a random number between -30 and 30 Validation: the quality of the fit exponential function, more appropriate and better fitted than the linear function y = 423.83e0.038x 400 500 600 700 800 900 1000 1100 1200 1300 1400 0 5 10 15 20 25 30 Validation: the quality of the fit Does the linear function fit the data well? Is it suitable for the observed distribution? data generation: 𝑦 = 𝑥 + 10 + 𝑚 where 𝑚 is a random number between -10 and 10 y = 1.1087x + 7.8663 0 5 10 15 20 25 30 35 40 45 0 5 10 15 20 25 30 Residual plot residual errors can be analysed using residual plots: the residual values (𝑒𝑖) on the y-axis and the predicted values ( ෝ𝑦𝑖) on the x-axis If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate Residual plot If we project all the residual values onto the y-axis, we end up with a normally distributed curve. This satisfies the assumption that the residuals of a regression model are independent and normally distributed Residual plot regression plot 75 95 115 135 155 175 195 215 235 25 30 35 40 45 50 -150 -100 -50 0 50 100 150 residual plot 𝑒𝑟𝑟𝑜𝑟 ~ 𝑁 0, 𝜎2 Residual plot regression plot residual plot non-random error 𝑒𝑟𝑟𝑜𝑟 ≠ 0 -300 -200 -100 0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 0 5 10 15 20 25 30 Residual plot regression plot residual plot random error random deviations 𝑒𝑟𝑟𝑜𝑟 ≈ 0 -20 -10 0 10 20 0 10 20 30 40 50 0 3 6 9 12 15 18 21 24 27 Coefficient of determination (R2) given ... 𝑆𝑆𝑅 = σ𝑖=1 𝑛 ෝ𝑦𝑖 − ത𝑦 2, regression sum of squares it quantifies how far the estimated regression line, ෝ𝑦𝑖, is from the sample mean ത𝑦 (horizontal “no relationship” line) 𝑆𝑆𝐸 = σ𝑖=1 𝑛 𝑒𝑖 2 = σ𝑖=1 𝑛 𝑦𝑖 − ෝ𝑦𝑖 2, error sum of squares it quantifies how much the data points, 𝑦𝑖, vary around the estimated regression line, ෝ𝑦𝑖 𝑆𝑆𝑇 = σ𝑖=1 𝑛 𝑦𝑖 − ത𝑦 2, total sum of squares it quantifies how much the data points, 𝑦𝑖, vary around their mean, ത𝑦 Coefficient of determination (R2) assuming that 𝑆𝑆𝑇 = SSR + SSE ... 𝑅2 = 𝑆𝑆𝑅 𝑆𝑆𝑇 = 1 − 𝑆𝑆𝐸 𝑆𝑆𝑇 • 𝑅2 is a proportion, its value ranges between 0 and 1 • 𝑅2 measures the proportion of variation in the dependent variable explained by the independent variable • 𝑅2 indicates how close the data is to the regression line: the closer it is to 1, the better the fit • 𝑅2 does not indicate whether the regression model is adequate; you can get small values with a good model, and vice versa Coefficient of determination (R2) • if 𝑅2 = 1, all of the data points fall perfectly on the regression line. The response variable can be perfectly explained without error by the predictor variable • The residuals are 0 and so is the sum of their squares: 𝑆𝑆𝑅 = 𝑆𝑆𝑇 • if 𝑅2 = 0, the estimated regression line is perfectly horizontal. The response variable cannot be explained by the predictor variable at all • the sum of residuals is maximum and we have 𝑆𝑆𝐸 = 𝑆𝑆𝑇 Coefficient of determination (R2) interpretation of 𝑅2 𝑅2  100 percent of the variance in 𝑦 is ‘explained by’ the variation in the predictor variable 𝑥 R² = 0.6798 75 95 115 135 155 175 195 215 235 25 30 35 40 45 50Mortality Latitude 68% of the variance in skin cancer mortality is due to or explained by latitude Coefficient of determination (R2) Relationship between 𝑅2 and 𝑟: • in simple linear regression, 𝑹𝟐 = 𝒓𝟐 – this relationship helps us understand why we have considered a value of 𝑟 = 0.5 to be weak. This value will represent 𝑅2 = 0.25, that is, the regression model only explains 25% of the variability of the observations! – 𝑟 gives us more information than 𝑅2, since the sign of 𝑟 tells us whether the relationship is positive or negative. With the value of 𝑟 we can always calculate the value of 𝑅2, but conversely the value of the sign will always remain indeterminate unless we know the slope of the line Observed data vs. predicted data regression plot observed data (x-axis) vs. predictions (y-axis) 75 95 115 135 155 175 195 215 235 75 95 115 135 155 175 195 215 235 75 95 115 135 155 175 195 215 235 25 30 35 40 45 50 Hypothesis testing the correlation coefficient 𝑟 and the coefficient of determination 𝑅2 summarize the strength of a linear relationship in samples only if we obtained a different sample of observations 𝑥𝑖, 𝑦𝑖 , we could obtain different 𝑟 and 𝑅2 values and different regression lines → potentially different conclusions we have to draw conclusions about populations, not just samples so, we have to conduct a hypothesis test (t-test) to see if the population slope 𝜷𝟏 is significant Note that the intercept 𝛽0 determines the average value of the variable Y for a value of X equal to zero. Since it does not always have a realistic interpretation in the context of the problem, we only make statistical inference about the slope Hypothesis testing t-test allows validating the linear relationship between the predictor variable and the response variable 𝐻0: 𝛽1 = 0, the null hypothesis 𝐻𝑎: 𝛽1 ≠ 0, the alternative hypothesis intuition if 𝛽1 = 0, there is not a linear relationship between 𝑥 and 𝑦 if 𝛽1 ≠ 0, there is a significant linear relationship between the variables objective to reject the null hypothesis (i.e., the variable 𝑥 has an influence on the variable 𝑦 and therefore, there is a linear relationship between the two variables) Hypothesis testing Steps for hypothesis testing: 1. specify the null and alternative hypotheses (see previous slide) 2. set a significance level  (typical values 0.01, 0.05) 3. construct a statistic 𝑇 to test the null hypothesis 𝐻0 4. define a decision rule to reject, or not, the null hypothesis 𝐻0 Hypothesis testing Steps for hypothesis testing: 1. specify the null and alternative hypotheses (see previous slide) 2. set a significance level  (typical values 0.01, 0.05) 3. construct a statistic 𝑇 to test the null hypothesis 𝐻0 4. define a decision rule to reject, or not, the null hypothesis 𝐻0 Hypothesis testing Steps for hypothesis testing: 1. specify the null and alternative hypotheses (see previous slide) 2. set a significance level  (typical values 0.01, 0.05) 3. construct a statistic 𝑇 to test the null hypothesis 𝐻0 𝑇 = 𝛽1 𝑆𝐸(𝛽1) where 𝛽 is the estimated coefficient of the population slope, and 𝑆𝐸 𝛽1 = 𝑀𝑆𝐸 σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 2 = Τσ𝑖=1 𝑛 𝑦𝑖− ො𝑦𝑖 2 𝑛−2 σ𝑖=1 𝑛 𝑥𝑖− ҧ𝑥 2 is the standard error of the estimated coefficient of the population slope Hypothesis testing Steps for hypothesis testing: 1. specify the null and alternative hypotheses (see previous slide) 2. set a significance level  (typical values 0.01, 0.05) 3. construct a statistic 𝑇 to test the null hypothesis 𝐻0 4. define a decision rule to reject, or not, the null hypothesis 𝐻0 • 𝑇 follows a Student’s t-distribution with 𝑛 − 2 degrees of freedom, where 𝑛 is the number of data points (– 2 because we have two parameters, 𝛽0 and 𝛽1) • we calculate the 𝑝-value: 𝑃 𝑡𝑛−2 > 𝑇 = 2𝑃 𝑡𝑛−2 > 𝑇 • we reject the null hypothesis 𝐻0 if 𝑝-value ≤ 𝛼 Hypothesis testing interpreting the result of the hypothesis test – the 𝑝-value indicates how likely is it to get such an extreme 𝑇 value if the null hypothesis 𝐻0 is true – if 𝑝-value ≤ 𝛼 means that there is sufficient evidence at the level  to conclude that there is a linear relationship in the population between the predictor and response variables → we reject the null hypothesis 𝐻0 – rejecting 𝐻0 entails accepting 𝐻𝑎 → there is a significant linear relationship between the variables – given 𝑇 and 𝑛 − 2, the 𝑝-value is obtained from the Student’s t- distribution tables or from some web sites","libVersion":"0.3.2","langs":""}