{"path":"_aula_virtual/SJK003/13-cnn.pdf","text":"Ramón A. Mollineda Cárdenas convolutional neural networks MACHINE LEARNING University Master's Degree in Intelligent Systems what we already know fully-connected neural network 𝑔𝑗𝑥 𝑦𝑊1 𝑊2 𝑔𝑗 Σ... 𝑤1 𝑤2 𝑤3 𝑤𝑑 𝑥1 𝑥2 𝑥3 𝑥𝑑 𝑥0 = 1 𝑤0 𝑔𝑗𝑥 𝑦𝑊1 𝑊2 𝑦𝑘 = ෍ 𝑗=1 𝐻 𝑤2 𝑗𝑘 · 𝑔 ෍ 𝑖=1 𝑑 𝑤1 𝑖𝑗 · 𝑥𝑖 = 𝑊2 · 𝑔 𝑊1 · 𝑥 𝑑, input space dimension 𝐻, number of hidden layer units 𝑔, nonlinear activation function what we already know fully-connected neural network 𝑔𝑗𝑥 𝑦𝑊1 𝑊2 • input: 𝑥 ∈ ℝ𝑁 • two-layer neural network • 𝑔 non-linear functions (e.g. sigmoide, relu) • output: linear combination of 𝑔𝑗 what we already know fully-connected neural network 𝑥1 𝑥2 𝑥3 . . . 𝑥784 𝑔𝑗 𝑦1 𝑦2 … 𝑦10 𝑊1 𝑊2 28 28 0 0 1 0 0 0 0 0 0 0 MNIST digits case study what we already know fully-connected neural network convolutional neural network (CNN) overview CNN: end-to-end solutions transforms primary representations into categories Illustration of LeCun et al. 1998 from CS231n 2017 Lecture 1 low level features (edges, spots, details) higher spatial resolution, lower depth high level features (patterns, abstractions) lower spatial resolution, lower depth (a) classification (b) regression (image reconstruction) convolutional neural network (CNN) architectural patterns feature extraction convolutional base discriminant function (a) classification (b) regression (image reconstruction) feature extraction convolutional base convolutional neural network (CNN) architectural patterns ImageNet challenge Large Scale Visual Recognition Challenge (ILSVRC) Deng, Jia et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conference on Computer Vision and Pattern Recognition (2009): 248-255. (http://image-net.org) task: given an image, identify the main object in the image training data: • 1,200,000 labeled images • 1,000 final classes/categories (ground truth) • one class label per image (identifies the main object) validation and test data: • 150,000 pictures (from Flickr and other search engines) • 50,000 labeled validation images • 100,000 unlabeled test data success/hit: the correct class (ground truth) is one of the 5 most probable classes found by the model (top-5 error) ImageNet challenge Large Scale Visual Recognition Challenge (ILSVRC) ImageNet challenge Large Scale Visual Recognition Challenge (ILSVRC) 26 16,4 11,7 7,3 6,7 3,6 3,1 0 5 10 15 20 25 30 2011 (XRCE) 2012 (AlexNet) 2013 (ZF) 2014 (VGG) 2014 (GoogLeNet) 2015 (ResNet) 2016 (GoogLeNet-v4) ImageNet classification top-5 error (%) ImageNet challenge winning models Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012 (ver). AlexNet (winning model in 2012) top-1 error: 38,1% top-5 error: 16,4% layers: 8 ¡60 millones de parámetros! ImageNet challenge Large Scale Visual Recognition Challenge (ILSVRC) Szegedy, Christian, et al. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" AAAI. Vol. 4. 2017 (ver). Inception v4 (winning model in 2016) top-1 error: 16,4% top-5 error: 3,1% layers ≈ 170 (trainable) ¡43 million parameters! stem inception A inception B inception C ImageNet challenge Large Scale Visual Recognition Challenge (ILSVRC) Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. convolution Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/. convolution digital representation of images grayscale images • array of elements called image pixels • matrix dimensions are called the image resolution • each cell/element/pixel stores a value: its intensity or gray level • intensity/gray levels take integer values between 0 and 255 • dark values are close to 0 (black); light greys, close to 255 (white) color images • array of elements called image pixels • matrix dimensions are called the image resolution • each pixel stores its intensity in 3 channels/values: Red, Green, Blue • each R, G or B intensity takes value between 0 and 255 • each final color is the result of combining the R, G or B values Source: Wikipedia (link) convolution digital representation of images let... • 𝑝𝑖,𝑗,𝑘 be the intensity value of channel 𝑘 ∈ 𝑅, 𝐺, 𝐵 for pixel 𝑖, 𝑗 • for grayscale images, 𝑝𝑖,𝑗,𝑘 reduces to 𝑝𝑖,𝑗 normalization [0, 1] normalization [-0.5, 0.5] ෤𝑝𝑖,𝑗,𝑘 = 𝑝𝑖,𝑗,𝑘 255 ෤𝑝𝑖,𝑗,𝑘 = 𝑝𝑖,𝑗,𝑘 255 − 0.5 convolution digital representation of images Source: Paul-Louis Pröve, An Introduction to different Types of Convolutions in Deep Learning. Medium.com (link) convolution 2D convolution stride = 1 padding = 1 kernel size = 3 input output convolution 2D convolution Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/. kernel input map image patch, image receptive field output map result of kernel operation convolution operation • scalar product • inner product • dot product a convolution operates on “grid-shaped” data: • a time series, interpretable as a 1D grid • an image, interpretable as a 2D/3D grid convolution 2D convolution convolution operation properties • measures spatial correlations in the image receptive field • translation invariance: detects pattern at any position in the image Source: Translational variance in convolutional neural networks. StackExchanged (link). convolution 2D convolution convolution 2D convolution: Sobel-Feldman (edge detection) ∗ −1 0 +1 −2 0 +2 −1 0 +1 ∗ −1 −2 −1 0 0 0 +1 +2 +1 kernel to detect horizontal changes (edges) 𝐺𝑥2 + 𝐺𝑦2 𝐺𝑥 𝐺𝑦 𝐺 kernel to detect vertical changes (edges) 𝐺𝑥, approximate gradient of 𝐼 𝑥, 𝑦 in 𝑥 𝐺𝑦, approximate gradient of 𝐼 𝑥, 𝑦 in 𝑦 𝐺, approximate gradient of 𝐼 𝑥, 𝑦 [[0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 1.4 2. 1.4 0. 0. 0. 0. 0. ] [0. 3.2 2. 3.2 0. 0. 0. 0. 0. ] [0. 4. 0. 4. 0. 1.4 2. 1.4 0. ] [0. 3.2 2. 3.2 0. 3.2 2. 3.2 0. ] [0. 1.4 2. 1.4 0. 4. 0. 4. 0. ] [0. 0. 0. 0. 0. 3.2 2. 3.2 0. ] [0. 0. 0. 0. 0. 1.4 2. 1.4 0. ] [0. 0. 0. 0. 0. 0. 0. 0. 0. ]] [[0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 0. -1. 0. 0. 0. 0. 0.] [ 0. 3. 0. -3. 0. 0. 0. 0. 0.] [ 0. 4. 0. -4. 0. 1. 0. -1. 0.] [ 0. 3. 0. -3. 0. 3. 0. -3. 0.] [ 0. 1. 0. -1. 0. 4. 0. -4. 0.] [ 0. 0. 0. 0. 0. 3. 0. -3. 0.] [ 0. 0. 0. 0. 0. 1. 0. -1. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 1. 2. 1. 0. 0. 0. 0. 0.] [ 0. 1. 2. 1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 1. 2. 1. 0.] [ 0. -1. -2. -1. 0. 1. 2. 1. 0.] [ 0. -1. -2. -1. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. -1. -2. -1. 0.] [ 0. 0. 0. 0. 0. -1. -2. -1. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0.]] Do you notice the invariance to the position of the pattern? image 𝐺𝑥 𝐺𝑦 𝐺 numeric output convolution 2D convolution: Sobel-Feldman (edge detection) easy task corner of my office with mostly straight edges and a few textures convolution 2D convolution: Sobel-Feldman (edge detection) convolution 2D convolution: Sobel-Feldman (edge detection) convolution 2D convolution: Sobel-Feldman (edge detection) difficult task corner of my office with edges of diverse geometry and rich textures convolution 2D convolution: Sobel-Feldman (edge detection) ∗ 1 9 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ∗ 1 25 ∗ 1 16 1 2 1 2 4 2 1 2 1 1 4 7 4 1 4 16 26 16 4 7 26 41 26 7 4 16 26 16 4 1 4 7 4 1 ∗ 1 273 mediana mean filter 3x3 mean filter 5x5 gaussian filter 3x3 gaussian filter 5x5 median filter 3x3 median filter 5x5 50% salt + 50% pepper mediana convolution 2D convolution: smoothing images 50% salt + 50% pepper convolution 2D convolution: smoothing images convolution 2D convolution: smoothing images median filter gaussian filter noisy source image convolution 2D convolution: stride, padding 0 1 2 3 4 0 1 2 3 4 0 1 2 0 1 2 · 0 1 2 0 1 2 ⚫ ⚫ stride = 1 padding = 0 kernel size 3 × 3 input size 5 × 5 output size (5-3+1) × (5-3+1) 0 1 2 3 4 0 1 2 3 4 0 1 2 0 1 2 · 0 1 0 1 ⚫ ⚫ input size 5 × 5 output size 5 − 3 2 + 1, 5 − 3 2 + 1 convolution 2D convolution: stride, padding stride = 2 padding = 0 kernel size 3 × 3 · 0 0 0 0 0 0 0 0 ⚫ ⚫ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 𝑝𝑎𝑑𝑑𝑖𝑛𝑔 ≤ 𝑡𝑎𝑚𝑎ñ𝑜 𝑘𝑒𝑟𝑛𝑒𝑙 2 convolution 2D convolution: stride, padding stride = 1 padding = 1 Source: Paul-Louis Pröve, An Introduction to different Types of Convolutions in Deep Learning. Medium.com (link) convolution 2D convolution: stride, padding stride = 1 padding = 1 input output Based on: Michael Nielsen (2019). Neural Networks and Deep Learning, online book (+). input size output sizekerneloutput map (output units/neurons) input map (input units/neurons) stride = 1 padding = 2 convolution 2D convolution 0 1 2 3 4 0 1 2 3 4 0 1 2 0 1 2 𝑖 𝑗 𝑞 𝑝 𝑟 𝑟 0 1 2 0 1 2 3D input volume 3D kernel 2D output map · convolution 2D convolution (on 3D maps) Source: https://webstyleguide.com/wsg1/graphics/display_primer.html multi-spectral image Source: wikipedia.org Magnetic Resonance Image (MRI) Source: istockphoto.com multi-channel images convolution 2D convolution (on multi-channel images) sagittal planeaxial plane 𝑂 𝑖, 𝑗 = 𝑊 · 𝐼 𝑖, 𝑗 = σ𝑝 σ𝑞 σ𝑟 𝐼𝑖+𝑝,𝑗+𝑞,𝑟 · 𝑊𝑝,𝑞,𝑟 + 𝑏 Let… 𝐼, be a 3D input volume 𝑊, be a 3D kernel (filter, operator) 𝑂, be the 2D output map convolution 2D convolution 𝑂 𝑖, 𝑗, 𝑐 = 𝑊𝑐 · 𝐼 𝑖, 𝑗, 𝑐 = σ𝑝 σ𝑞 σ𝑟 𝐼𝑖+𝑝,𝑗+𝑞,𝑟 · 𝑊𝑝,𝑞,𝑟 𝑐 + 𝑏𝑐 Let… 𝐼, be a 3D input volume 𝑊𝑐, the 𝑐-th 3D kernel of a set of 𝐶 kernels 𝑂, be the 3D output volume convolution 2D convolution example above: with a 5x5x3 input map, 3x3x3 kernel, padding 0, and stride 1, the output map size was 3x3x1. what would have been the dimension of the output map with a kernel... • 2x2x3 -> ? • 4x4x3 -> ? • 5x5x3 -> ? generalization: given an HxWxC input, and KxKxC kernel, padding P, and stride S, what would be the size of the output map? • KxKxC -> ? convolution 2D convolution: practical exercise example above: with a 5x5x3 input map, 3x3x3 kernel, padding 0, and stride 1, the output map size was 3x3x1. what would have been the dimension of the output map with a kernel... • 2x2x3 -> 4x4x1 • 4x4x3 -> 2x2x1 • 5x5x3 -> 1x1x1 generalization: given an HxWxC input, and KxKxC kernel, padding P, and stride S, what would be the size of the output map? • KxKxC -> convolution 2D convolution: practical exercise 𝐻 − 𝐾 + 2𝑃 𝑆 + 1, 𝑊 − 𝐾 + 2𝑃 𝑆 + 1 sparse connectivity (when kernel is smaller than input) dense connectivity convolution convolutional connection pattern vs dense pattern Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/. kernel parameters input map output map CNN Convolutional Neural Network FCN Fully Connected Network FCN Fully Connected Network CNN Convolutional Neural Network shared parameters o the no. param. depends on the kernel o fewer parameters are stored o fewer calculations are made one parameter per connection o the number of parameters depends on the units of the connected layers convolution convolutional connection pattern vs dense pattern Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/. FCN Fully Connected Network CNN Convolutional Neural Network it keeps working connections/parameters do not depend on input size What if you change the size of the network input? it fails connections/parameters DO depend on input size convolution convolutional connection pattern vs dense pattern Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/. o x2, x3, x4 (input units) are known as the receptive field of s3 o deeper layer units are related to larger receptive fields o units in very deep layers may be indirectly connected to all or most of the input image CNN Convolutional Neural Network convolution convolutional connection pattern vs dense pattern Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning. MIT Press (2016). Disponible en http://www.deeplearningbook.org/.Source: https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/ convolutional layer 2D convolutions Source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2 convolutional layer 2D convolutions kernel Are you able to describe this scenario? Source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2 input data volume 32x32x10 output data volume (result of applying 10 kernels over input volume) kernel 3D kernel spatially small operator; depth equal to that of the entrance 32x32x1 map result from applying 1 kernel convolutional layer 2D convolutions Source: https://towardsdatascience.com/applied-deep-learning- part-4-convolutional-neural-networks-584bc134c1e2 o operator: 5x5x3 kernel o input (to the convolution operation): 5x5x3 block of the input volume o operation: scalar product o result: scalar (1x1x1) o kernel trainable params = 75 + 1 o kernel depth = input volumen depth o output volumen depth = number of kernels the kernel moves over the input volume and convolves with each position convolutional layer 2D convolutions case study convolutional neural network for the MNIST task 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 input (28, 28, 1) conv 5x5 flatten conv 5x5 0 0 1 0 0 0 0 0 0 0 dense layer (100) output layer (10) 1 2 3 4 5 40 case study convolutional neural network for the MNIST task 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 input (28, 28, 1) conv 5x5 flatten conv 5x5 0 0 1 0 0 0 0 0 0 0 dense layer (100) output layer (10) 1 2 3 4 5 40 What size is the output of each layer? (assume zero padding) 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 0 0 1 0 0 0 0 0 0 0 1 2 3 4 5 40 24 24 20 20 16.000 (20·20·40) case study convolutional neural network for the MNIST task input (28, 28, 1) conv 5x5 flatten conv 5x5 dense layer (100) output layer (10) 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 0 0 1 0 0 0 0 0 0 0 1 2 3 4 5 40 24 24 20 20 16.000 (20·20·40) case study convolutional neural network for the MNIST task input (28, 28, 1) conv 5x5 flatten conv 5x5 dense layer (100) output layer (10) How many parameters does the model have? 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 0 0 1 0 0 0 0 0 0 0 1 2 3 4 5 40 24 24 20 20 16.000 (20·20·40) case study convolutional neural network for the MNIST task input (28, 28, 1) conv 5x5 flatten conv 5x5 dense layer (100) output layer (10) 20 x 5 x 5 + 20 + 40 x 5 x 5 x 20 + 40 + 16.000 x 100 + 100 + 100 x 10 + 10 = 520 + 20.040 + 1.600.100 + 1.010 = 1.621.670 trainable parameters 𝑦0 𝑦1 … 𝑦9 28 28 1 2 3 4 5 20 0 0 1 0 0 0 0 0 0 0 1 2 3 4 5 40 24 24 20 20 16.000 (20·20·40) case study convolutional neural network for the MNIST task input (28, 28, 1) conv 5x5 flatten conv 5x5 dense layer (100) output layer (10) 20 x 5 x 5 + 20 + 40 x 5 x 5 x 20 + 40 + 16.000 x 100 + 100 + 100 x 10 + 10 = 520 + 20.040 + 1.600.100 + 1.010 = 1.621.670 trainable parameters implementation in Keras... • Keras is a Python library for creating neural networks (deep learning framework) • interface to high-level modules based on Tensorflow, Theano, or the Microsoft Cognitive Toolkit • it allows you to combine predefined pieces common in neural networks such as layers, objective functions, activation functions, optimizers, etc. case study convolutional neural network for the MNIST task (X_train,y_train), (X_test, y_test) = mnist.load_data() X_train = X_train.reshape(60000, 28, 28, 1).astype('float32')/255 X_test = X_test.reshape(10000, 28, 28, 1).astype('float32')/255 y_train = to_categorical(y_train) y_test = to_categorical(y_test) model = Sequential() model.add(Conv2D(20, kernel_size=5, activation='relu', input_shape=(28, 28, 1))) model.add(Conv2D(40, kernel_size=5, activation='relu')) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(10, activation='softmax')) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20) simplified script case study convolutional neural network for the MNIST task (X_train,y_train), (X_test, y_test) = mnist.load_data() X_train = X_train.reshape(60000, 28, 28, 1).astype('float32')/255 X_test = X_test.reshape(10000, 28, 28, 1).astype('float32')/255 y_train = to_categorical(y_train) y_test = to_categorical(y_test) model = Sequential() model.add(Conv2D(20, kernel_size=5, activation='relu', input_shape=(28, 28, 1))) model.add(Conv2D(40, kernel_size=5, activation='relu')) model.add(Flatten()) model.add(Dense(100, activation='relu')) model.add(Dense(10, activation='softmax')) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20) hyperparameters case study convolutional neural network for the MNIST task activation 'relu' • Rectified Linear Unit (ReLU) • activation function of hidden layers activation 'softmax' • output layer activation function • probability distribution over K outputs loss 'categorical_crossentropy' • one-hot vector + softmax + cross entropy • loss function • measures discrepancy between two distributions optimizer 'adam' • adam = adaptive moment estimation • parameter optimization method case study convolutional neural network for the MNIST task optimizer Adam: Adaptive Moment Estimation gradient descent 𝜃𝑡+1 = 𝜃𝑡 − 𝛾 𝜕𝐿 𝜕𝜃 𝜃𝑡 Adam: 𝜃𝑡+1 = 𝜃𝑡 − 𝜂 ෝ𝑚𝑡 ො𝑣𝑡 + 𝜖 = 𝜃𝑡 − 𝜂 ො𝑣𝑡 + 𝜖 ෝ𝑚𝑡 ෝ𝑚𝑡 = 𝑚𝑡 1−𝛽1 𝑡 ො𝑣𝑡 = 𝑣𝑡 1−𝛽2 𝑡 𝑚𝑡 = 𝛽1𝑚𝑡−1 + 1 − 𝛽1 𝑔𝑡 𝑣𝑡 = 𝛽2𝑣𝑡−1 + 1 − 𝛽2 𝑔𝑡 2 𝑔𝑡 = 𝜕𝐿 𝜕𝜃 𝜃𝑡 typical values 𝛽1 = 0.9 𝛽2 = 0.999 𝜖 = 10−8 Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. Int. Conf. on Learning Representations, p. 1–13, 2015. • Adam adjusts the learning rate for each parameter dynamically • it is an evolution of Stochastic Gradient Descent with momentum • 𝑚𝑡 is a first-order moment estimate (𝑚0 = 0): a moving average of the gradient to smooth out updates and provide directionality • 𝑣𝑡 is a second-order moment estimate (𝑣0 = 0): a moving average of the squared gradients that characterizes the variance of 𝑔 (see details) • 𝑣𝑡 scales the learning rate adaptively for the parameter 𝜃𝑡; the adapted rate is inversely proportional to the variance of the gradient • ෝ𝑚𝑡 y ො𝑣𝑡 are unbiased estimators of the first- and second-order moments • the epsilon (ε) value is a small constant added for numerical stability. • Adam has become a go-to choice for many machine learning practitioners optimizer Adam: considerations optimizer AdamW: Adaptive Moment Estimation with Weight decay AdamW: 𝜃𝑡+1 = 1 − 𝜆 𝜃𝑡 − 𝜂 ෝ𝑚𝑡 ො𝑣𝑡 + 𝜖 = 1 − 𝜆 𝜃𝑡 − 𝜂 ො𝑣𝑡 + 𝜖 ෝ𝑚𝑡 𝜆 is a weight decay applied directly to weights, decoupling it from gradient computation in L2 regularization; typically leads to better generalization ෝ𝑚𝑡 = 𝑚𝑡 1−𝛽1 𝑡 ො𝑣𝑡 = 𝑣𝑡 1−𝛽2 𝑡 𝑚𝑡 = 𝛽1𝑚𝑡−1 + 1 − 𝛽1 𝑔𝑡 𝑣𝑡 = 𝛽2𝑣𝑡−1 + 1 − 𝛽2 𝑔𝑡 2 𝑔𝑡 = 𝜕𝐿 𝜕𝜃 𝜃𝑡 typical values 𝛽1 = 0.9 𝛽2 = 0.999 𝜖 = 10−8 𝜆 = 0.004 (default value in K) CNN hyperparameters (MNIST case study) • 4 trainable layers: 2 convolutional + 2 dense • layer 1: 20 x 5x5 filters, ReLU activation • layer 2: 40 x 5x5 filters, ReLU activation • layer 3: dense with 100 units, ReLU activation • layer 4: output layer, softmax activation • padding: 0 • stride: 1 hyperparameters convolutional neural network for the MNIST task ConvNetJS CIFAR-10 demo https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html application scope • CNN is able to detect and characterize local spatial patterns • typical applications on CNN-based images: o image classification o detection (and location) of objects in images o image reconstruction, noise removal o image fusion o image super-resolution o counting objects by regression … limitations • CNN might not be useful if data has no spatial or temporal order CNN application scope","libVersion":"0.3.2","langs":""}