{"path":"_aula_virtual/SJK003/15-transfer-learning.pdf","text":"Ramón A. Mollineda Cárdenas deep transfer learning MACHINE LEARNING University Master's Degree in Intelligent Systems a quote After supervised learning, transfer learning will be the next driver of ML commercial success. Andrew Ng data scientist driver = clave, motor, fuerza impulsora… a sentence in german please, refrain german speakers! Mein Gott ist gut any hypotheses about its meaning? natural knowledge transfer humans naturally take advantage of previously acquired knowledge and skills, to learn how to solve new related tasks. if you know… it’ll be easier for you to learn... relationship I have… It is long… Where is my book? My name is Juan. Ich habe… Es ist lang… Wo ist mein buch? Mein name ist Juan. english and german are germanic languages trumpet and French horn are brass instruments transfer learning takes advantage of the knowledge acquired in the solution of a task to solve other similar tasks; this way you avoid learning from scratch. natural knowledge transfer english & german source: https://www.europelanguagejobs.com/blog/german-english-cognates context traditional machine learning learn from scratch: starts with arbitrary parameter values and uses data from a target task to adjust them large data set from task A untrained model small data set from task B (B is related to A) optimized model for A untrained model model of B does NOT take advantage of learning from A optimized model for B context tradicional machine learning deep learning inherits this practice… promise – deep networks are universal approximators, capable of solving any problem fine print – they depend on millions of parameters to optimize! Myth – therefore, you will not be able to create a DL solution unless you have sufficient amounts of data from the target task context deep learning X context deep learning deep learning inherits this practice… promise – deep networks are universal approximators, capable of solving any problem fine print – they depend on millions of parameters to optimize! Myth – therefore, you will not be able to create a DL solution unless you have sufficient amounts of data from the target task context deep learning deep learning inherits this practice… promise – deep networks are universal approximators, capable of solving any problem fine print – they depend on millions of parameters to optimize! truth – it's possible to… • transfer learned representations to related tasks • learn good representations from unlabeled data • learn representations common to different domains traditional scenario learn from scratch with insufficient data => overfitting (cheap solution, but generally useless) context deep learning UNICORN scenario learn from scratch with unlimited resources… • unlimited labeled data • unlimited computing power • unlimited time (optimal solution at a prohibitive cost) fuente: flickr context deep learning motivation aim good generalization at a low cost with limited amounts of data (good and affordable solution) definition Transfer learning … refers to the situation where what has been learned in one setting … is exploited to improve generalization in another setting. I. Goodfellow, Y. Bengio & A. Courville. Deep Learning, 2016. transfer learning – popular strategy pretrained model, optimized for A, but useful in related tasks adapted model, optimized for Baccurate, fast and cheap solution knowledge (features, weights) knowledge transfer large data set from task A small data set from task B (B is related to A) untrained model optimized model for A another quote… Deep Learning on Steroids with the Power of Knowledge Transfer! Dipanjan Sarkar author of the book “Hands-On Transfer Learning with Python” (link) formal definition domain and task Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–1359 (link). domain 𝒟 = 𝒳, 𝑃 𝑋 𝒳, feature space 𝑃 𝑋 , marginal probability distribution 𝑋 = 𝑥1, … , 𝑥𝑛 , 𝑥𝑖 ∈ 𝒳 example: document classification (bag-of-words) • 𝒳, space for all possible documents • 𝑋, training document set, 𝑋 ⊂ 𝒳 • 𝑥𝑖, document representation 𝑖 task 𝒯 = 𝒴, 𝑃 𝑦 𝑥 𝒴, label space 𝑃 𝑦 𝑥 , conditional probability distribution (prediction) 𝑃 𝑦 𝑥 learns from 𝑥𝑖, 𝑦𝑖 , 𝑥𝑖 ∈ 𝑋, 𝑦𝑖 ∈ 𝒴 example: document classification (bag-of-words) • in sentiment analysis, 𝒴 = {Positive, Negative, Neutral} given • 𝒟𝑠, 𝒯𝑠 , source domain and task • 𝒟𝑡, 𝒯𝑡 , target domain and task transfer learning aims at… • learning 𝑃𝑡 𝑦 𝑥 using knowledge from 𝒟𝑠 and/or 𝒯𝑠 • cases: 𝒟𝑠 ≠ 𝒟𝑡 xor 𝒯𝑠 ≠ 𝒯𝑡 • generally, the number of labeled samples from 𝒯𝑡 is significantly less than the number of labeled samples from 𝒯𝑠 formal definition domain and task Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–1359 (link). scenarios domain 𝒟 = 𝒳, 𝑃 𝑋 𝒳𝑠 ≠ 𝒳𝑡 e.g., documents written in different languages 𝑃 𝑋𝑠 ≠ 𝑃 𝑋𝑡 e.g., documents discuss different topics (politics, science…); it is known as domain adaptation. task 𝒯 = 𝒴, 𝑃 𝑦 𝑥 𝒴𝑠 ≠ 𝒴𝑡 e.g., documents are assigned to different classes/labels; it usually occurs together with the following scenario. 𝑃𝑠 𝑦 𝑥 ≠ 𝑃𝑡 𝑦 𝑥 e.g., different distributions of documents among classes formal definition scenarios Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–1359 (link). TL methods in deep learning • transfer of pretrained features, when 𝒴𝑠 ≠ 𝒴𝑡 o simple and popular method o it takes advantage of the convolutional base trained in a related task 𝒯𝑠 • domain adaptation, when 𝑃 𝑋𝑠 ≠ 𝑃 𝑋𝑡 o it learns common representation to 𝒟𝑠 y 𝒟𝑡 o it minimizes classification error on 𝒯𝑠 o it maximizes confusion between 𝒟𝑠 y 𝒟𝑡 (on common representation) pre-trained CNN features CNN architecture convolutional layers learning hierarchical representations (feature spaces) optimized for 𝒯𝑠 dense layers classification: it learns 𝑃𝑠 𝑦 𝑥 prediction (according to 𝒴𝑠) generic (image) features (task independent) specific (image) features (task dependent) feature extraction decision making input (e.g. image in 𝒟𝑠) learning processes from 𝒟𝑠, 𝒯𝑠 => pre-trained models input (e.g. image in 𝒟𝑡) convolutional layers optimized representations for 𝒯𝑠 prediction (according to 𝒴𝑡) any classifier (e.g. SVM, KNN) learns 𝑃𝑡 𝑦 𝑥 input (e.g. image in 𝒟𝑡) convolutional layers optimized for 𝒯𝑠 (generic features) prediction (according to 𝒴𝑡) un clasificador (e.g. SVM, KNN) aprende 𝑃𝑡 𝑦 𝑥 added dense layers learns 𝑃𝑡 𝑦 𝑥 conv. layers optimiz. for 𝒯𝑡 (specific feat.) input (e.g. image in 𝒟𝑡) prediction (according to 𝒴𝑡) un clasificador (e.g. SVM, KNN) aprende 𝑃𝑡 𝑦 𝑥 added dense layers learns 𝑃𝑡 𝑦 𝑥 convolutional layers optimized representations for 𝒯𝑠 exploitation on 𝒟𝑡, 𝒯𝑡 / scenario: 𝒟𝑠 ≈ 𝒟𝑡, 𝒴𝑠 ≠ 𝒴𝑡 𝑙𝑟 = 0 𝑙𝑟 = 0 𝑙𝑟 = 0 𝑙𝑟 > 0 𝑙𝑟 > 0 𝑙𝑟 > 0 𝑙𝑟 - learning rate pre-trained CNN features strategies pre-trained CNN features typical workflow • There is a base model previously optimized for a source task. • Choose a subsequence of convolutional layers from the start (conv. base). • Freeze weights of previous layers to avoid destroying learned knowledge. • Add new layers (new subnet) connected to the output of the conv. base. • Train new layers to adapt old features in useful predictions for a target task. • Optionally perform a fine tuning of the entire model: o unfreeze convolutional base layers (usually all) o retrain the entire model with data from the new target task with a very small 𝑙𝑟 o …for a small number of epochs: danger of overfitting! # instantiate a base model with pre-trained weights base_model = keras.applications.Xception( weights='imagenet', # load weights pre-trained on ImageNet. input_shape=(150, 150, 3), include_top=False) # do not include the ImageNet classifier at the top. # freeze the base model base_model.trainable = False # create a new model on top inputs = keras.Input(shape=(150, 150, 3)) # make sure that base_model is running in inference mode by passing `training=False` x = base_model(inputs, training=False) # convert features to vectors x = keras.layers.GlobalAveragePooling2D()(x) # a Dense classifier with a single unit (binary classification) outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs) # train the model on new data model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()]) model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...) Transfer learning & fine-tuning, fcollet, 2020. pre-trained CNN features typical workflow in Keras # optional fine-tuning of the whole base model # unfreeze the base model base_model.trainable = True # it's important to recompile your model after you make any changes to the `trainable` attribute of any inner layer, so that your changes are take into account model.compile(optimizer=keras.optimizers.Adam(1e-5), # very low learning rate loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()]) # train end-to-end; be careful to stop before you overfit! model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...) pre-trained CNN features typical workflow in Keras Transfer learning & fine-tuning, fcollet, 2020. pretrained models for computer vision tasks • VGG16 (+) • VGG19 (+) • Inception V3 (+) • Xception (+) • ResNet-50 (+) • EfficientNet (+) Dipanjan (DJ) Sarkar, A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning, Medium, 2018 (link). pretrained models for NLP tasks (sentiment analysis, name entity recognition, machine translation, text summarization, natural language generation, speech recognition, QA systems, etc.) • BERT by Google (+) • RoBERTa by Facebook (+) • GPT-3 by OpenAI (+) • Huggingface transformers (+) Pre-trained models can be easily loaded from libraries such as PyTorch, Tensorflow, etc. pre-trained CNN features examples of reusable models similarity between 𝒟𝑠 y 𝒟𝑡 few data + different domains => (try to) retrain a few convolutional layers (worst case scenario, overfitting risk) lots of data + different domains => (re)train the entire model lots of data + similar domains => retrain some or all convolutional layers (best case scenario) few data + similar domains => freeze convolutional layers + train only the new decision rule (transfer learned spaces) pre-trained CNN features strategies Long Ang, A comprehensive guide on how to fine-tune deep neural networks using Keras on Google Colab (Free GPU), https://towardsdatascience.com/a-comprehensive-guide-on-how-to-fine-tune-deep-neural-networks-using-keras-on-google-colab-free-daaaa0aced8f target task (𝒯𝑡): car segmentation on a highway objective: to create a CNN model capable of solving the segmentation task restriction: we only have 25 training samples (image pairs)! pre-trained CNN features case study source task (𝒯𝑠): image classification (ImageNet) ** Deng, Jia et al. “ImageNet: A large-scale hierarchical image database.” 2009 IEEE Conf. on Comp. Vision and Pattern Recognition (2009): 248-255. ** pre-trained CNN features case study source task (𝒯𝑠): image classification (ImageNet) training: • 1.200.000 labeled images • 1.000 class/labels (ground truth) • one label per image: identifies the main object validation and test: • 150.000 real world photos (obtained through search engines) • 50.000 for validation purposes • 100.000 for testing purposes • typical output: the 5 most likely categories/classes success/hit: the ground truth label is one of these 5 categories (top-5 error) pre-trained CNN features case study pre-trained CNN features case study VGG16 **: source task solution (𝒯𝑠, ImageNet) ** Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition“. ArXiv:1409.1556 (2014). pre-trained CNN features case study extraction of image features learned from ImageNet 26 16,4 11,7 7,3 6,7 3,6 3,1 0 5 10 15 20 25 30 2011 (XRCE) 2012 (AlexNet) 2013 (ZF) 2014 (VGG) 2014 (GoogLeNet) 2015 (ResNet) 2016 (GoogLeNet-v4) ImageNet classification top-5 error (%) ImageNet, winning models at ILSVRC (link) pre-trained CNN features case study architecture adaptation of VGG16 (convolutional base is transferred)conv 1.1conv 1.2poolingconv 2.1conv 2.2poolingconv 3.1conv 3.2conv 3.3poolingconv 4.1conv 4.2conv 4.3poolingconv 5.1conv 5.2conv 5.3poolingdensadensadensahighwayconv 1.1conv 1.2poolingconv 2.1conv 2.2poolingconv 3.1conv 3.2conv 3.3poolingconv 4.1conv 4.2conv 4.3poolingconv 5.1conv 5.2conv 5.3deconvdeconvdeconvdeconvdeconv original model adapted model 240x320x3 240x320x64 120x160x128 60x80x256 30x40x512 15x20x512 240x320x1 𝒯𝑠 𝒯𝑡 pre-trained CNN features case study source domain (𝒟𝑠) target domain (𝒟𝑡) self-driving car simulator source: TechCrunch Google self-driving car source: Google Research blog contextless objects source: Sun et al., 2016 objects in the wild source: Sun et al., 2016 NLP, news docs source: pxhere.com social media msg source: flickr.com ASR systems trained with standard accents ASR systems used by people with non-standard accents, speech difficulties, dyslexics, etc. Source: Sebastian Ruder (2017). Transfer Learning - Machine Learning's Next Frontier, post (link). NLP = Natural Language Processing ASR = Automatic Speech Recognition 𝒟𝑠 ≠ 𝒟𝑡, 𝒴𝑠 = 𝒴𝑡 domain adaptation introduction 𝒟𝑠 ≠ 𝒟𝑡, 𝒴𝑠 = 𝒴𝑡 domain adaptation introduction it is assumed… • classification task on 𝒳⨂𝒴 • there are two distributions 𝑃𝑠 𝑋𝑠 y 𝑃𝑡 𝑋𝑡 • 𝑃𝑠 𝑋𝑠 is shifted with respect to 𝑃𝑡 𝑋𝑡 en 𝒳 (domain shift) • objective: learn 𝑃𝑡 𝑦 𝑥 • we have: 𝑥𝑖, 𝑦𝑖 ∈ 𝑋𝑠⨂𝒴, 𝑥𝑗 ∈ 𝑋𝑡 source link domain adaptation domain shift domain adaptation neural network [Ganin et al. 2015] Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). training/learning phase o 𝑥𝑖, 𝑦𝑖 ∈ 𝑋𝑠⨂𝒴: labeled samples of the source domain o 𝑥𝑗 ∈ 𝑋𝑡: unlabeled samples of the target domain test/exploitation phase o goal: predict labels 𝑦𝑗 ∈ 𝒴 for 𝑥𝑗 ∈ 𝑋𝑡 domain adaptation neural network use of data in training and testing Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). source -> 0 target -> 1 y1 y2 ... yk domain adaptation neural network one feature space, two classification tasks Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). domain adaptation neural network loss function di Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). domain adaptation neural network optimization di domain adaptation neural network optimization (Xs,Ys), Xt <- training_data() model, model_domain, model_source <- models(𝜆) do_label = array([0]*batch_size + [1]*batch_size) do_label_adv = array([1]*batch_size + [0]*batch_size) for each update: Xs_b, Ys_b <- next(batch_generator(Xs, Ys)) Xt_b <- next(batch_generator(Xt)) save(𝜃𝑓) model_domain.train([Xs_b, Xt_b], do_label) restore(𝜃𝑓) save(𝜃𝑑) model.train([Xs_b, Xt_b], [Ys_b, zeros_like(Ys_b)], do_label_adv) restore(𝜃𝑑) domain adaptation neural network [Ganin et al. 2015] Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). transfer learning expected benefits","libVersion":"0.3.2","langs":""}