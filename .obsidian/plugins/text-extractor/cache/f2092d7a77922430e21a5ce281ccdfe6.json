{"path":"_aula_virtual/SJK003/15-transfer-learning.pdf","text":"RamÃ³n A. Mollineda CÃ¡rdenas deep transfer learning MACHINE LEARNING University Master's Degree in Intelligent Systems a quote After supervised learning, transfer learning will be the next driver of ML commercial success. Andrew Ng data scientist driver = clave, motor, fuerza impulsoraâ€¦ a sentence in german please, refrain german speakers! Mein Gott ist gut any hypotheses about its meaning? natural knowledge transfer humans naturally take advantage of previously acquired knowledge and skills, to learn how to solve new related tasks. if you knowâ€¦ itâ€™ll be easier for you to learn... relationship I haveâ€¦ It is longâ€¦ Where is my book? My name is Juan. Ich habeâ€¦ Es ist langâ€¦ Wo ist mein buch? Mein name ist Juan. english and german are germanic languages trumpet and French horn are brass instruments transfer learning takes advantage of the knowledge acquired in the solution of a task to solve other similar tasks; this way you avoid learning from scratch. natural knowledge transfer english & german source: https://www.europelanguagejobs.com/blog/german-english-cognates context traditional machine learning learn from scratch: starts with arbitrary parameter values and uses data from a target task to adjust them large data set from task A untrained model small data set from task B (B is related to A) optimized model for A untrained model model of B does NOT take advantage of learning from A optimized model for B context tradicional machine learning deep learning inherits this practiceâ€¦ promise â€“ deep networks are universal approximators, capable of solving any problem fine print â€“ they depend on millions of parameters to optimize! Myth â€“ therefore, you will not be able to create a DL solution unless you have sufficient amounts of data from the target task context deep learning X context deep learning deep learning inherits this practiceâ€¦ promise â€“ deep networks are universal approximators, capable of solving any problem fine print â€“ they depend on millions of parameters to optimize! Myth â€“ therefore, you will not be able to create a DL solution unless you have sufficient amounts of data from the target task context deep learning deep learning inherits this practiceâ€¦ promise â€“ deep networks are universal approximators, capable of solving any problem fine print â€“ they depend on millions of parameters to optimize! truth â€“ it's possible toâ€¦ â€¢ transfer learned representations to related tasks â€¢ learn good representations from unlabeled data â€¢ learn representations common to different domains traditional scenario learn from scratch with insufficient data => overfitting (cheap solution, but generally useless) context deep learning UNICORN scenario learn from scratch with unlimited resourcesâ€¦ â€¢ unlimited labeled data â€¢ unlimited computing power â€¢ unlimited time (optimal solution at a prohibitive cost) fuente: flickr context deep learning motivation aim good generalization at a low cost with limited amounts of data (good and affordable solution) definition Transfer learning â€¦ refers to the situation where what has been learned in one setting â€¦ is exploited to improve generalization in another setting. I. Goodfellow, Y. Bengio & A. Courville. Deep Learning, 2016. transfer learning â€“ popular strategy pretrained model, optimized for A, but useful in related tasks adapted model, optimized for Baccurate, fast and cheap solution knowledge (features, weights) knowledge transfer large data set from task A small data set from task B (B is related to A) untrained model optimized model for A another quoteâ€¦ Deep Learning on Steroids with the Power of Knowledge Transfer! Dipanjan Sarkar author of the book â€œHands-On Transfer Learning with Pythonâ€ (link) formal definition domain and task Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345â€“1359 (link). domain ð’Ÿ = ð’³, ð‘ƒ ð‘‹ ð’³, feature space ð‘ƒ ð‘‹ , marginal probability distribution ð‘‹ = ð‘¥1, â€¦ , ð‘¥ð‘› , ð‘¥ð‘– âˆˆ ð’³ example: document classification (bag-of-words) â€¢ ð’³, space for all possible documents â€¢ ð‘‹, training document set, ð‘‹ âŠ‚ ð’³ â€¢ ð‘¥ð‘–, document representation ð‘– task ð’¯ = ð’´, ð‘ƒ ð‘¦ ð‘¥ ð’´, label space ð‘ƒ ð‘¦ ð‘¥ , conditional probability distribution (prediction) ð‘ƒ ð‘¦ ð‘¥ learns from ð‘¥ð‘–, ð‘¦ð‘– , ð‘¥ð‘– âˆˆ ð‘‹, ð‘¦ð‘– âˆˆ ð’´ example: document classification (bag-of-words) â€¢ in sentiment analysis, ð’´ = {Positive, Negative, Neutral} given â€¢ ð’Ÿð‘ , ð’¯ð‘  , source domain and task â€¢ ð’Ÿð‘¡, ð’¯ð‘¡ , target domain and task transfer learning aims atâ€¦ â€¢ learning ð‘ƒð‘¡ ð‘¦ ð‘¥ using knowledge from ð’Ÿð‘  and/or ð’¯ð‘  â€¢ cases: ð’Ÿð‘  â‰  ð’Ÿð‘¡ xor ð’¯ð‘  â‰  ð’¯ð‘¡ â€¢ generally, the number of labeled samples from ð’¯ð‘¡ is significantly less than the number of labeled samples from ð’¯ð‘  formal definition domain and task Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345â€“1359 (link). scenarios domain ð’Ÿ = ð’³, ð‘ƒ ð‘‹ ð’³ð‘  â‰  ð’³ð‘¡ e.g., documents written in different languages ð‘ƒ ð‘‹ð‘  â‰  ð‘ƒ ð‘‹ð‘¡ e.g., documents discuss different topics (politics, scienceâ€¦); it is known as domain adaptation. task ð’¯ = ð’´, ð‘ƒ ð‘¦ ð‘¥ ð’´ð‘  â‰  ð’´ð‘¡ e.g., documents are assigned to different classes/labels; it usually occurs together with the following scenario. ð‘ƒð‘  ð‘¦ ð‘¥ â‰  ð‘ƒð‘¡ ð‘¦ ð‘¥ e.g., different distributions of documents among classes formal definition scenarios Source: Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345â€“1359 (link). TL methods in deep learning â€¢ transfer of pretrained features, when ð’´ð‘  â‰  ð’´ð‘¡ o simple and popular method o it takes advantage of the convolutional base trained in a related task ð’¯ð‘  â€¢ domain adaptation, when ð‘ƒ ð‘‹ð‘  â‰  ð‘ƒ ð‘‹ð‘¡ o it learns common representation to ð’Ÿð‘  y ð’Ÿð‘¡ o it minimizes classification error on ð’¯ð‘  o it maximizes confusion between ð’Ÿð‘  y ð’Ÿð‘¡ (on common representation) pre-trained CNN features CNN architecture convolutional layers learning hierarchical representations (feature spaces) optimized for ð’¯ð‘  dense layers classification: it learns ð‘ƒð‘  ð‘¦ ð‘¥ prediction (according to ð’´ð‘ ) generic (image) features (task independent) specific (image) features (task dependent) feature extraction decision making input (e.g. image in ð’Ÿð‘ ) learning processes from ð’Ÿð‘ , ð’¯ð‘  => pre-trained models input (e.g. image in ð’Ÿð‘¡) convolutional layers optimized representations for ð’¯ð‘  prediction (according to ð’´ð‘¡) any classifier (e.g. SVM, KNN) learns ð‘ƒð‘¡ ð‘¦ ð‘¥ input (e.g. image in ð’Ÿð‘¡) convolutional layers optimized for ð’¯ð‘  (generic features) prediction (according to ð’´ð‘¡) un clasificador (e.g. SVM, KNN) aprende ð‘ƒð‘¡ ð‘¦ ð‘¥ added dense layers learns ð‘ƒð‘¡ ð‘¦ ð‘¥ conv. layers optimiz. for ð’¯ð‘¡ (specific feat.) input (e.g. image in ð’Ÿð‘¡) prediction (according to ð’´ð‘¡) un clasificador (e.g. SVM, KNN) aprende ð‘ƒð‘¡ ð‘¦ ð‘¥ added dense layers learns ð‘ƒð‘¡ ð‘¦ ð‘¥ convolutional layers optimized representations for ð’¯ð‘  exploitation on ð’Ÿð‘¡, ð’¯ð‘¡ / scenario: ð’Ÿð‘  â‰ˆ ð’Ÿð‘¡, ð’´ð‘  â‰  ð’´ð‘¡ ð‘™ð‘Ÿ = 0 ð‘™ð‘Ÿ = 0 ð‘™ð‘Ÿ = 0 ð‘™ð‘Ÿ > 0 ð‘™ð‘Ÿ > 0 ð‘™ð‘Ÿ > 0 ð‘™ð‘Ÿ - learning rate pre-trained CNN features strategies pre-trained CNN features typical workflow â€¢ There is a base model previously optimized for a source task. â€¢ Choose a subsequence of convolutional layers from the start (conv. base). â€¢ Freeze weights of previous layers to avoid destroying learned knowledge. â€¢ Add new layers (new subnet) connected to the output of the conv. base. â€¢ Train new layers to adapt old features in useful predictions for a target task. â€¢ Optionally perform a fine tuning of the entire model: o unfreeze convolutional base layers (usually all) o retrain the entire model with data from the new target task with a very small ð‘™ð‘Ÿ o â€¦for a small number of epochs: danger of overfitting! # instantiate a base model with pre-trained weights base_model = keras.applications.Xception( weights='imagenet', # load weights pre-trained on ImageNet. input_shape=(150, 150, 3), include_top=False) # do not include the ImageNet classifier at the top. # freeze the base model base_model.trainable = False # create a new model on top inputs = keras.Input(shape=(150, 150, 3)) # make sure that base_model is running in inference mode by passing `training=False` x = base_model(inputs, training=False) # convert features to vectors x = keras.layers.GlobalAveragePooling2D()(x) # a Dense classifier with a single unit (binary classification) outputs = keras.layers.Dense(1)(x) model = keras.Model(inputs, outputs) # train the model on new data model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()]) model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...) Transfer learning & fine-tuning, fcollet, 2020. pre-trained CNN features typical workflow in Keras # optional fine-tuning of the whole base model # unfreeze the base model base_model.trainable = True # it's important to recompile your model after you make any changes to the `trainable` attribute of any inner layer, so that your changes are take into account model.compile(optimizer=keras.optimizers.Adam(1e-5), # very low learning rate loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()]) # train end-to-end; be careful to stop before you overfit! model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...) pre-trained CNN features typical workflow in Keras Transfer learning & fine-tuning, fcollet, 2020. pretrained models for computer vision tasks â€¢ VGG16 (+) â€¢ VGG19 (+) â€¢ Inception V3 (+) â€¢ Xception (+) â€¢ ResNet-50 (+) â€¢ EfficientNet (+) Dipanjan (DJ) Sarkar, A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning, Medium, 2018 (link). pretrained models for NLP tasks (sentiment analysis, name entity recognition, machine translation, text summarization, natural language generation, speech recognition, QA systems, etc.) â€¢ BERT by Google (+) â€¢ RoBERTa by Facebook (+) â€¢ GPT-3 by OpenAI (+) â€¢ Huggingface transformers (+) Pre-trained models can be easily loaded from libraries such as PyTorch, Tensorflow, etc. pre-trained CNN features examples of reusable models similarity between ð’Ÿð‘  y ð’Ÿð‘¡ few data + different domains => (try to) retrain a few convolutional layers (worst case scenario, overfitting risk) lots of data + different domains => (re)train the entire model lots of data + similar domains => retrain some or all convolutional layers (best case scenario) few data + similar domains => freeze convolutional layers + train only the new decision rule (transfer learned spaces) pre-trained CNN features strategies Long Ang, A comprehensive guide on how to fine-tune deep neural networks using Keras on Google Colab (Free GPU), https://towardsdatascience.com/a-comprehensive-guide-on-how-to-fine-tune-deep-neural-networks-using-keras-on-google-colab-free-daaaa0aced8f target task (ð’¯ð‘¡): car segmentation on a highway objective: to create a CNN model capable of solving the segmentation task restriction: we only have 25 training samples (image pairs)! pre-trained CNN features case study source task (ð’¯ð‘ ): image classification (ImageNet) ** Deng, Jia et al. â€œImageNet: A large-scale hierarchical image database.â€ 2009 IEEE Conf. on Comp. Vision and Pattern Recognition (2009): 248-255. ** pre-trained CNN features case study source task (ð’¯ð‘ ): image classification (ImageNet) training: â€¢ 1.200.000 labeled images â€¢ 1.000 class/labels (ground truth) â€¢ one label per image: identifies the main object validation and test: â€¢ 150.000 real world photos (obtained through search engines) â€¢ 50.000 for validation purposes â€¢ 100.000 for testing purposes â€¢ typical output: the 5 most likely categories/classes success/hit: the ground truth label is one of these 5 categories (top-5 error) pre-trained CNN features case study pre-trained CNN features case study VGG16 **: source task solution (ð’¯ð‘ , ImageNet) ** Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognitionâ€œ. ArXiv:1409.1556 (2014). pre-trained CNN features case study extraction of image features learned from ImageNet 26 16,4 11,7 7,3 6,7 3,6 3,1 0 5 10 15 20 25 30 2011 (XRCE) 2012 (AlexNet) 2013 (ZF) 2014 (VGG) 2014 (GoogLeNet) 2015 (ResNet) 2016 (GoogLeNet-v4) ImageNet classification top-5 error (%) ImageNet, winning models at ILSVRC (link) pre-trained CNN features case study architecture adaptation of VGG16 (convolutional base is transferred)conv 1.1conv 1.2poolingconv 2.1conv 2.2poolingconv 3.1conv 3.2conv 3.3poolingconv 4.1conv 4.2conv 4.3poolingconv 5.1conv 5.2conv 5.3poolingdensadensadensahighwayconv 1.1conv 1.2poolingconv 2.1conv 2.2poolingconv 3.1conv 3.2conv 3.3poolingconv 4.1conv 4.2conv 4.3poolingconv 5.1conv 5.2conv 5.3deconvdeconvdeconvdeconvdeconv original model adapted model 240x320x3 240x320x64 120x160x128 60x80x256 30x40x512 15x20x512 240x320x1 ð’¯ð‘  ð’¯ð‘¡ pre-trained CNN features case study source domain (ð’Ÿð‘ ) target domain (ð’Ÿð‘¡) self-driving car simulator source: TechCrunch Google self-driving car source: Google Research blog contextless objects source: Sun et al., 2016 objects in the wild source: Sun et al., 2016 NLP, news docs source: pxhere.com social media msg source: flickr.com ASR systems trained with standard accents ASR systems used by people with non-standard accents, speech difficulties, dyslexics, etc. Source: Sebastian Ruder (2017). Transfer Learning - Machine Learning's Next Frontier, post (link). NLP = Natural Language Processing ASR = Automatic Speech Recognition ð’Ÿð‘  â‰  ð’Ÿð‘¡, ð’´ð‘  = ð’´ð‘¡ domain adaptation introduction ð’Ÿð‘  â‰  ð’Ÿð‘¡, ð’´ð‘  = ð’´ð‘¡ domain adaptation introduction it is assumedâ€¦ â€¢ classification task on ð’³â¨‚ð’´ â€¢ there are two distributions ð‘ƒð‘  ð‘‹ð‘  y ð‘ƒð‘¡ ð‘‹ð‘¡ â€¢ ð‘ƒð‘  ð‘‹ð‘  is shifted with respect to ð‘ƒð‘¡ ð‘‹ð‘¡ en ð’³ (domain shift) â€¢ objective: learn ð‘ƒð‘¡ ð‘¦ ð‘¥ â€¢ we have: ð‘¥ð‘–, ð‘¦ð‘– âˆˆ ð‘‹ð‘ â¨‚ð’´, ð‘¥ð‘— âˆˆ ð‘‹ð‘¡ source link domain adaptation domain shift domain adaptation neural network [Ganin et al. 2015] Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). training/learning phase o ð‘¥ð‘–, ð‘¦ð‘– âˆˆ ð‘‹ð‘ â¨‚ð’´: labeled samples of the source domain o ð‘¥ð‘— âˆˆ ð‘‹ð‘¡: unlabeled samples of the target domain test/exploitation phase o goal: predict labels ð‘¦ð‘— âˆˆ ð’´ for ð‘¥ð‘— âˆˆ ð‘‹ð‘¡ domain adaptation neural network use of data in training and testing Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). source -> 0 target -> 1 y1 y2 ... yk domain adaptation neural network one feature space, two classification tasks Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). domain adaptation neural network loss function di Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). domain adaptation neural network optimization di domain adaptation neural network optimization (Xs,Ys), Xt <- training_data() model, model_domain, model_source <- models(ðœ†) do_label = array([0]*batch_size + [1]*batch_size) do_label_adv = array([1]*batch_size + [0]*batch_size) for each update: Xs_b, Ys_b <- next(batch_generator(Xs, Ys)) Xt_b <- next(batch_generator(Xt)) save(ðœƒð‘“) model_domain.train([Xs_b, Xt_b], do_label) restore(ðœƒð‘“) save(ðœƒð‘‘) model.train([Xs_b, Xt_b], [Ys_b, zeros_like(Ys_b)], do_label_adv) restore(ðœƒð‘‘) domain adaptation neural network [Ganin et al. 2015] Ganin, Y., & Lempitsky, V. (2015). Unsupervised Domain Adaptation by Backpropagation. In Proceedings of the 32nd International Conference on Machine Learning. (Vol. 37). transfer learning expected benefits","libVersion":"0.3.2","langs":""}