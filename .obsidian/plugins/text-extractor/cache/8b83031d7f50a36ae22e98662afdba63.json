{"path":"_aula_virtual/SJK003/06-SVM.pdf","text":"Department of Computer Languages and Systems Support Vector Machine Introduction A support vector machine (SVM) tries to find a hyperplane that best separates the two classes â€¦ but there are many (infinite) separating hyperplanes, which one does it select? Introduction We will try to select the hyperplane that maximizes the distance (margin) to the closest samples of each class The optimal separation hyperplane is equidistant from the closest example of each class Types of SVM algorithms Linear SVM Non-linear SVM When the data is perfectly linearly separable â†’ hard margin When the data is not linearly separable, we have to use some kernel â†’ soft margin What is a margin? it is the distance between the hyperplane and the samples closest to the hyperplane (support vectors). the optimal hyperplane will be the one with the largest margin! maximum margin = minimum loss How to find the biggest margin? Given a data set ğ’Ÿ with ğ‘› ğ‘‘-dimensional vectors ğ±ğ‘–linearly separable, each associated with a value ğ‘¦ğ‘– to indicate if ğ±ğ‘– belongs to class -1 or to class +1 ğ’Ÿ = ğ±ğ‘–, ğ‘¦ğ‘– à¸« ğ±ğ‘–âˆˆ â„ ğ‘‘, ğ‘¦ğ‘– âˆˆ âˆ’1, +1 ğ‘–=1 ğ‘› Steps: 1. Select two hyperplanes that separate the data with no points between them 2. Maximize their distance (the margin) The region bounded by the two hyperplanes will be the largest possible margin! Step 1: Select two hyperplanes â€¦ Given a hyperplane ğ»0 separating the data set and satisfying: ğ»0: ğ°ğ‘‡ğ± + ğ‘ = 0 â€“ we can select two others hyperplanes ğ»1 and ğ»2 that also separate the data and have the following equations: ğ»1: ğ°ğ‘‡ğ± + ğ‘ = ğ›¿ and ğ»2: ğ°ğ‘‡ğ± + ğ‘ = âˆ’ğ›¿ â€“ to simplify the problem, we can set ğ›¿ = 1: ğ»1: ğ°ğ‘‡ğ± + ğ‘ = 1 and ğ»2: ğ°ğ‘‡ğ± + ğ‘ = âˆ’1 where ğ° is the weight vector (to define the orientation of the hyperplane) and ğ‘ is the bias Step 1 (cont.) â€¦ but not any hyperplane is valid, we will only select those that meet the following two constraints for all ğ±ğ‘– âˆˆ ğ’Ÿ: ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¥ 1 for ğ‘¦ğ‘– = 1 or ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¤ 1 for ğ‘¦ğ‘– = âˆ’1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = âˆ’1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = âˆ’1 Step 1 (cont.) Understanding the constraints: we need to verify the points do not violate the constraints ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¥ 1 for ğ‘¦ğ‘– = 1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¤ 1 for ğ‘¦ğ‘– = âˆ’1 â€¢ when ğ±ğ‘– = ğ´, we see that the point is on the hyperplane, so ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 1 and the constraint is respected â€¢ when ğ±ğ‘– = ğ¶, we see that the point is above the hyperplane, so ğ°ğ‘‡ğ±ğ‘– + ğ‘ > 1 and the constraint is respected ğ°ğ‘‡ğ±ğ‘– + ğ‘ = âˆ’1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 1 Step 1 (cont.) Understanding the constraints: we need to verify the points do not violate the constraints ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¥ 1 for ğ‘¦ğ‘– = 1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ â‰¤ 1 for ğ‘¦ğ‘– = âˆ’1 when a constraint is not satisfied (i.e. there are points between the two hyperplanes) means that we cannot select these two hyperplanes ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 1 ğ°ğ‘‡ğ±ğ‘– + ğ‘ = âˆ’1 Step 1 (cont.) Previous equations can be combined into a single constraint: ğ‘¦ğ‘–(ğ°ğ‘‡ğ±ğ‘– + ğ‘) â‰¥ 1 âˆ€ğ‘– = 1, â€¦ , ğ‘› Step 2: Maximize the margin Recall that the shortest distance between a point ğ±ğ‘– and a hyperplane ğ°ğ‘‡ğ±ğ‘– + ğ‘ = 0 can be calculated as: ğ‘‘ğ‘– = ğ°ğ‘‡ğ±ğ‘– + ğ‘ ğ° where ğ° represents the Euclidean norm of the vector ğ°, which also has the property of being perpendicular to the hyperplane. Step 2 (cont.) The distance of the closest points (i.e., support vectors) to the optimal hyperplane will be Â±ğŸ ğ° = ğŸ ğ° and therefore the margin (the closest distance between the two classes) will be: ğœŒ = ğŸ ğ° Step 2 (cont.) We want to maximize the margin ğœŒ: max ğ°,ğ‘ ğŸ ğ° â‰¡ min ğ°,ğ‘ ğ° For computational reasons, this term is usually expressed as min ğ°,ğ‘ 1 2 ğ° 2 subject to ğ‘¦ğ‘–(ğ°ğ‘‡ğ±ğ‘– + ğ‘) â‰¥ 1 âˆ€ğ‘– = 1, â€¦ , ğ‘› An example (i)An example (ii) âˆ’0.5ğ‘¥ + 0.5ğ‘¦ + 0.5 = âˆ’1 âˆ’0.5 âˆ™ 6 + 0.5 âˆ™ 3 + 0.5 = âˆ’1 âˆ’0.5ğ‘¥ + 0.5ğ‘¦ + 0.5 = 1 âˆ’0.5 âˆ™ 3 + 0.5 âˆ™ 4 + 0.5 = 1 An example (iii) ğ°ğ‘‡ğ± + ğ‘ = 0 âˆ’0.5 0.5 âˆ™ ğ‘¥ ğ‘¦ + ğ‘ = 0 An example (iv)An example (v) Prediction for a new sample An example (vi) Prediction for a new sample An example (vii)An example (viii) Margin An example (ix)An example (x)An example (xi)An example (xii) âˆ’0.5ğ‘¥ + 0.5ğ‘¦ + ğ‘ = 0 â†’ âˆ’0.5 âˆ™ 3 + 0.5 âˆ™ 4 + 0.5 = 1 âˆ’0.5ğ‘¥ + 0.5ğ‘¦ + ğ‘ = 0 â†’ âˆ’0.5 âˆ™ 1 + 0.5 âˆ™ 4 + 0.5 = 2 An example (xiii)SVM with outliers General situation in real-life applications: â€“ data is almost linearly separable or non-linearly separable â€“ there are outliers in the data How to tackle the outliers problem? to modify the optimization function in such a way that it allows few misclassifications (some points are inside or on the wrong side of the margin) by introducing two hyperparameters: a regularization factor C and a slack variable ğœ‰ğ‘– for each data point ğ±ğ‘– â†’ SOFT MARGIN SVM Hard margin vs soft margin â€¢ In hard margin SVM, the points in a class must not lie within the two support hyperplanes: ğ‘¦ğ‘–(ğ°ğ‘‡ğ±ğ‘– + ğ‘) â‰¥ 1 âˆ€ğ‘– = 1, â€¦ , ğ‘› Slack variables A slack variable ğœ‰ğ‘– â‰¥ 0 (ğ‘– = 1, â€¦ , ğ‘›) is the distance of ğ±ğ‘– from its classâ€™s margin if ğ±ğ‘– is on the wrong side of the margin; otherwise zero (this corresponds to linearly separable samples) thus the points that are far away from the margin on the wrong side would get more penalty Slack variables (cont.) â€¢ A slack variable ğœ‰ğ‘– = 0 corresponds to a linearly separable sample â€¢ A slack variable 0 < ğœ‰ğ‘– < 1 corresponds to a non-separable sample â€¢ A slack variable ğœ‰ğ‘– > 1 corresponds to a non-separable and misclassified sample The sum of all slack variables Ïƒğ‘–=1 ğ‘› ğœ‰ğ‘– allows, in some way, measuring the cost associated with the number of non-separable examples Regularization factor It is a constant, chosen by the user, which allows controlling the cost of non-separable samples in minimization, that is, it tries to set a balance between maximizing the margin and minimizing the misclassifications. The value of C can be optimized by using some resampling method (e.g., cross-validation). Regularization factor (cont.) â€¢ When C is small, the focus is more on obtaining a wider margin at the expense of allowing more misclassifications: â€“ it would allow very large values of ğœ‰ğ‘–: in the limit (ğ¶ â†’0), all samples would be allowed to be misclassified (ğœ‰ğ‘– â†’ âˆ) â€¢ When C is large, the focus is more on avoiding errors at the expense of resulting in a narrower margin: â€“ it would allow very small values of ğœ‰ğ‘–: in the limit (ğ¶ â†’ âˆ), we would considering the case of perfectly separable samples (ğœ‰ğ‘– â†’ 0) Regularization factor (cont.) Small C gives a large minimum margin, but a sample will be misclassified Large C gives a much smaller margin, but all samples will be classified correctly Soft margin SVM: formulation Now each data point needs to satisfy the following constraint: ğ‘¦ğ‘–(ğ°ğ‘‡ğ±ğ‘– + ğ‘) â‰¥ 1 âˆ’ ğœ‰ğ‘– for ğ‘– = 1, â€¦ , ğ‘› and the objective function to minimize will be: min ğ°,ğ‘ 1 2 ğ° 2 + ğ¶ à· ğ‘–=1 ğ‘› ğœ‰ğ‘– subject to ğ‘¦ğ‘–(ğ°ğ‘‡ğ±ğ‘– + ğ‘) + ğœ‰ğ‘– âˆ’ 1 â‰¥ 0 ğœ‰ğ‘– â‰¥ 0, âˆ€ğ‘– = 1, â€¦ , ğ‘› An example (xiv) This term is to allow misclassifications >1 An example (xv) <10< An example (xvi)Non-linear SVM: the kernel trick Solution to non-linear problems: 1. Map the data onto another feature space to convert the non-linear data to linear data: ğ±ğ‘– â†’ ğœ™(ğ±ğ‘–) 2. Solve a linear SVM in the new feature space Therefore, the kernel trick is to define a kernel function ğ‘˜ ğ±ğ‘–, ğ±ğ‘— Non-linear SVM: the kernel trick (ii) â€¢ Polynomial kernel of degree d: ğ¾ ğ±ğ‘–, ğ±ğ‘— = (ğ±ğ‘– ğ‘‡ â‹… ğ±ğ‘— + 1) ğ‘‘ â€¢ RBF kernel: ğ¾ ğ±ğ‘–, ğ±ğ‘— = ğ‘’âˆ’ğ›¾ ğ±ğ‘–âˆ’ğ±ğ‘— 2 where ğ›¾ > 0, and ğ±ğ‘– âˆ’ ğ±ğ‘— . Non-linear SVM: the kernel trick (iii) â€¢ A special case of RBF kernel is ğ›¾ = Î¤1 2ğœ2 â†’ Gaussian kernel: ğ¾ ğ±ğ‘–, ğ±ğ‘— = ğ‘’ âˆ’ ğ±ğ‘–âˆ’ğ±ğ‘— 2 2ğœ2 where ï³ is the variance. Non-linear SVM: the kernel trick (iv) How to choose the right kernel? â€“ if the data set is linearly separable, then you must opt for a linear function because it is very easy to use and the complexity is much lower compared to other kernel functions â€“ so, start with a hypothesis that data is linearly separable and choose a linear function. Then use an RBF kernel function (polynomial kernel is rarely used due to poor efficiency) â€“ if linear and RBF both give approximately similar results, choose the linear SVM. Otherwise, choose the RBF kernel Non-linear SVM: the kernel trick (v) An example with similar results for both linear and RBF functions Non-linear SVM: the kernel trick (vi) An example where a linear SVM gives poor results, while the RBF kernel makes a correct decision boundary","libVersion":"0.3.2","langs":""}