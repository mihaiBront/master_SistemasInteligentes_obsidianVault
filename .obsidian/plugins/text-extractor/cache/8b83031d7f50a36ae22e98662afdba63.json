{"path":"_aula_virtual/SJK003/06-SVM.pdf","text":"Department of Computer Languages and Systems Support Vector Machine Introduction A support vector machine (SVM) tries to find a hyperplane that best separates the two classes … but there are many (infinite) separating hyperplanes, which one does it select? Introduction We will try to select the hyperplane that maximizes the distance (margin) to the closest samples of each class The optimal separation hyperplane is equidistant from the closest example of each class Types of SVM algorithms Linear SVM Non-linear SVM When the data is perfectly linearly separable → hard margin When the data is not linearly separable, we have to use some kernel → soft margin What is a margin? it is the distance between the hyperplane and the samples closest to the hyperplane (support vectors). the optimal hyperplane will be the one with the largest margin! maximum margin = minimum loss How to find the biggest margin? Given a data set 𝒟 with 𝑛 𝑑-dimensional vectors 𝐱𝑖linearly separable, each associated with a value 𝑦𝑖 to indicate if 𝐱𝑖 belongs to class -1 or to class +1 𝒟 = 𝐱𝑖, 𝑦𝑖 ห 𝐱𝑖∈ ℝ 𝑑, 𝑦𝑖 ∈ −1, +1 𝑖=1 𝑛 Steps: 1. Select two hyperplanes that separate the data with no points between them 2. Maximize their distance (the margin) The region bounded by the two hyperplanes will be the largest possible margin! Step 1: Select two hyperplanes … Given a hyperplane 𝐻0 separating the data set and satisfying: 𝐻0: 𝐰𝑇𝐱 + 𝑏 = 0 – we can select two others hyperplanes 𝐻1 and 𝐻2 that also separate the data and have the following equations: 𝐻1: 𝐰𝑇𝐱 + 𝑏 = 𝛿 and 𝐻2: 𝐰𝑇𝐱 + 𝑏 = −𝛿 – to simplify the problem, we can set 𝛿 = 1: 𝐻1: 𝐰𝑇𝐱 + 𝑏 = 1 and 𝐻2: 𝐰𝑇𝐱 + 𝑏 = −1 where 𝐰 is the weight vector (to define the orientation of the hyperplane) and 𝑏 is the bias Step 1 (cont.) … but not any hyperplane is valid, we will only select those that meet the following two constraints for all 𝐱𝑖 ∈ 𝒟: 𝐰𝑇𝐱𝑖 + 𝑏 ≥ 1 for 𝑦𝑖 = 1 or 𝐰𝑇𝐱𝑖 + 𝑏 ≤ 1 for 𝑦𝑖 = −1 𝐰𝑇𝐱𝑖 + 𝑏 = −1 𝐰𝑇𝐱𝑖 + 𝑏 = 1 𝐰𝑇𝐱𝑖 + 𝑏 = 1 𝐰𝑇𝐱𝑖 + 𝑏 = −1 Step 1 (cont.) Understanding the constraints: we need to verify the points do not violate the constraints 𝐰𝑇𝐱𝑖 + 𝑏 ≥ 1 for 𝑦𝑖 = 1 𝐰𝑇𝐱𝑖 + 𝑏 ≤ 1 for 𝑦𝑖 = −1 • when 𝐱𝑖 = 𝐴, we see that the point is on the hyperplane, so 𝐰𝑇𝐱𝑖 + 𝑏 = 1 and the constraint is respected • when 𝐱𝑖 = 𝐶, we see that the point is above the hyperplane, so 𝐰𝑇𝐱𝑖 + 𝑏 > 1 and the constraint is respected 𝐰𝑇𝐱𝑖 + 𝑏 = −1 𝐰𝑇𝐱𝑖 + 𝑏 = 1 Step 1 (cont.) Understanding the constraints: we need to verify the points do not violate the constraints 𝐰𝑇𝐱𝑖 + 𝑏 ≥ 1 for 𝑦𝑖 = 1 𝐰𝑇𝐱𝑖 + 𝑏 ≤ 1 for 𝑦𝑖 = −1 when a constraint is not satisfied (i.e. there are points between the two hyperplanes) means that we cannot select these two hyperplanes 𝐰𝑇𝐱𝑖 + 𝑏 = 1 𝐰𝑇𝐱𝑖 + 𝑏 = −1 Step 1 (cont.) Previous equations can be combined into a single constraint: 𝑦𝑖(𝐰𝑇𝐱𝑖 + 𝑏) ≥ 1 ∀𝑖 = 1, … , 𝑛 Step 2: Maximize the margin Recall that the shortest distance between a point 𝐱𝑖 and a hyperplane 𝐰𝑇𝐱𝑖 + 𝑏 = 0 can be calculated as: 𝑑𝑖 = 𝐰𝑇𝐱𝑖 + 𝑏 𝐰 where 𝐰 represents the Euclidean norm of the vector 𝐰, which also has the property of being perpendicular to the hyperplane. Step 2 (cont.) The distance of the closest points (i.e., support vectors) to the optimal hyperplane will be ±𝟏 𝐰 = 𝟏 𝐰 and therefore the margin (the closest distance between the two classes) will be: 𝜌 = 𝟐 𝐰 Step 2 (cont.) We want to maximize the margin 𝜌: max 𝐰,𝑏 𝟐 𝐰 ≡ min 𝐰,𝑏 𝐰 For computational reasons, this term is usually expressed as min 𝐰,𝑏 1 2 𝐰 2 subject to 𝑦𝑖(𝐰𝑇𝐱𝑖 + 𝑏) ≥ 1 ∀𝑖 = 1, … , 𝑛 An example (i)An example (ii) −0.5𝑥 + 0.5𝑦 + 0.5 = −1 −0.5 ∙ 6 + 0.5 ∙ 3 + 0.5 = −1 −0.5𝑥 + 0.5𝑦 + 0.5 = 1 −0.5 ∙ 3 + 0.5 ∙ 4 + 0.5 = 1 An example (iii) 𝐰𝑇𝐱 + 𝑏 = 0 −0.5 0.5 ∙ 𝑥 𝑦 + 𝑏 = 0 An example (iv)An example (v) Prediction for a new sample An example (vi) Prediction for a new sample An example (vii)An example (viii) Margin An example (ix)An example (x)An example (xi)An example (xii) −0.5𝑥 + 0.5𝑦 + 𝑏 = 0 → −0.5 ∙ 3 + 0.5 ∙ 4 + 0.5 = 1 −0.5𝑥 + 0.5𝑦 + 𝑏 = 0 → −0.5 ∙ 1 + 0.5 ∙ 4 + 0.5 = 2 An example (xiii)SVM with outliers General situation in real-life applications: – data is almost linearly separable or non-linearly separable – there are outliers in the data How to tackle the outliers problem? to modify the optimization function in such a way that it allows few misclassifications (some points are inside or on the wrong side of the margin) by introducing two hyperparameters: a regularization factor C and a slack variable 𝜉𝑖 for each data point 𝐱𝑖 → SOFT MARGIN SVM Hard margin vs soft margin • In hard margin SVM, the points in a class must not lie within the two support hyperplanes: 𝑦𝑖(𝐰𝑇𝐱𝑖 + 𝑏) ≥ 1 ∀𝑖 = 1, … , 𝑛 Slack variables A slack variable 𝜉𝑖 ≥ 0 (𝑖 = 1, … , 𝑛) is the distance of 𝐱𝑖 from its class’s margin if 𝐱𝑖 is on the wrong side of the margin; otherwise zero (this corresponds to linearly separable samples) thus the points that are far away from the margin on the wrong side would get more penalty Slack variables (cont.) • A slack variable 𝜉𝑖 = 0 corresponds to a linearly separable sample • A slack variable 0 < 𝜉𝑖 < 1 corresponds to a non-separable sample • A slack variable 𝜉𝑖 > 1 corresponds to a non-separable and misclassified sample The sum of all slack variables σ𝑖=1 𝑛 𝜉𝑖 allows, in some way, measuring the cost associated with the number of non-separable examples Regularization factor It is a constant, chosen by the user, which allows controlling the cost of non-separable samples in minimization, that is, it tries to set a balance between maximizing the margin and minimizing the misclassifications. The value of C can be optimized by using some resampling method (e.g., cross-validation). Regularization factor (cont.) • When C is small, the focus is more on obtaining a wider margin at the expense of allowing more misclassifications: – it would allow very large values of 𝜉𝑖: in the limit (𝐶 →0), all samples would be allowed to be misclassified (𝜉𝑖 → ∞) • When C is large, the focus is more on avoiding errors at the expense of resulting in a narrower margin: – it would allow very small values of 𝜉𝑖: in the limit (𝐶 → ∞), we would considering the case of perfectly separable samples (𝜉𝑖 → 0) Regularization factor (cont.) Small C gives a large minimum margin, but a sample will be misclassified Large C gives a much smaller margin, but all samples will be classified correctly Soft margin SVM: formulation Now each data point needs to satisfy the following constraint: 𝑦𝑖(𝐰𝑇𝐱𝑖 + 𝑏) ≥ 1 − 𝜉𝑖 for 𝑖 = 1, … , 𝑛 and the objective function to minimize will be: min 𝐰,𝑏 1 2 𝐰 2 + 𝐶 ෍ 𝑖=1 𝑛 𝜉𝑖 subject to 𝑦𝑖(𝐰𝑇𝐱𝑖 + 𝑏) + 𝜉𝑖 − 1 ≥ 0 𝜉𝑖 ≥ 0, ∀𝑖 = 1, … , 𝑛 An example (xiv) This term is to allow misclassifications >1 An example (xv) <10< An example (xvi)Non-linear SVM: the kernel trick Solution to non-linear problems: 1. Map the data onto another feature space to convert the non-linear data to linear data: 𝐱𝑖 → 𝜙(𝐱𝑖) 2. Solve a linear SVM in the new feature space Therefore, the kernel trick is to define a kernel function 𝑘 𝐱𝑖, 𝐱𝑗 Non-linear SVM: the kernel trick (ii) • Polynomial kernel of degree d: 𝐾 𝐱𝑖, 𝐱𝑗 = (𝐱𝑖 𝑇 ⋅ 𝐱𝑗 + 1) 𝑑 • RBF kernel: 𝐾 𝐱𝑖, 𝐱𝑗 = 𝑒−𝛾 𝐱𝑖−𝐱𝑗 2 where 𝛾 > 0, and 𝐱𝑖 − 𝐱𝑗 . Non-linear SVM: the kernel trick (iii) • A special case of RBF kernel is 𝛾 = Τ1 2𝜎2 → Gaussian kernel: 𝐾 𝐱𝑖, 𝐱𝑗 = 𝑒 − 𝐱𝑖−𝐱𝑗 2 2𝜎2 where  is the variance. Non-linear SVM: the kernel trick (iv) How to choose the right kernel? – if the data set is linearly separable, then you must opt for a linear function because it is very easy to use and the complexity is much lower compared to other kernel functions – so, start with a hypothesis that data is linearly separable and choose a linear function. Then use an RBF kernel function (polynomial kernel is rarely used due to poor efficiency) – if linear and RBF both give approximately similar results, choose the linear SVM. Otherwise, choose the RBF kernel Non-linear SVM: the kernel trick (v) An example with similar results for both linear and RBF functions Non-linear SVM: the kernel trick (vi) An example where a linear SVM gives poor results, while the RBF kernel makes a correct decision boundary","libVersion":"0.3.2","langs":""}