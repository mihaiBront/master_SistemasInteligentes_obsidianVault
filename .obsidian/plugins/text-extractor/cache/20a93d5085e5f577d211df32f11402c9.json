{"path":"_aula_virtual/SJK003/04-knn.pdf","text":"Department of Computer Languages and Systems Distance-based Classifiers Introduction: general context Non-parametric, supervised models can be applied if there exist ‚Ä¶ ‚Ä¢ objects capable of being described by some formal representation (a vector, a string, ...) ‚Ä¢ classes or categories that group the objects ‚Ä¢ need to automate the classification (or identification) of new objects into some known class Introduction: an example of classification ‚Ä¢ objects: satellite images ‚Ä¢ formal representation: vector of features or attributes (image pixels, image features, etc.) ‚Ä¢ classes: oil-spills, look-alikes (ships, islands, etc.) A real problem: identification of oil-spills in the ocean Distance-based classifiers Motivation: ‚Ä¢ the best model is not always the most complicated ‚Ä¢ these classifiers are based on the idea that objects of the same class are the most similar ‚Ä¢ they use some similarity (distance) measures defined on attribute vectors Minimum distance classifier This is by far the simplest classification model ‚Ä¢ it does not take all the training data into account ‚Ä¢ it measures the similarity between test data and some representative prototype of each class ‚Ä¢ a common prototype is the centroid of each class ‚Ä¢ some distance is calculated for each prototype and the shortest distance is determined ‚Ä¢ a test object is assigned to the class of the nearest centroid (the minimum distance) Minimum distance classifier (ii) minimum_distance(Ttra, x) min ÔÇ¨ MAX for each class ÔÅ∑i zi ÔÇ¨ compute_centroid(ÔÅ∑i) for each prototype zi dist ÔÇ¨ compute_distance(x, zi) if dist < min min ÔÇ¨ dist class(x) ÔÇ¨ ÔÅ∑i Minimum distance classifier (iii) Drawback: ‚Ä¢ Loss of useful information k Nearest neighbours (k-NN) ‚Ä¢ one of the conceptually simplest yet powerful models ‚Ä¢ it takes all the training data into consideration ‚Ä¢ it searches for the k training data closest to a test point ‚Ä¢ a class label is assigned on the basis of a majority vote: the class that is most frequently represented among the k neighbours ‚Ä¢ it is said to be a lazy learner since it does not perform any training. Instead, it just stores the data k Nearest neighbours (k-NN) (ii) We need to define two hyperparameters ‚ñ™ the distance metric, to determine which training data are the closest to a given test point ‚ñ™ the k value, which defines how many neighbours will be checked to guess the classification of a test point k Nearest neighbours (k-NN) (iii) Selecting the k value is the most critical question k Nearest neighbours (k-NN) (iv) ‚Ä¢ Training error is zero at k = 1 because the nearest neighbour to a point is that point itself ‚Ä¢ Test error is high at low values of k due to variance (overfitting), it subsequently lowers and stabilises, and with further increase in the value of k, the test error increases again due to bias (underfitting) Using error curves is a common method to choose the value of k The value of k at which the test error stabilises and is low is taken as the optimal value of k the lowest test error Similarity measures (i) The distance metric: ‚Ä¢ Minkowski distance: the generalized form of the Euclidean (p = 2), Manhattan (p = 1) and Chebyshev ((p =ÔÇµ) distance metrics ‚ñ™ Euclidean (or L2 norm) distance: the most commonly used distance measure, and it is limited to real-valued vectors ‚ñ™ Manhattan (or city block or L1 norm) distance: measures the absolute value between two points ‚ñ™ Chebyshev (or chessboard) distance: the minimum number of moves needed by a king to go from one square on a chessboard to another equals the Chebyshev distance Similarity measures (ii) The distance metric: ‚Ä¢ Hamming distance: typically used with Boolean and string vectors ‚Ä¢ Cosine similarity: defined as the cosine of the angle between two attribute vectors Similarity measures (iii) Minkowski distance: ùëëùëù ùë•, ùë¶ = (‡∑ç ùëñ=1 ùëë »Åùë¶ùëñ ‚àí »Åùë•ùëñ ) 1/ùëù Euclidean distance: Manhattan distance: ùëë2 ùë•, ùë¶ = œÉùëñ=1 ùëë (ùë¶ùëñ‚àíùë•ùëñ)2 ùëë1 ùë•, ùë¶ = œÉùëñ=1 ùëë »Åùë¶ùëñ ‚àí »Åùë•ùëñ Chebyshev distance: ùëë‚àû ùë•, ùë¶ = max ùëñ=1‚Ä¶ùëë»Åùë¶ùëñ ‚àí »Åùë•ùëñ Similarity measures (iv) An example of the Minkowski distances: Manhattan distance =12 (red, blue, or yellow) Euclidean distance = 8.5 (green ‚Äì continuous) Chebyshev distance = 6 (green ‚Äì d i s c r e t e) Similarity measures (v) ‚Ä¢ Hamming distance: ùëëùêª ùë•, ùë¶ = ‡∑ç ùëñ=1 ùëë »Åùë¶ùëñ ‚àí »Åùë•ùëñ if ùë• = ùë¶ ‚áí ùëëùêª ùë•, ùë¶ = 0 if ùë• ‚â† ùë¶ ‚üπ ùëëùêª ùë•, ùë¶ = 1 Similarity measures (vi) An example of the Hamming distance: ùëëùêª ùë•, ùë¶ = ‡∑ç ùëñ=1 ùëë »Åùë¶ùëñ ‚àí »Åùë•ùëñ the Hamming distance is 2 because only two elements of the vectors differ dH = 0+0+1+0+0+0+1+0 = 2 Similarity measures (vii) ‚Ä¢ Cosine similarity: ùëÜùê∂ ‘¶ùë•, ‘¶ùë¶ = cos ùúÉ = œÉùëñ=1 ùëë ùë•ùëñùë¶ùëñ œÉùëñ=1 ùëë ùë•ùëñ 2 œÉùëñ=1 ùëë ùë¶ùëñ 2 if ùëÜùê∂ ‘¶ùë•, ‘¶ùë¶ = ‚àí1 ‚üπ ‘¶ùë• and ‘¶ùë¶ are exactly opposite if ùëÜùê∂ ‘¶ùë•, ‘¶ùë¶ = +1 ‚üπ ‘¶ùë• and ‘¶ùë¶ are exactly the same if ùëÜùê∂ ‘¶ùë•, ‘¶ùë¶ = 0 ‚üπ ‘¶ùë• and ‘¶ùë¶ are orthogonal intermediate values indicate intermediate similarity Normalization ‚Ä¢ Continuous numerical attributes: to avoid that some attributes dominate over others, we should generally ‚Äúnormalize‚Äù the attributes in a common range such as [0,1] or [-1, 1] ‚Ä¢ Normalization is generally required when we are dealing with attributes on a different scale as this may lead to poor model performance Normalization (ii): typical methods ‚Ä¢ z-score (or zero-mean) normalization or standardization: it re- scales a feature value so that it has distribution with 0 mean value and variance equals to 1 ùëßùëñ = ùë•ùëñ ‚àí “ßùë•ùëñ ùúéùëñ where “ßùë•ùëñ and ùúéùëñ are the mean value and the standard deviation of attribute i, respectively ‚Ä¢ min-max normalization: it re-scales a feature or observation value with distribution value between 0 and 1 ùëßùëñ = ùë•ùëñ ‚àí min ùëñ ùëã max ùëñ ùëã ‚àí min ùëñ ùëã Normalization (iii): typical methods ‚Ä¢ decimal scaling: it normalizes by moving the decimal point of values of the data. We divide each value of the data by the maximum absolute value of data ùëßùëñ = ùë•ùëñ 10ùëó where j is the smallest integer such that max ùëñ ( ùëßùëñ ) < 1 Example: our input data is -10, 201, 301, -401, 501, 601, 701 Step 1: Maximum absolute value in input data: 701 Step 2: Divide the input data by 1000 (i.e., j = 3, 103) Result: The normalized data is: -0.01, 0.201, 0.301, -0.401, 0.501, 0.601, 0.701 The k-NN algorithm 1. Select the number k of neighbours 2. Calculate the distance between the training data and the test sample 3. Take the k nearest neighbours as per the calculated distance 4. Among these k neighbours, count the number of the training data in each class 5. Assign the test sample to that category (class) with a majority of neighbours The k-NN algorithm (ii) Example: k-NN classifier (k = 5 and Euclidean distance) Advantages k-NN Advantages: ‚Ä¢ Easy to understand and implement: given its simplicity, it is one of the first classifiers that a new data scientist will learn ‚Ä¢ Adapts easily: as new training samples are added, the algorithm adjusts to account for any new data since all training data are stored into memory ‚Ä¢ Few hyperparameters: only requires a k value and a distance metric, which is low when compared to other machine learning models Disadvantages k-NN Disadvantages: ‚Ä¢ Does not scale well: since k-NN is a lazy algorithm, it uses all the training data at the runtime and hence is slow ‚Ä¢ Curse of dimensionality: the k-NN algorithm does not perform well with high-dimensional data ‚Ä¢ Complexity: O(n) for each instance to be classified ‚Ä¢ Computationally expensive: it takes up more memory and data storage compared to other classifiers The (k,l)-NN algorithm Let l be a positive integer ÔÉ©k/2ÔÉπ < l ‚â§ k, a threshold for the majority in the voting of the k nearest neighbours ‚Ä¢ This classifier consists of applying the k-NN model and make a decision based on: ‚Ä¢ if some class has received at least l votes, then assign the test sample to the most voted class ‚Ä¢ otherwise, reject the classification of the test sample (i.e., assign it to a dummy class, ÔÅ∑0) ‚Ä¢ This is a classifier with reject option Application of k-NN: the IRIS data set Sepal Length Sepal Width Species Distance Rank 5.3 3.7 Setosa 5.1 3.8 Setosa 7.2 3.0 Virginica 5.4 3.4 Setosa 5.1 3.3 Setosa 5.4 3.9 Setosa 7.4 2.8 Virginica 6.1 2.8 Versicolor 7.3 2.9 Virginica 6.0 2.7 Versicolor 5.8 2.8 Virginica 6.3 2.3 Versicolor 5.1 2.5 Versicolor 6.3 2.5 Versicolor Application of k-NN: the IRIS data set (ii) Sepal Leght Sepal Width Species 5.2 3.1 ? ùëë2(ùë•, ùë¶) = (ùë•1‚àíùë¶1)2 + (ùë•2‚àíùë¶2)2= = (5.2 ‚àí 5.3)2+(3.1 ‚àí 3.7)2= 0.608 Compute the distances between the given test sample and all training data: Application of k-NN: the IRIS data set (iii) Sepal Length Sepal Width Species Distance Rank 5.3 3.7 Setosa 0.608 5.1 3.8 Setosa 0.707 7.2 3.0 Virginica 2.002 5.4 3.4 Setosa 0.360 5.1 3.3 Setosa 0.220 5.4 3.9 Setosa 0.820 7.4 2.8 Virginica 2.220 6.1 2.8 Versicolor 0.940 7.3 2.9 Virginica 2.100 6.0 2.7 Versicolor 0.890 5.8 2.8 Virginica 0.670 6.3 2.3 Versicolor 1.360 5.1 2.5 Versicolor 0.600 6.3 2.5 Versicolor 1.250 Application of k-NN: the IRIS data set (iv) Sepal Length Sepal Width Species Distance Rank 5.3 3.7 Setosa 0.608 3 5.1 3.8 Versicolor 0.707 6 7.2 3.0 Virginica 2.002 12 5.4 3.4 Setosa 0.360 2 5.1 3.3 Setosa 0.220 1 5.4 3.9 Setosa 0.820 7 7.4 2.8 Virginica 2.220 14 6.1 2.8 Versicolor 0.940 9 7.3 2.9 Virginica 2.100 13 6.0 2.7 Versicolor 0.890 8 5.8 2.8 Virginica 0.670 5 6.3 2.3 Versicolor 1.360 11 5.1 2.5 Versicolor 0.600 4 6.3 2.5 Versicolor 1.250 10 Find ranks Application of k-NN: the IRIS data set (v) Sepal Leght Sepal Width Species 5.2 3.1 ? Find the k nearest neighbours: If k = 1 Rank = 1 Setosa If k = 5 Rank = 1, 2, ..., 5 Setosa If k = 10 Rank = 1, 2, ..., 10 Versicolor","libVersion":"0.3.2","langs":""}