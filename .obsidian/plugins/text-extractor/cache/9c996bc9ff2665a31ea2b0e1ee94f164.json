{"path":"_aula_virtual/SJK001/Reading Assessments/[Ritter11] _Ritter11 Manual Intelligence as a Rosetta Stone for Robot Cognition .pdf","text":"Manual Intelligence as a Rosetta Stone for Robot Cognition Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil Neuroinformatics Faculty of Technology Bielefeld University 33615 Bielefeld, Germany Abstract: A major unsolved problem is to provide robots with suﬃcient manual intelligence so that they can seamlessly interact with environments made for humans, where almost all objects have been designed for being acted upon by human hands. With the recent advent of anthropomorphic hand de- signs whose conﬁguration space begins to approximate that of human hands in a realistic fashion, manual intelligence for robots is rapidly emerging as an exciting interdisciplinary research ﬁeld, connecting robotics research with advances in the cognitive and brain sciences about the representation and production of dextrous motion. We argue that a thorough understanding of manual intelligence will be basic for our concepts of objects,actions, and the acquisition of new skills, while the rich grounding of manual intelligence in the physical level of interaction may make it much more approachable for anal- ysis than other, “higher level” aspects of intelligence. Therefore, we envisage manual intelligence as a “Rosetta stone” for robot cognition. To substantiate that claim, we present and discuss some of the manifold connections between manual actions and cognitive functions, review some recent developments and paradigm shifts in the ﬁeld, discuss what we consider major challenges and point out promising directions for future research. 1 Manual Intelligence as a Cross-Cutting Research Field Much of the future of our ageing society will depend on our capability to realize robots that can assist us in unprepared home environments. These robots will have to interact with humans and with objects that have been designed for being handled by humans in the ﬁrst place. Realizing robots that can cope successfully with such environment goes signiﬁcantly beyond the challenge of building robots for the factory ﬂoor. We will have to build robots whose shape and whose capabilities are well matched to the needs, expectations and domestic environments of us human beings. And to be useful in our world, 2 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil these robots will have to have hands, together with the ability to use them in a human-like fashion. This poses the signiﬁcant challenge of realizing manual intelligence. In classical AI, intelligence was primarily equated with problem solving. As we now know, this focus on reasoning and logical operations caused a long deadlock and left out all the problems that have to be solved when actions are embedded in a physical world, under conditions of partial observability, high variability, and noise. From the perspective of robotics, it omitted precisely the “prerational” parts of intelligence [15] that embodied robots require in the ﬁrst place and as a basis for the more abstract intelligence functions to erect on. When we ask where intelligence for structuring interaction is in the center, we immediately hit upon hands. Like vision, many forms of manual action also involve a high degree of ﬁne-grained perception. However, and unlike vision, this perception is now inseparably connected to action. In fact, in manual actions we ﬁnd a most impressive integration of capabil- ities to shape physical interaction, comprising all levels ranging from micro to macro and even beyond: at the “micro” scale, we ﬁnd the control of local ﬁnger contacts, involving diﬀerent contact types and the exploitation of dynamic in- teraction patterns such as rolling and sliding. These local interactions become integrated into grasp patterns to constrain objects of widely varying shape and ﬁrmness, or into haptic exploration behavior using controlled contact to identify objects and action aﬀordances. Hand-eye coordination, bimanual coordination, and goal-directed sequences of manual actions introduce even more global levels of integration and give rise to the question how interaction patterns formulated originally at the level of physics can become connected with more abstract perspectives of action semantics, goal-directedness, and intentionality. On these higher levels, we ﬁnd that hands also serve impor- tant roles in communication, thereby reaching even into the social sphere by contributing in an important way to the transfer of emotions and the experi- ence of presence; qualities that have only more recently come into the focus of modern robotics. This crucial positioning of hands and manual action at the “crossroads” of many central sensorimotor and cognitive functions makes it likely that they can play the role of a “Rosetta stone” for cognition 1 and motivates to capture the rich complex of capabilities connected with manual actions by the notion of Manual Intelligence. Like the more traditional, “higher” forms of intelligence, manual intelligence will require for its elucidation the close cooperation of researchers from many disciplines, including roboticists, computer scientists, biologists, psychologists, researchers in brain sciences, linguists, and more. 1 The Rosetta stone found 1799 by a soldier of the Napoleon troops near the village of Rosetta along the Nile river was covered by hieroglyphic, greek and demotic scripts next to each other. This enabled Jean-Francois Champollion twenty years later to decipher the hieroglyphic writing system. Manual Intelligence as a Rosetta Stone for Robot Cognition 3 2 Platforms for Manual Intelligence Research The availability of increasingly sophisticated robot hands [6] is a strong driv- ing force for robot manual intelligence. While the Utah-MIT hand [29] was a kind of yardstick design for a long time, the recent decade has seen a surge of developments towards lighter and more ﬂexibly useable hands. The character- istics of some major contenders are summarized in Table 1. Systems like these begin to provide us with ”output devices” to reach beyond simulation when trying to test ideas about the synthesis of manual actions or when aspiring to turn such ideas into practical utility. Since most ”natural” hand actions tend to involve bimanual interaction, an ideal setup should comprise a pair of interacting arms. The high eﬀort to set up such systems makes such platforms even nowadays still a scarce resource. Among the few existing bimanual systems with advanced hands, the perhaps most widely known platforms are at DLR [48], NASA [36], and the Dexter system [17] using two non-anthropomorphic Barrett hands. Figure 1: Bimanual system with two Shadow Hands [67] mounted on 7-DOF PA-10 arms for positioning. Model ﬁn- joints active act. Ref gers DOFs type Shadow 5 24 20 pn. [67] Robonaut 5 22 14 el. [36] GIFU-III 5 20 16 el. [42] DLR-II 4 18 13 el. [11] Utah-MIT 4 16 16 pn. [29] Barrett 3 8 4 el. [76] Table 1: Data of some dextrous robot hands (el.=electrical, pn.=pneumatic actuator type). The recently completed Bielefeld research platform is depicted in Fig. 1. Featuring two anthropomorphic Shadow Hands with 20 DOF each, mounted on Mitsubishi PA-10 arms, it comprises a total of 54 independent degrees of freedom. 24 hall sensors per hand provide accurate joint angle feedback to control the 80 miniature solenoid on-oﬀ valves that adjust air in- and outﬂow into the pneumatically driven ”muscle”-like actuators transmitting their forces via tendons to the ﬁngers. The system is complemented with a 4-DOF mobile camera head for visual monitoring of the workspace. In the ﬁnal setup each manipulator will be additionally equipped with 186 tactile sensors distributed over the ﬁnger pads. Despite still far away from the capabilities of human hands, platforms like these begin to cross the critical threshold beyond which one can begin to study issues of manual intelligence in a robotics setting. 4 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil 3 Low Level Aspects Low level aspects of manual control are the groundwork required for manual intelligence to exist. The traditional issues in this ﬁeld are the sensing and modeling of local contacts, forces and the resulting dynamics. While mechanics is a well-established branch of physics, the modelling of realistic soft ﬁnger con- tacts with friction, sliding and rolling still poses signiﬁcant challenges and even gaps in our knowledge, e.g., with regard to a consistent modeling of friction. Much work has been done in these areas, excellent reviews with exhaustive references to earlier work can be found in [5],[47], [74]. It is very helpful that nowadays there exist simulation packages [38],[14] that oﬀer or allow to build simulators to explore aspects of manual interaction in simulation, although the realism of these simulators is still limited due to the aforementioned gaps in our knowledge how to precisely model physical interaction. Haptic perception is another closely linked area. Sensing technology to match better the rich tactile sensing capabilities of our hands remains a largely unsolved major challenge [74]. This is very diﬀerent from vision, where high resolution cameras are readily available. Although some analogies with vi- sion are likely to exist, the much stronger coupling between sensor activations and self-generated movements causes signiﬁcant new diﬃculties. This makes it likely that ideas borrowed from robot vision will require non-trivial gen- eralizations towards a much stronger coupling between sensor patterns and control actions. Therefore, it seems not surprising that the development of algorithms for haptic perception in robots is a much less developed ﬁeld than in robot vision. Much like visual databases have proven tremendously use- ful, a systematic development for robot haptics might beneﬁt from similar databases in the haptic domain [66]. Another major challenge is the devel- opment of cross-modal visuo-haptic representations to guide manual actions and exploration, or to provide a principled basis for a multi-modal memory system. 4 Grasping The question of what is a good grasp and how such grasps may be formed is another shared focal point of researchers in robotics, neuroscience and psy- chology [68],[9],[60],[13]. This has exposed fruitful interconnections between these disciplines: an- alytical approaches in robotics viewing grasp formation as a constrained se- lection of grasp points according to some optimization criterion [10],[8] have found sucessful analogues in modeling aspects of human grasp selection [69]. In the other direction, analysis and modeling of human reach-to-grasp behavior with respect to timing [30] and the role of sensory guidance [62] has suggested low-dimensional ”dynamical templates” for grasp behavior that are shaped by adjusting only a small number of parameters. Adopting such biologically moti- vated templates as behavioral primitives stimulated the realization of robust Manual Intelligence as a Rosetta Stone for Robot Cognition 5 grasp behavior in robots [33],[26]. With regard to the ﬁnal grasping phase, these models replace the optimization-based grasp point selection by a dy- namical ﬁnger closure process starting from a hand preshape and “wrapping” the ﬁngers under tactile feedback around the object. This shares the idea of grasp generalization from prototypes [53], but along a more behaviorally mo- tivated route. A major issue then is the choice of a good hand preshape, which can be based on existing grasp taxonomies, such as [16]. If this choice is care- fully made, even as few as ﬁve diﬀerent preshapes can enable the grasping of a wide range of diﬀerent objects [58], oﬀering an approach to robust grasping in the absence of detailed object models. A more detailed study [59], involving also measurements of human grasping, suggests further optimizations, such as the maximization of ﬁnger contact synchrony and thumb opposition. Figure 2: Example grasps (left) of the Shadow Hand with the algorithm from [59] for a benchmark collection of 21 common household objects (shown on the right). Finally, we only mention that grasping is connected with further non- trivial cognitive abilities, including the interplay of visual object recognition and non-visual memory to predict object properties such as weight, ﬁrmness and surface friction and the anticipation of the future state of the grasped object to properly constrain grasp choices [78], e.g., to minimize the need for regrasping. 5 Manipulation and Tool Use Most manual skills require to move the grasped object within the hand. Small movements can be eﬀected by changing the ﬁnger stiﬀness matrix to shift a current equilibrium conﬁguration. Larger movements may require regrasping, 6 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil necessitating coordinated ”ﬁnger gaits” [27],[34]. A typical characteristic of such manipulation sequences is their hybrid nature: smooth changes in ﬁnger state variables are interrupted by discrete events when contact conditions change. A suitable architecture to deal with such a situation is a combination of several controllers, with event-triggered switches between them [52]. Such techniques may help to organize coordinated ﬁnger movements into the numerous higher level interaction patterns that make up our daily man- ual skills. In many of these interaction patterns hands act as a specialized tool, such as tweezers, pliers, pincers, a hook, a hammers, a specialized feeder mechanism and more. In fact, it has been argued that tools themselves can be viewed as extensions of the natural capabilities of our hands [22]. There- fore, tool use, – either through conﬁguring the hand itself, or augmenting it with a suitable object – is at the core of manual intelligence since it connects the physical properties of actuator mechanisms with the functional roles that they can ﬁll in particular contexts. The concept of aﬀordances has been put forward long ago [22] to capture that point. However, it has been found ex- tremely diﬃcult to ground in physical robot-world interactions, for one of the very rare demonstrations, see [73]. In a recent paper the creators of the NASA robonaut system confess that currently autonomous tool use for robots ap- pears as an ”inﬁnitely open challenge” [56]. A review and analysis of cognitive requirements for a tool-using agent [2] concludes that a ”Tooling test” might oﬀer an worthwhile major benchmark about robot intelligence. 6 Communication and Social Interaction A clear bias of robotics for manual intelligence is to carry out actions on ob- jects. However, in humans hands are also strongly involved in various levels of communication. Gestures accompanying speech can greatly add to the expres- siveness of the utterances, and frequently also help to resolve ambiguities [37]. Such an auxiliary role can be even more useful for robots (both in the speaker and in the listener role), given that their speech capabilities are much more limited than in humans [72],[39]. Perhaps the least replaceable communica- tive function of hands is in demonstrating manual skills: here being able to visually watch how the hands interact with the task object(s) is in most cases crucial for being able to learn to imitate the skill. Therefore, to be extensible manual intelligence has also to integrate highly specialized visual capabilities for advanced hand posture recognition [44] – unless one is willing to resort to more technical means of skill acquisition, such as motion capture utilizing gloves, exoskeletons or special markers [31]. Finally, hands are centrally involved in emotional communication. This is already apparent in gestures [37], but becomes even more evident when using hands for “getting in touch”. Comparing the degree of “presence” felt for artiﬁcial agents, ranging from unembodied over virtual embodied and ﬁnally physically embodied it has been found that the possibility of touching an agent Manual Intelligence as a Rosetta Stone for Robot Cognition 7 with our hands is a major factor that strongly distinguishes the capabilities of virtual and physically agents to elicit an experience of presence and to aﬀect feelings such as, e.g. loneliness [35]. This suggests that even the emotional and social aspects of haptic interaction [23] can be an important factor for the acceptance of future robots, even if it may in many situations be expressed in not more than a friendly handshake. 7 Learning How to Grasp - Grasping How to Learn Most of what our human hands can do has been acquired by learning. This should make it not too surprising that learning is a pervading topic for manual intelligence. For the control of robot hands, learning approaches have been considered at various levels. The most longstanding work is on learning of the various coordinate mappings required for eye-hand coordination. Here, the target usu- ally is the construction of a mapping between two coordinate systems. Many approaches have been developed for this task, for overviews, see e.g. [3],[50],[4]. Forming a grasp can be approached with similar techniques, however, now the number of involved degrees of freedom is usually higher, and it is less clear which features should be used as input and as output. Often these works assume the availability of a geometric object model or exploit the use of simulation techniques to generate artiﬁcial training examples [21],[51],[70]. Another interesting approach is the development of analytically motivated schemes for generalizing a small set of accurately observed action examples, usually gained from motion trackers [53],[55],[54]. A recent review [31] links these techniques to the general issue of (VR-based) action capture and its connections to imitation learning. Direct use of visual input is much more demanding. Most works attempt to estimate a suitable gripper orientation for ﬁxed, programmed grasping prim- itive [64],[50],[26]. Some works also demonstrate direct grasp point extraction along 2D object contours [32] or even on novel 3D objects [63]. These works have made apparent that the high dimensionality of man- ual interactions will make learning scalable only when we manage, connect, and guide learning at the lower levels with learning at more abstract lev- els of representation. This insight – together with ﬁndings from neuroscience hinting at a shared neural substrate for the representation of perception and action in a ”mirror neuron system” [57],[13] – has sparked a lot of interest in investigating imitation as a suﬃciently powerful route for skill acquisition [49],[7],[18]. Cognitive scientists interested in a deeper understanding how in- fants imitate distinguish three major levels of increasing abstraction on which imitation can be attempted: (i) body trajectories, (ii) limb relations rela- tive to objects, and (iii) intentions [40]. While many current approaches to imitation learning address the ﬁrst level [1],[28],[43], only a relatively small number of works demonstrates imitation at the upper levels of task relations 8 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil and intention understanding [75], [45],[79],[46],[71]. Synthesizing higher levels of manual intelligence thus appears to depend crucially on our ability to merge existing statistical and interpolation type approaches of learning with novel approaches [77],[65] enabled from a deep understanding how we can represent, recognize and reason about the functional signiﬁcance of hand, objects, their aﬀordances, relations and the underlying intentions of the involved actors. 8 Measuring Manual Intelligence A natural question to ask is: how might we measure a robot’s level of manual intelligence? While some domains in robotics begin to enjoy a gradual emergence of procedures for performance comparisons [19], e.g. through suitable compe- titions, any established benchmark or competition procedures even for the rather circumscribed activity of grasping (within the larger spectrum of man- ual intelligence) at present simply do not exist. A tentative proposal within the EURON initiative is based on a bimanual Barrett hand system and pro- poses to evaluate grasp success for a number of (artiﬁcial) benchmark objects [41]. A diﬀerent benchmark, employing a set of 21 widely available household objects (shown in Fig.2), has been suggested in [59] and has been used to compare grasp optimization schemes on two diﬀerent robot hands [58]. Useful guidance for measuring manual intelligence might be provided from surgery, where the comparison of diﬀerent training strategies with respect to their impact on the acquisition of manual skills in surgeons is an important issue [24]. For instance, manual skills in using a laparoscope have been suc- cessfully modelled as temporal force and torque proﬁles imparted on the in- strument [61]. In the study of child development, a widely accepted procedure for measuring the development stage of motor skills is the Peabody Motor Development Scale [20]. It has a part speciﬁcally focusing on grasping skills, featuring 26 diﬀerent test tasks each of which is ranked on a nominal 3-scale. Another 72 tasks measure visuo-motor coordination. While the majority of these tests are probably still too hard for the level of manual intelligence of today’s robots, they might become useable in the near future when robot hands can do more than now. Until then, these test designs might provide use- ful inspiration how to design manual skill benchmarks for robots, for instance, embracing instruction by demonstration as a natural part of any performance measurement. 9 Concluding Remarks Evolutionary anthropologists are discussing the question how closely the de- velopment of rich manual capabilities may be linked with the evolutionary origin of human intelligence [12]. While this is an open problem, the richness Manual Intelligence as a Rosetta Stone for Robot Cognition 9 of issues connected with the dextrous use of sophisticated hands makes it very likely that manual skills in robots will become a major measure of our progress towards creating intelligent machines. Therefore, we envisage Man- ual Intelligence as a promising upcoming research ﬁeld with the potential to connect many key strands of current robotics research in a fruitful fashion, as well as oﬀering fascinating interdisciplinary bridges into physics, biology, brain science, cognition research and linguistics. References 1. A. Alissandrakis, C. L. Nehaniv, and K. Dautenhahn (2002) Imitation with ALICE: Learning to Imitate Corresponding Actions Across Dissimilar Embod- iments. IEEE Trans. Systems, Man and Cybernetics, 32(4):482-296. 2. St. Amant, R., and Wood, A. B. (2005). Tool use for autonomous agents. Proc. National Conf. on Artiﬁcial Intelligence (AAAI), pp. 184-189. 3. Atkeson CG (1989) Learning Arm Kinematics and Dynamics Ann. Rev. Neu- rosci. 12:157-183. 4. Barreto, G., Araujo A. & Ritter H. (2003) Self-Organizing Feature Maps for Modeling and Control of Robotic Manipulators. J. of Intelligent and Robotic Systems 36, 407-450. 5. A Bicchi, V Kumar (2000) Robotic grasping and contact: a review Proceedings ICRA’00 pp. 348-353 6. A. Bicchi (2000) Hands for dexterous manipulation and robust grasping: a diﬃcult road toward simplicity IEEE Trans. Robotics Autom. 16(6):652-662 7. A. Billard and R. Siegwart, editors. Special Issue on Robot Learning from Demonstration, volume 47 of Robotics and Autonomous Systems, 2004. 8. C. Borst, M. Fischer, and G. Hirzinger (2003). Grasping the dice by dicing the grasp. IROS’03 Proceedings, pp. 3692-3697 9. C. Borst, M. Fischer, and G. Hirzinger (2005). Eﬃcient and precise grasp plan- ning for real world objects. In: Multi-point Interaction with Real and Virtual Objects (F. Barbagli, D. Prattichizzo, and K. Salisbury, eds.), Tracts in Ad- vanced Robotics 18, pp. 91-111. 10. Borst C., Fischer M., Hirzinger G. (2002) Calculating hand conﬁgurations for precision and pinch grasps. Proc. IEEE IROS 2002, pp. 1553-1559. 11. J. Butterfass, M. Fischer, M. Grebenstein, S. Haidacher, and G. Hirzinger (2004) Design and experiences with DLR Hand II, in Proc. World Automa- tion Congress, Sevilla. 12. RW Byrne (2003) The manual skills and cognition that lie behind hominid tool use. In: AE Russon & DR Begun (eds) Evolutionary origins of great ape intelligence. Cambridge University press. 13. U Castiello (2005) The Neuroscience of Grasping Nat Rev Neurosci 6:726-736 14. CMLabs (2006) Vortex physics engine for real-time simulation. URL http://www.cm-labs.com/products/vortex/. 15. Cruse H., Dean J. and Ritter H. (eds) (2000) Prerational Intelligence – Adaptive Behavior and Intelligent Systems Without Symbols and Logic. Vol 1-3, Studies in Cognitive Systems, Kluwer Academic Publishers 16. Cutkosky M.R. (1989) On Grasp choice, grasp models and the design of hands for manufacturing tasks. IEEE Trans. Robotics and Automation 5(3), 269-279 17. Dexter - Mechanism, Control and Developmental Programming http://www- robotics.cs.umass.edu/Research/Humanoid/humanoid index.html 10 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil 18. K. Dautenhahn and C. Nehaniv, editors. (2002) Imitation in Animals and Ar- tifacts. MIT Press. 19. R¨udiger Dillmann (2004) Benchmarks for Robotics Research. Euro- pean Robotics Network http://www.cas.kth.se/euron/euron-deliverables/ka1- 10-benchmarking.pdf 20. M.R. Folio, R. R. Fewell (2000) Peabody Developmental Motor Scales PDMS-2 Therapy Skill Builders Publishing 21. Fuentes, O. Nelson, R.C. (1998) Learning dextrous manipulation skills for mul- tiﬁngered robot hands using the evolution strategy. Machine Learning 26:1-16 22. Gibson, J. J. (1979) The ecological approach to visual perception. Houghton Miﬃn, Boston. 23. A. Haansa, W. Ijsselsteijnb (2007) Mediated Social Touch: A Review of Current Research and Future Directions Virtual Reality 24. Hamdorf J.M. Hall J.C. (2000) Acquiring surgical skills. British Journal of Surgery (87):28-37 25. R. Haschke, J. J. Steil, I. Steuwer, and H. Ritter. Task-oriented quality measures for dextrous grasping. In Proc. Conference on Computational Intelligence in Robotics and Automation. IEEE, 2005. 26. Hauck, A. Passig, G. Schenk, T. Sorg, M. F¨arber, G. (2000) On the perfor- mance of a biologically motivated visual controlstrategy for robotic hand-eye coordination Proc. IROS 2000, 3:1626-1632 27. M. Huber and R. A. Grupen (2002) Robust ﬁnger gaits from closed-loop con- trollers IROS’02 Proceedings pp.1578-1584 28. A. Ijspeert, J. Nakanishi, and S. Schaal (2002). Movement Imitation with Non- linear Dynamical Systems in Humanoid Robots. In IEEE International Confer- ence on Robotics and Automation. 29. S. C. Jacobsen, E. K. Iversen, D. F. Knutti, R. T. Johnson, and K. B. Biggers (1986) Design of the Utah/MIT dexterous hand. ICRA Conf. Proceedings, pp. 1520-1532 30. Jeannerod (1984) The timing of natural prehension movments. J. Motor Be- havior 16(3), 235-254. 31. B. Jung, H. Ben Amor, G. Heumer & M. Weber (2006) From Motion Cap- ture to Action Capture: A Review of Imitation Learning Techniques and their Application to VR-based Character Animation. Proc. VRST’06 pp. 145-154 32. Kamon, I. Flash, T. Edelman, S. (1996) Learning to grasp using visual infor- mation ICRA’96 Vol 3. 2470-2476 33. Kragic D, Christensen H.I. (2003) Biologically motivated visual servoing and grasping for real world tasks IROS 2003 Proceedings Volume 4:3417-3422 34. Y. Kurita, J. Ueda, Y. Matsumoto, and T. Ogasawara (2004) CPG-based ma- nipulation: generation of rhythmic ﬁnger gaits from human observation. ICRA Conf. Proc. pp. 1209-1214 35. KM Lee, Y Jung, J Kim, SR Kim (2006) Are physically embodied social agents better than disembodied social agents?: The eﬀects of physical embodiment, tactile interaction, and people’s loneliness in human-robot interaction Int. J. of Human-Computer Studies 64(10):962-973 36. C. S. Lovchik and M. A. Diftler (1999) The robonaut hand: A dexterous robot hand for space in Proc. ICRA, Detroit, 1999. 37. D MacNeill (1992) Hand and Mind: what gestures reveal about thought Uni- versity of Chicago Press 1992 38. A.Miller and P. K. Allen (2004) Graspit!: A versatile simulator for robotic grasping. IEEE Robotics and Automation Magazine 11(4):110-122 39. P. McGuire, J. Fritsch, H. Ritter, J. Steil, F. Rothling, G. A. Fink, S. Wachsmuth, and G. Sagerer (2002) Multi-modal human-machine communica- tion for instructing robot grasping tasks in IROS, pp.1082-1089. Manual Intelligence as a Rosetta Stone for Robot Cognition 11 40. A. N. Meltzoﬀ (1996). The Human Infant as Imitative Generalist: A 20-year Progress Report on Infant Imitation with Implications for Comparative Psy- chology. In Social Learning in Animals: The Roots of Culture, pages 347-370. 41. A. Morales (2006) Experimental benchmarking of grasp reliability. http://www.robot.uji.es/people/morales/experiments/benchmark.html 42. T Mouri, H Kawasaki, K Yoshikawa, J Takai, S Ito (2002) Anthropomorphic Robot Hand: Gifu Hand III Proc. of Int. Conf. ICCAS2002 43. J. Nakanishi, J. Morimoto, G. Endo, G. Cheng, S. Schaal, and M. Kawato (2004). Learning from Demonstration and Adaptation of Biped Locomotion. Robotics and Autonomous Systems, 47(2-3):79-91, 44. N¨olker, C. Ritter, H. (2002) Visual recognition of continuous hand postures IEEE Transactions on Neural Networks 13(4):983-994 45. K. Ogawara, S. Iba, T. Tanuki, H. Kimura, and K. Ikeuchi (2001) Acquiring handaction models by attention point analysis. Intl. Conf. on Robotics and Automation, pp. 465-470. 46. K. Ogawara, J. Takamatsu, H. Kimura, and K. Ikeuchi (2003) Extraction of Es- sential Interactions through Multiple Observations of Human Demonstrations. IEEE Trans. on Industrial Electronics, 50(4), 2003. 47. AM Okamura, N Smaby, MR Cutkosky (2000) An overview of dexterous ma- nipulation Proceedings ICRA’00 pp. 255-262 48. Ott C, Eiberger O, Friedl W, Bauml B, Hillenbrand U, Borst C, Albu-Schaﬀer A, Brunner B, Hirschmuller H, Kielhofer S, Konietschke R, Suppa M, Wim- bock T, Zacharias F, Hirzinger G (2006) A Humanoid Two-Arm System for Dexterous Manipulation. In 6th Humanoid Robots Conf., pp. 276-283 49. E Oztop, M Kawato, M Arbib (2006) Mirror neurons and imitation: a compu- tationally guided review Neural Networks 19:254-271 50. Pauli J (1998) Learning to Recognize and Grasp Objects Autonomous Robots 5(3-4):407-420 51. Pelossof, R. Miller, A. Allen, P. Jebara, T. (2004) An SVM learning approach to robotic grasping Proc. ICRA’04 pp. 3512-3518 52. R. Platt, A. H. Fagg, and R. Grupen (2004) Manipulation gaits: Sequences of grasp control tasks in Proc. ICRA, New Orleans. 53. N. S. Pollard (1996) Synthesizing grasps from generalized prototypes ICRA Conf. Proc. 3:2124-2130 54. Nancy S. Pollard, Victor B. Zordan (2005) Physically Based Grasping Control from Example (2005) Eurographics/ACM SIGGRAPH Symposium on Com- puter Animation, pp. 311-318 55. Nancy S. Pollard, Jessica K. Hodgins (2002) Generalizing Demonstrated Ma- nipulation Tasks Workshop on the Algorithmic Foundations of Robotics, France 56. F Rehnmark, W Bluethmann, J Mehling, RO Ambrose, Myron Diftler, Mars Chu, and Ryan Necessary (2005) Robonaut: The Short List of Technology Hur- dles Computer 38:28-37 57. G. Rizzolatti, L. Fogassi, and V. Gallese (2001). Neurophysiological Mecha- nisms Underlying the Understanding and Imitation of Action. Nature Reviews Neuroscience, pages 661-770 58. R¨othling F., Haschke R., Steil J.J. & Ritter H. (2007) Platform Portable An- thropomorphic Grasping with the Bielefeld 20 DOF Shadow and 9 DOF TUM Hand. IEEE IROS Conference Proceedings 59. R¨othling F. (2007) Real Robot Hand Grasping using Simulation-Based Opti- misation of Portable Strategies Dissertation, Faculty of Technology, Bielefeld University 60. DA Rosenbaum, R Meulenbroek, J Vaughan (2001) Planning Reaching and Grasping Movements: Theoretical Premises and Practical Implications. Motor Control 2:99-115 12 Helge Ritter, Robert Haschke, Frank R¨othling, and Jochen J. Steil 61. Rosen, J. Hannaford, B. Richards, C.G. Sinanan, M.N. (2001) Markov modeling of minimally invasive surgery based on tool/tissueinteraction and force/torque signatures for evaluating surgical skills IEEE Trans. Biomed. Engineering, 48(5):579-591 62. M Santello, M Flanders, JF Soechting (2002) Patterns of Hand Motion dur- ing Grasping and the Inﬂuence of Sensory Guidance Journal of Neuroscience 22(4):1426-1435 63. A. Saxena, J. Driemeyer, J. Kearns, and A. Ng (2007) Robot grasping of novel objects. NIPS 19 (B. Schlkopf, J. Platt and T. Hoﬀman eds) pp. 1209-1216 64. M Salganicoﬀ, LH Ungar, R Bajcsy (1996) Active Learning for Vision-Based Robot Grasping Machine Learning 23:251-278 65. van Schie HT, Koppen M, Erlhagen W, Bekkering H (2006) Goals and means in action observation: a computational approach Neural Networks 19:311-322 66. Sch¨opfer M, Ritter H, Heidemann G (2007) Acquisition and Application of a Tactile Database ICRA Conf. Proceedings pp.1517-1522 67. Shadow Robot Company, The Shadow Dextrous Hand. [Online]. Available: http://www.shadow.org.uk/products/newhand.shtml 68. K.B. Shimoga (1996) Robot Grasp Synthesis Algorithms: A Survey Int. J. Robotics Research 15(3):230-266 69. JB Smeets and E Brenner (1999) A new view on grasping Motor Control 3:237- 271 70. Steﬀen J.F. , Haschke R. & Helge Ritter (2007) Experience-based and Tactile- driven Dynamic Grasp Control. Proc. IEEE IROS Conf. (in press). 71. J. Steil, F. Rothling, R. Haschke, and H. Ritter (2004) Situated robot learn- ing for multi-modal instruction and imitation of grasping. Robotics and Au- tonomous Systems, Special Issue Robot Learning by Demonstration, 47:129- 141. 72. J. Steil, G. Heidemann, J. Jockusch, R. Rae, N. Jungclaus, and H. Ritter (2001) Guiding attention for grasping tasks by gestural instruction: The GRAVIS- robot architecture. Proc. IROS, pp.1570-1577. 73. Stoytchev, A. (2005) Toward learning the binding aﬀordances of objects: A behavior-grounded approach. Proc. AAAI Symposium on Developmental Robotics, 17-22 74. J Tegin, J Wikander (2005) Tactile sensing in intelligent robotic manipulation - a review Industrial Robot: An International Journal 32(1):64-70 75. H. Tominaga and K. Ikeuchi (1999) Acquiring Manipulation Skills through Ob- servation. IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems, pages 7-12. 76. Townsend W (2000) The BarrettHand grasper – programmably ﬂexible part handling and assembly Industrial Robot 27(3):181-188. 77. M Viezzer, CHM Nieywenhuis (2005) Learning aﬀordance concepts: some sem- inal ideas. Int. Joint Conf. on Artiﬁcial Intelligence 78. Weigelt M., Kunde W. & Prinz W. (2006) End-state comfort in bimanual object manipulation. Exp. Psychol. 53(2):143-148. 79. R. Z¨ollner, O. Rogalla, R. Dillmann, and M. Z¨ollner (2002). Understanding Users Intention: Programming Fine Manipulation Tasks by Demonstration. IEEE/RSJ Int. Conference on Intelligent Robots and Systems, IROS 2002","libVersion":"0.3.2","langs":""}