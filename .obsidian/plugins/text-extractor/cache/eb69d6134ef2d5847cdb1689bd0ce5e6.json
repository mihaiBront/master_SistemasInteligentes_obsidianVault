{"path":"_aula_virtual/SJK004/Unit 1_ Modelling an Intelligent agent_3.pdf","text":"It is the same situation in RL settings than in MDP settings? Not. In RL settings we do not know the transitions neither the rewards. Therefore, in RL we have to experiment with the environment to figure out something about T and R. How? Maybe: First: Drawbacks? Any? ‚Ä¶ YES! Remember the overall view of RL Environment Agent Policy Value function Model? State Action Reward Policy: how to choose an action. Action : performed by the agent on the environment. Value function : to valuate how well it is the current situation for the agent. Model: an idealized representation of the real environment (might be useful for the goals of the agent). State: the set of values that characterize the current situation in the environment. Reward: the value returned by the environment to the agent in response to the consequences of the action performed. Model Free vs Model Based approaches Model Based approaches tries to estimates the probability distribution of this function (the model) before estimating the expectation. So, in this case, works by sampling K points from a distribution P(x), then, estimates , and finally, the expectation of f(X) is calculated by Suppose we want to estimate the expectation of an unknown function f(X) given K samples. Model Free vs Model Based approaches Model Free approaches estimates the expectation of f(X) as: So, it is valued how each x i contributed to the f(X) averaging. How the Q-value iteration algorithm, previously exposed, can be rewritten now in this RL context using the sample-based approach? Model Free approaches (Q-learning ) Recall from the Q-value-state iteration rule: In RL we don‚Äôt know transitions neither rewards, so we have to try an action from a state and observe what happens. The above formula tells us how we can iteratively compute the Q values for the optimum policy by bootstrapping . But, the summatory is unknown: samples (s, a, s‚Äô, r) are obtained one by one, Then, for each sample the above formula for compute Q*(s,a) is just : Model Free approaches (Q-learning ) First option: taking k samples from the same pair (state, action): And then: sample1 Drawbacks? Any? ‚Ä¶ YES! sample2 samplek But, given that 0 < ùõÇ <= 1, then . We get: Second option (EMA): Model Free approaches (Q-learning ) Then, using EMA for estimating the Q values, we get: Using the Exponential Moving A verage: Model Free approaches (Q-learning ) Example: Given a MDP as the one pictured in the classroom. Let suppose that we don‚Äôt know transitions neither rewards. Let's suppose that we had collected the following samples in that order: (s1, a1, s1, 1), (s1, a1, s2, -1), (s2, a2, s1, 1) and using an alpha value of 0.75 and a gamma value of 0.5. Of course, Q(s,a) = 0 for step 0 for all s and all a. Calculate the values of Q for those samples. Recall that: or, equivalently: Algorithm to compute the Q values Set Q0 (s,a) arbitrarily (e.g. as 0 for all pairs state,action) For i in range(number_steps): Choose an action a (****) Collect a sample: How to figure out in each step which action to execute? Model Free approaches (Q-learning ) How to choose the value of ùõÇ? Bigger values makes the process unstable. Smaller values slows down the convergence. The ùõÜ-greedy approach to select an action The ùõÜ-greedy approach tries to balance between exploration and exploitation . ‚óè Exploration tries to figure out other ways to act from the most performed. ‚óè Exploitation tries to follow up the most successful actions found up to now. This approach does the following: ‚óã With probability ùõÜ, sample a random action. ‚óã With probability 1 - ùõÜ, choose the best currently available action. The value of ùõÜ should decay with time after certain step during training. The previous algorithm for training using this ùõÜ-greedy approach, as the code line marked as (****) in the previous shown algorithm is known as: Q-learning algorithm Model Free approaches (Q-learning ) An easy example, but powerful: How to train an taxi from scratch to behave optimally without programming how to resolve its goals? Q.learning! In the aula virtual you can read the Q-learning code example for this problem. This scenario is drawn by the Gymnasium initiative driven by Open-AI to facilitate several examples for algorithms of RL to be applied to. You can take a look at https://gymnasium.farama.org/index.html Model Free approaches (Q-learning ) The taxi environment is shown in the following animation image (its features are deeply explained in the code link at the aula virtual) Q-learning example: Learning to play Blackjack Blackjack is a game whose goal is to obtain cards from a deck, the sum of whose numerical values be as much as possible without exceeding 21. An ace can count as 1 or 11 points, and all face cards count as 10. There is a dealer and a player. The game starts with two cards dealt to both the dealer and the player. The dealer has one card face up and one face down, while the player has both cards face up. A natural is a starting hand in which a player has exactly 21 (an ace and a face card), and in this case, the player wins unless the dealer also has 21. In the latter case, it's a draw (nobody wins or loses). If the player does not have a natural, it can request more cards, one by one (hit) until she decides to stop (stick) or it exceeds 21 (go bust and lose). If the player sticks, it is the dealer turn. Suppose that the dealer strategy is to stick on any sum of 17 or greater, and to hit otherwise. If the dealer goes bust, player wins; otherwise,the outcome of the game is determined by whose final sum is closer to 21 (description closely drawn from the book Reinforcement Learning by Sutton and Barto) Q-learning example: Learning to play Blackjack How to approach this game as an RL problem for the player? States? Actions? Rewards? Three values characterize each possible state: the sum of the cards from the player (X), the sum for the cards face up by the dealer (Y) and if the player has an usable ace or not (Z): (X, Y, Z) with X from 12 to 21, Y from 1 to 10 and Z binary (whether or not she holds and usable ace). How many possible states? 200. Two actions: hit (0), asking for one more card, or stick (1) +1 if player wins, -1 if player loses, 0 if player draws ‚Äî Read, discuss and understand the Colab Notebook for Q-learning Blackjack available in the aula virtual ‚Äî Model Free approaches (limitations of Q-learning ) Q-learning is a great method for RL, but what could happen if the number of states and/or actions is really big? We might need a very big table for representing all the values for the Q values, and maybe this would be inefficient, or even impossible! Even more, how Q-learning can deal with states representing continuous measures? Well, there is always the option to discretize the continuous values in sequential and no overlapping intervals, but some features of the environment could be lost. More drawbacks? Yes. If the number of states is big (being this number continuous or discrete) how you can guarantee that the experiences run by the RL agent visit every one of these states for each possible action? The most usual setting is that you can not guarantee that! But, we could use another approach to deal with these issues: represent the states, and the whole process of Q-learning, as continuous functions that we want to approximate with computational techniques. Advantages of this approach? A lot! Model Free approaches (Neural network )Model Free approaches (Deep Q-learning ) DQN (Deep Q-Network) is Q-learning + Artificial Neural Network What is an Artificial Neural Network (ANN)? An ANN is a computational structure composed of several layers of neurons. Each neuron receives some weighted (ùöπ) linear combination of input data and produces a new datum according to its function of activation. All neurons in a layer usually apply the same activation function. Examples of neuron activation functions : b ùöπ 1 ùöπ n ùöπ i x i x n x 1 f(x) x = x 1ùöπ 1 + .. + x iùöπ i + ‚Ä¶ + x nùöπ n + b f(x) = max(0,x) = 0 if x < 0 x otherwise f(x) = tanh(x) ReLU: Tanh: 1 -1 The whole ANN goal is to approximate an (usually) unknown function by iterating: Provide input data to the first layer of neurons, Move data from the neurons of the input layer to the neurons of the output layer. Each neuron uses its activation function given the weighted linear combination of its inputs to calculate its output value, From the output data in the last layer of neurons compute the loss (i.e. comparison of this output data with the correct result), And, finally, minimise the loss (e.g. using a gradient descent method) by changing the weights of the neurons from the output to the input layer (backpropagation) What is a Deep ANN (DNN)? An ANN with 3 or more layers (e.g. the one above). Input layer Output layer Hidden layer/s Input dataNeural network (NN) Model Free approaches (Deep Q-learning ) Output data 1 2 3 4 1 3 2 4 Model Free approaches (Deep Q-learning ) The true value of f for the input x i The output value of f for the input x i provided by the NN The loss, ùìõ, enable to measure how far the network output is from the correct output. So, we want to minimize this loss by adjusting the weight parameters of all neurons in the NN: ùöπ* = arg minùöπ ùìõ(ùöπ) The loss has to be differentiable, because the modifications on the weights will be carried out by using gradients of this loss. A very usual definition of loss is the Mean Square Error (MSE) over a batch of M inputs: Model Free approaches (Deep Q-learning ) How to minimize the loss ? A common optimization technique is the gradient descent . The gradient of the loss function is: (the vector in the space of all weights which points in the direction where the loss function is bigger) As our aim is to minimize the loss function, we have to follow the negative gradient ! There are three ways to sequentially update the weights using the negative gradients of the loss function (ùõº, hyperparameter, is the learning rate , the ‚Äúsize‚Äù of the step to give in the direction of the negative gradient): Batch gradient descent: Big cost, low variance Stochastic gradient descent: Low cost, high variance from sample to sample Mini-batch gradient descent: a balance between the above two Model Free approaches (Deep Q-learning ) What is backpropagation ? An ANN computes a compositional function: the weighted output of several activation functions as the input of another activation function, layer by layer up to the output layer). Then, the loss is calculated and the gradient descent to minimize the loss has to be applied to every weight in the network from the output to the input layer. The chain rule of derivatives can be applied in this backward pass for the compositional function of the ANN. Remember, the chain rule states that for y = f(x) and z = g(y) =g(f(x)) then: The gradients of a compositional function with respect to the inputs of a inner function is computed as a multiplication of the Jacobian of the inner function g and the gradient of the outer function with respect to its inputs. The backpropagation algorithm uses automatic differentiation programming techniques to compute these gradients. It is usual that modern frameworks for computing with ANN (pytorch, tensorflow, etc) provide functions to automatically compute this backpropagation pass, hiding for the programmer its implementation details. Model Free approaches (Deep Q-learning ) An ANN can be used to approximate a policy function or a value function : to map states (input data) to state-action pairs of Q values or to V values (output data) Input layer Output layer Hidden layer/s Current state Q-value action 1 Q-value action 2 Q-value action 3 Q-value action k Deep Q-network (DQN) Input layer Output layer Hidden layer/s Current state V-value Model Free approaches (Deep Q-learning ) Remember, the loss function measures the differences between the true value and the output value provided by the ANN. But in RL we don‚Äôt know the true values! What can we do? Let use as the approximate true value, the value known as the target: (as we just do in Q-learning) Then, the loss function in a DQN is: Model Free approaches (Deep Q-learning ) However, RL with DNN is shown to be unstable. Why? ‚óè There is a correlation between the Q-values and the target-values. ‚óè In a DNN the target is fixed and does not change, in QDN the target value changes with every iteration during training. How to turn the combination of RL with DNN stable? By using two techniques: ‚óè Experience play. It is a technique to stabilize the training of the Q-function by allowing the agent to ‚Äúreplay‚Äù again past experiences (from s i, a j was obtained r k, s t). These past experiences are stored in a replay memory (RM). During the process of training, the agent randomly samples a batch of experiences from the RM to update the Q-function. ‚óè Target network. It is a copy of the DQN, which is used to approximate the Q-function Model Free approaches (Deep Q-learning ) Current state Q(s,a; ùöπ) Q-Network Target Network ùöπ updates ùöπ- every C (hyperparameter) time steps QT(s,a; ùöπ- )Current state Model Free approaches (Deep Q-learning ) Therefore, in the Q-Network the internal weights of the neurons are modified by backpropagation the loss subject to the gradients of the activation function in each neuron: The Target-Network stabilizes the training process of the DQN because the most recent changes of the weights in the Q-Network are only applied to the Target-Network every C time steps. In the aula virtual you can get a Google Colab tutorial on how to control a cart-pole scenario using a DQN. Do, first!, the also available brief tutorial on pytorch , one of the most popular packages to help in the programming of DNN. Deep Q-learning: The Cart-pole example Each state of the cart-pole is composed of 4 data: (Cart-position [-4.8,4.8], Cart-Velocity [-inf, +inf], Pole-angle [-0.418, 0.418], Pole-angular-velocity [-inf, +inf]) The agent receives a reward of +1 every time step that the pole remains upright on the cart -> Goal is to maintain the pole upright as much time steps as possible. There are two actions: 0, push the cart to the left, or 1, push the cart to the right Deep Q-learning: Cart-pole pytorch code examples class DQN (nn.Module): def __init__(self, n_observations, n_actions, hidden_size): super(DQN, self).__init__() self.layer1 = nn.Linear(n_observations, hidden_size) self.layer2 = nn.Linear(hidden_size, hidden_size) self.layer3 = nn.Linear(hidden_size, n_actions) def forward(self, x): x = F.relu(self.layer1(x)) x = F.relu(self.layer2(x)) return self.layer3(x) policy_network = DQN(4, 2, 128).to(device) target_network = DQN(4 ,2, 128).to(device) # Updates the parameters of the target_network with the # parameters of the policy_network target_network.load_state_dict(policy_network.state_dict()) Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward')) class ReplayMemory (object): def __init__(self, capacity): self.memory = deque([], maxlen=capacity) def push(self, *args): self.memory.append(Transition(*args)) def sample(self, batch_size): return random.sample(self.memory, batch_size) def __len__(self): return len(self.memory) memory = ReplayMemory(10000) memory.push(state, action, next_state, reward) transition = memory.sample(128) Some lines of code using the above definition: Some lines of code using the above definitions: Deep Q-learning: The Cart-pole example Abstract view of the software architecture of the model with DQN Training loop 1. Choose random/policy action to perform. 2. Sample environment response. 3. Record memory update 4. Optimize NN. 5. Soft Target-Network update. Replay memory Policy-Net (Q-Network) Target-Network Update Choose action Optimize random batch Deep Q-learning: The Cart-pole example What is doing the optimizer? Take a batch sample of the RM. Compute Q(s t, a) with the Policy Network. Compute V(st+1) = max a Q(st+1, a) with the Target Network. Calculate expected Q-value: reward + ùõÑ V(St+1) Compute Huber Loss between state-action-values and expected state-action-values- The Huber Loss acts like a Mean Square Error (MSE) when the error is small and like the Mean Absolute Error when the error is large. This makes more robust to outliers (uncommon unexpected results) when the estimates of Q are very noisy. Also, the technique of gradient clipping is performed to optimize the Policy Network: If the parameters of the Policy Network exceed 100 in magnitude, they will be set to 100 (or -100 for negative numbers). Deep Q-learning: The Cart-pole example The main program in the Cart-pole solution provided in the aula virtual does the following: Set features of the environment and the values for the hyperparameters of the agent. Create Policy-Network and Target-Network. Set Optimizer. Set Memory for RM. for each episode: Start environment Set initial state. for not done: # max number of steps in a episode or pole falling down. Choose action Get experience (s, a, r, s‚Äô) Set done True or False Put experience in RM Move to the next state (s‚Äô) Optimize model. Replace ùöπ‚Äô in Target-Network with those ùöπ of Policy-Network.","libVersion":"0.3.2","langs":""}