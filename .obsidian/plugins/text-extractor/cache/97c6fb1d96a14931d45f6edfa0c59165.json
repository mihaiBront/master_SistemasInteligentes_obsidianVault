{"path":"_aula_virtual/SJK003/08-multiclassifiers.pdf","text":"Department of Computer Languages and Systems Multi-classifiers Terminologies â€¢ Lots of terms are used to refer to multi-classifiers: â€“ ensemble of classifiers â€“ combining classifiers â€“ decision committee â€“ multiple classifier system â€“ mixture of experts â€“ committee-based learning â€“ etc. Introduction: motivation â€¢ When you have to face a complex classification problem: â€“ which learning algorithm to use? â€“ which parameters to choose? â€“ how to use the training data? â€“ which vector space to map the data onto? What is the most discriminating representation? Introduction: motivation â€¢ Different models may appear while searching for a solution, but often none of them is better than the rest â€“ In this case, a reasonable choice is to keep them all and create a final system integrating the pieces â€“ The core idea behind this is to aggregate multiple models to obtain a combined model D that outperforms every single model Di in it â€“ Each single model Di is called base learner (classifier) or individual learner (classifier) Strategies to build a multi-classifier â€¢ Combination level: design different combiners â€¢ Classifier level: use different base classifiers â€¢ Data level: use different data subsets â€¢ Feature level: use different feature subsets Combiner D1 D2 DL x Data set ... x(1) x(2) x(L) Source: http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf Combination level: fusion vs. selection Fusion â€¢ each ensemble member is supposed to have knowledge of the whole feature space â€¢ some combiner such as the average and majority vote is applied to label the input object x Selection â€¢ each ensemble member is supposed to know well a part of the feature space and to be responsible for objects in this part â€¢ one member is chosen to label the input object x Combination level (ii): fusion vs. selection Fusion â€¢ competitive classifiers â€¢ ensemble approach â€¢ multiple topology Selection â€¢ cooperative classifiers â€¢ modular approach â€¢ hybrid topology Fusion: Majority vote Decision rule: to choose the class most voted by the base classifiers Three consensus patterns: â€¢ Unanimity (all agree) â€¢ Simple majority (50%+1) â€¢ Plurality (most votes) Fusion: Majority vote (ii) Let it be â€¢ ğ‘‘ğ‘–,1, â€¦ , ğ‘‘ğ‘–,ğ¶ T âˆˆ 0,1 ğ¶, ğ‘– = 1, â€¦ , ğ¿, where ğ‘‘ğ‘–,ğ‘— = 1 if ğ·ğ‘– labels x in class ğœ”ğ‘—, and 0 otherwise Then, the plurality vote rule will result in an ensemble decision for class ğœ”ğ‘˜ if à· ğ‘–=1 ğ¿ ğ‘‘ğ‘–,ğ‘˜ = max ğ‘—=1,â€¦,ğ‘ à· ğ‘–=1 ğ¿ ğ‘‘ğ‘–,ğ‘— This rule coincides with the simple majority rule if ğ¶ = 2 Fusion: Majority vote (iii) A thresholded plurality vote: we increase the set of classes with one more class ï·c+1, for objects for which the ensemble does not determine a class label with a sufficient confidence. Now, the decision is ğœ”ğ‘˜, if à· ğ‘–=1 ğ¿ ğ‘‘ğ‘–,ğ‘˜ â‰¥ ğ›¼ Â· ğ¿ ğœ”ğ‘+1, otherwise where 0 < ğ›¼ â‰¤ 1. If ğ›¼ = 1, this becomes the unanimity vote rule Fusion: Majority vote (iv) Weighted majority vote: â€“ an adequate option when the base classifiers are not of very similar accuracy â€“ it attempts to give the more competent classifiers more power in making the final decision Fusion: Majority vote (v) Weighted majority vote: â€“ we can represent the outputs as ğ‘‘ğ‘–,ğ‘— = á‰Š1 if ğ·ğ‘– labels x in ğœ”ğ‘— 0 otherwise â€“ then, the decision is ï·k if à· ğ‘–=1 ğ¿ ğ‘¤ğ‘–ğ‘‘ğ‘–,ğ‘˜ = max ğ‘—=1,â€¦,ğ‘ à· ğ‘–=1 ğ¿ ğ‘¤ğ‘–ğ‘‘ğ‘–,ğ‘— where ğ‘¤ğ‘– ï‚³ 0 (Ïƒğ‘–=1 ğ‘ ğ‘¤ğ‘– = 1) is a weight for classifier Di Selection Suppose an ensemble D = {D1, â€¦, DL} of classifiers already trained. Then, the feature space â„ğ‘‘ is divided into K > 1 selection regions (or regions of competence), which are denoted by R1, â€¦, RK â€“ usually, K = L â€“ each region Ri is associated with a classifier, which will be responsible for deciding on the input objects in this part of the space â€“ these regions are not associated with specific classes, nor do they need to be of a certain shape or size Selection (ii) Example: suppose a data set with 2000 points and two classes ï·1 and ï·2, and we have an ensemble with three classifiers D1, D2, D3, each one associated with regions R1, R2, R3 â€¢ D1 always predicts ï·1 â€¢ D2 always predicts ï·2 â€¢ D3 is a linear classifier whose discriminant function is shown as a dashed line â€¢ Accuracy of the individual classifiers or that of a majority vote (fusion) is approximately 0.5 â€¢ Accuracy of the selection combiner will be close to 1 Source: http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf R1 R2 R3 Classifier level: stacking Idea: â€¢ learn various different weak learners (base learners) â€¢ combine the base learners by training a meta-model Comments: â€¢ we need to define two things in order to build our stacking model: the L base learners we want to fit and the meta- model that combines them â€¢ for example, we can choose as weak learners a k-NN classifier, a decision tree and a SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it Classifier level: stacking (ii)Classifier level: stacking (iii) 1. Initialize the parameters ğ¿, the number of weak learners 2. Split the data into two folds 3. For ğ‘™ = 1, â€¦ , ğ¿ Train the weak learner to data of the first fold Make predictions for data in the second fold 4. Train the meta-model on the second fold, using predictions made by the weak learners as inputs Data level: bagging Idea: â€¢ the ensemble is made of classifiers built on bootstrap replicates of the training set Ttra = {x1, â€¦, xn} â€¢ the classifier outputs are combined by the plurality vote Comments: â€¢ we sample with replacement from the original Ttra to create L new training sets (often, also of size n) â€¢ all L base classifiers are the same classification model â€¢ the base classifier should be unstable (small changes in Ttra lead to large changes in the classifier output (neural networks and decision trees are unstable, k-NN is stable) â€¢ this is a parallel algorithm in both its training and operational phases Data level (ii): baggingData level (iii): bagging Training phase 1. Initialize the parameters ğ· = âˆ…, the ensemble ğ¿, the number of classifiers to train 2. For ğ‘™ = 1, â€¦ , ğ¿ Take a bootstrap sample ğ‘†ğ‘™ from the original training set Ttra Build a classifier ğ·ğ‘™ using ğ‘†ğ‘™ as the training set Add the classifier to the current ensemble, ğ· = ğ· âˆª ğ·ğ‘™ Return ğ· Classification (regression) phase 1. Run ğ·1 â‹¯ ğ·ğ¿ on the input x 2. Assign x to the class with the maximum number of votes (simple majority voting, for classification) Assign x with the average of the estimated values (simple average, for regression) Data level (iv): variants of bagging Random forest â€“ a collection of full decision trees built in parallel from random bootstrap sample of the data set â€“ the final prediction is an average of all of the decision tree predictions Data level (vi): boosting Idea: â€¢ to develop the ensemble D incrementally, adding one base classifier at a time â€¢ some classifiers have more say in the classification than others â€¢ the classifier Di is made by taking the errors of the classifier Di-1 into account Comments: â€¢ this is a sequential algorithm â€¢ the errors that the first classifier makes influence how the second classifier is made, and so on Data level (vii): boosting The idea of boosting could be seen as a golfer who initially hits a golf ball towards the hole at position y, but only goes as far as f0. The golfer then repeatedly hits the ball more gently, moving it toward the hole a little at a time and after reassessing the direction and distance to the hole with each shot. Data level (viii): boostingData level (ix): boostingData level (x): boosting (AdaBoost) Training phase 1. Initialize the parameters Set the weights ğ‘¤ğ‘– = Î¤1 ğ‘› (equal weights to each data point) ğ· = âˆ…, the ensemble ğ¿, the number of classifiers 2. For ğ‘™ = 1, â€¦ , ğ¿ Build a classifier ğ·ğ‘™ with the training data using ğ‘¤ğ‘– for ğ‘– = 1, â€¦ , ğ‘› Calculate the proportion of errors in classification ğ‘’ğ‘™ Compute ğ‘†ğ‘™ = log 1 âˆ’ ğ‘’ğ‘™ /ğ‘’ğ‘™ Update the weights ğ‘¤ğ‘– (weights of correctly classified samples do not change; incorrectly classified samples are given more weight by multiplying their previous weight by (1 âˆ’ ğ‘’ğ‘™)/ğ‘’ğ‘™ Data level (xi): boosting (AdaBoost) Classification phase Given a sample ğ±, if we denote à·ğ‘¦ğ‘™(ğ±) its classification using classifier ğ·ğ‘™, then à·œğ‘¦ ğ± = ğ‘ ğ‘–ğ‘”ğ‘› à·ğ‘™ğ‘†ğ‘™ à·ğ‘¦ğ‘™(ğ±) (if the sum is positive, the observation is classified as belonging to class +1, otherwise to class âˆ’1) Data level (xii): variants of boosting Gradient boosting â€“ it involves three elements: â€¢ a loss function to be optimized (e.g., regression may use mean squared error and classification may use logarithmic loss) â€¢ a weak learner to make predictions (usually, decision tress) â€¢ an additive model that minimizes the loss function when adding trees (gradient descent is used to minimize the loss) Data level (xiii): variants of boosting Extreme gradient boosting (XGBoost) â€“ an efficient and effective implementation of gradient boosting â€“ it is highly scalable and can handle large data sets â€“ trees are built in parallel, instead of sequentially like gradient boosting â€“ it implements early stopping so we can stop model evaluation when additional trees offer no improvement Data level (xiv): variants of boosting Categorical boosting (CatBoost) â€“ it is designed to work on heterogeneous data (categorical, numerical, logical, â€¦) â€“ it works well with less data â€“ improved accuracy by reducing overfitting Feature level: random subspace Idea: â€¢ the ensemble is made of classifiers built on random subsets of features (with replacement) of predefined size drs (drs < d) â€¢ the classifier outputs are combined by the plurality vote Comments: â€¢ an attractive choice for high-dimensional problems where the number of features (d) is much larger than the number of training points (n) â€¢ it works best when the discriminative information is â€œdispersedâ€ across all the features Feature level (ii): random subspace Training phase 1. Initialize the parameters D = ïƒ†, the ensemble L, the number of classifiers to train 2. For k = 1, â€¦, L Pick up drs features from d with replacement Build a classifier Dk using the subspace sample Add the classifier to the current ensemble, D = D ïƒˆ Dk 3. Return D Classification phase 1. Run D1, â€¦, DL on the input x 2. Assign x to the class with the maximum number of votes","libVersion":"0.3.2","langs":""}