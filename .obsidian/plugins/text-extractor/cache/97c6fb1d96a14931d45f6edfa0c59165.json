{"path":"_aula_virtual/SJK003/08-multiclassifiers.pdf","text":"Department of Computer Languages and Systems Multi-classifiers Terminologies • Lots of terms are used to refer to multi-classifiers: – ensemble of classifiers – combining classifiers – decision committee – multiple classifier system – mixture of experts – committee-based learning – etc. Introduction: motivation • When you have to face a complex classification problem: – which learning algorithm to use? – which parameters to choose? – how to use the training data? – which vector space to map the data onto? What is the most discriminating representation? Introduction: motivation • Different models may appear while searching for a solution, but often none of them is better than the rest – In this case, a reasonable choice is to keep them all and create a final system integrating the pieces – The core idea behind this is to aggregate multiple models to obtain a combined model D that outperforms every single model Di in it – Each single model Di is called base learner (classifier) or individual learner (classifier) Strategies to build a multi-classifier • Combination level: design different combiners • Classifier level: use different base classifiers • Data level: use different data subsets • Feature level: use different feature subsets Combiner D1 D2 DL x Data set ... x(1) x(2) x(L) Source: http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf Combination level: fusion vs. selection Fusion • each ensemble member is supposed to have knowledge of the whole feature space • some combiner such as the average and majority vote is applied to label the input object x Selection • each ensemble member is supposed to know well a part of the feature space and to be responsible for objects in this part • one member is chosen to label the input object x Combination level (ii): fusion vs. selection Fusion • competitive classifiers • ensemble approach • multiple topology Selection • cooperative classifiers • modular approach • hybrid topology Fusion: Majority vote Decision rule: to choose the class most voted by the base classifiers Three consensus patterns: • Unanimity (all agree) • Simple majority (50%+1) • Plurality (most votes) Fusion: Majority vote (ii) Let it be • 𝑑𝑖,1, … , 𝑑𝑖,𝐶 T ∈ 0,1 𝐶, 𝑖 = 1, … , 𝐿, where 𝑑𝑖,𝑗 = 1 if 𝐷𝑖 labels x in class 𝜔𝑗, and 0 otherwise Then, the plurality vote rule will result in an ensemble decision for class 𝜔𝑘 if ෍ 𝑖=1 𝐿 𝑑𝑖,𝑘 = max 𝑗=1,…,𝑐 ෍ 𝑖=1 𝐿 𝑑𝑖,𝑗 This rule coincides with the simple majority rule if 𝐶 = 2 Fusion: Majority vote (iii) A thresholded plurality vote: we increase the set of classes with one more class c+1, for objects for which the ensemble does not determine a class label with a sufficient confidence. Now, the decision is 𝜔𝑘, if ෍ 𝑖=1 𝐿 𝑑𝑖,𝑘 ≥ 𝛼 · 𝐿 𝜔𝑐+1, otherwise where 0 < 𝛼 ≤ 1. If 𝛼 = 1, this becomes the unanimity vote rule Fusion: Majority vote (iv) Weighted majority vote: – an adequate option when the base classifiers are not of very similar accuracy – it attempts to give the more competent classifiers more power in making the final decision Fusion: Majority vote (v) Weighted majority vote: – we can represent the outputs as 𝑑𝑖,𝑗 = ቊ1 if 𝐷𝑖 labels x in 𝜔𝑗 0 otherwise – then, the decision is k if ෍ 𝑖=1 𝐿 𝑤𝑖𝑑𝑖,𝑘 = max 𝑗=1,…,𝑐 ෍ 𝑖=1 𝐿 𝑤𝑖𝑑𝑖,𝑗 where 𝑤𝑖  0 (σ𝑖=1 𝑐 𝑤𝑖 = 1) is a weight for classifier Di Selection Suppose an ensemble D = {D1, …, DL} of classifiers already trained. Then, the feature space ℝ𝑑 is divided into K > 1 selection regions (or regions of competence), which are denoted by R1, …, RK – usually, K = L – each region Ri is associated with a classifier, which will be responsible for deciding on the input objects in this part of the space – these regions are not associated with specific classes, nor do they need to be of a certain shape or size Selection (ii) Example: suppose a data set with 2000 points and two classes 1 and 2, and we have an ensemble with three classifiers D1, D2, D3, each one associated with regions R1, R2, R3 • D1 always predicts 1 • D2 always predicts 2 • D3 is a linear classifier whose discriminant function is shown as a dashed line • Accuracy of the individual classifiers or that of a majority vote (fusion) is approximately 0.5 • Accuracy of the selection combiner will be close to 1 Source: http://www.ccas.ru/voron/download/books/machlearn/kuncheva04combining.pdf R1 R2 R3 Classifier level: stacking Idea: • learn various different weak learners (base learners) • combine the base learners by training a meta-model Comments: • we need to define two things in order to build our stacking model: the L base learners we want to fit and the meta- model that combines them • for example, we can choose as weak learners a k-NN classifier, a decision tree and a SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it Classifier level: stacking (ii)Classifier level: stacking (iii) 1. Initialize the parameters 𝐿, the number of weak learners 2. Split the data into two folds 3. For 𝑙 = 1, … , 𝐿 Train the weak learner to data of the first fold Make predictions for data in the second fold 4. Train the meta-model on the second fold, using predictions made by the weak learners as inputs Data level: bagging Idea: • the ensemble is made of classifiers built on bootstrap replicates of the training set Ttra = {x1, …, xn} • the classifier outputs are combined by the plurality vote Comments: • we sample with replacement from the original Ttra to create L new training sets (often, also of size n) • all L base classifiers are the same classification model • the base classifier should be unstable (small changes in Ttra lead to large changes in the classifier output (neural networks and decision trees are unstable, k-NN is stable) • this is a parallel algorithm in both its training and operational phases Data level (ii): baggingData level (iii): bagging Training phase 1. Initialize the parameters 𝐷 = ∅, the ensemble 𝐿, the number of classifiers to train 2. For 𝑙 = 1, … , 𝐿 Take a bootstrap sample 𝑆𝑙 from the original training set Ttra Build a classifier 𝐷𝑙 using 𝑆𝑙 as the training set Add the classifier to the current ensemble, 𝐷 = 𝐷 ∪ 𝐷𝑙 Return 𝐷 Classification (regression) phase 1. Run 𝐷1 ⋯ 𝐷𝐿 on the input x 2. Assign x to the class with the maximum number of votes (simple majority voting, for classification) Assign x with the average of the estimated values (simple average, for regression) Data level (iv): variants of bagging Random forest – a collection of full decision trees built in parallel from random bootstrap sample of the data set – the final prediction is an average of all of the decision tree predictions Data level (vi): boosting Idea: • to develop the ensemble D incrementally, adding one base classifier at a time • some classifiers have more say in the classification than others • the classifier Di is made by taking the errors of the classifier Di-1 into account Comments: • this is a sequential algorithm • the errors that the first classifier makes influence how the second classifier is made, and so on Data level (vii): boosting The idea of boosting could be seen as a golfer who initially hits a golf ball towards the hole at position y, but only goes as far as f0. The golfer then repeatedly hits the ball more gently, moving it toward the hole a little at a time and after reassessing the direction and distance to the hole with each shot. Data level (viii): boostingData level (ix): boostingData level (x): boosting (AdaBoost) Training phase 1. Initialize the parameters Set the weights 𝑤𝑖 = Τ1 𝑛 (equal weights to each data point) 𝐷 = ∅, the ensemble 𝐿, the number of classifiers 2. For 𝑙 = 1, … , 𝐿 Build a classifier 𝐷𝑙 with the training data using 𝑤𝑖 for 𝑖 = 1, … , 𝑛 Calculate the proportion of errors in classification 𝑒𝑙 Compute 𝑆𝑙 = log 1 − 𝑒𝑙 /𝑒𝑙 Update the weights 𝑤𝑖 (weights of correctly classified samples do not change; incorrectly classified samples are given more weight by multiplying their previous weight by (1 − 𝑒𝑙)/𝑒𝑙 Data level (xi): boosting (AdaBoost) Classification phase Given a sample 𝐱, if we denote ෝ𝑦𝑙(𝐱) its classification using classifier 𝐷𝑙, then ො𝑦 𝐱 = 𝑠𝑖𝑔𝑛 ෍𝑙𝑆𝑙 ෝ𝑦𝑙(𝐱) (if the sum is positive, the observation is classified as belonging to class +1, otherwise to class −1) Data level (xii): variants of boosting Gradient boosting – it involves three elements: • a loss function to be optimized (e.g., regression may use mean squared error and classification may use logarithmic loss) • a weak learner to make predictions (usually, decision tress) • an additive model that minimizes the loss function when adding trees (gradient descent is used to minimize the loss) Data level (xiii): variants of boosting Extreme gradient boosting (XGBoost) – an efficient and effective implementation of gradient boosting – it is highly scalable and can handle large data sets – trees are built in parallel, instead of sequentially like gradient boosting – it implements early stopping so we can stop model evaluation when additional trees offer no improvement Data level (xiv): variants of boosting Categorical boosting (CatBoost) – it is designed to work on heterogeneous data (categorical, numerical, logical, …) – it works well with less data – improved accuracy by reducing overfitting Feature level: random subspace Idea: • the ensemble is made of classifiers built on random subsets of features (with replacement) of predefined size drs (drs < d) • the classifier outputs are combined by the plurality vote Comments: • an attractive choice for high-dimensional problems where the number of features (d) is much larger than the number of training points (n) • it works best when the discriminative information is “dispersed” across all the features Feature level (ii): random subspace Training phase 1. Initialize the parameters D = , the ensemble L, the number of classifiers to train 2. For k = 1, …, L Pick up drs features from d with replacement Build a classifier Dk using the subspace sample Add the classifier to the current ensemble, D = D  Dk 3. Return D Classification phase 1. Run D1, …, DL on the input x 2. Assign x to the class with the maximum number of votes","libVersion":"0.3.2","langs":""}