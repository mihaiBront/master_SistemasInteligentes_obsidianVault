{"path":"_aula_virtual/SJK001/Reading Assessments/[Billard17] On the mechanical, cognitive and sociable facets of human compliance and their robotic counterparts RAS.pdf","text":"Robotics and Autonomous Systems 88 (2017) 157–164 Contents lists available at ScienceDirect Robotics and Autonomous Systems journal homepage: www.elsevier.com/locate/robot On the mechanical, cognitive and sociable facets of human compliance and their robotic counterparts Aude Billard Learning Algorithms and Systems Laboratory (LASA), Institute of MicroEngineering, Institute of Mechanical Engineering, School of Engineering, Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland h i g h l i g h t s • Distinction across mechanical, cognitive and social compliance. • Emphasize the importance to endow robots with all three levels. • Overview approaches to learning the compliance. • Provide pointers with recent surveys in related fields. a r t i c l e i n f o Article history: Received 20 February 2015 Received in revised form 29 September 2015 Accepted 31 August 2016 Available online 14 November 2016 Keywords: Compliance Social robotics Human–robot interaction a b s t r a c t Compliance has become a key requirement for robots meant to interact with humans. It is viewed as a necessary property to increase safety and efficiency in human–robot cooperative actions. In humans, compliance takes three dimensions: mechanical, cognitive and social. While robotics has focused primarily on modeling the first two, we here discuss the importance to consider also the social dimension that compliance takes in human–human interactions and how this can be extended to human–robot interactions. We discuss situations in which requesting that the human complies to the machine may be advantageous, and not the converse. We conclude with a list of open ethical and legal issues that may arise from developing actively non-compliant machines. © 2016 Elsevier B.V. All rights reserved. 1. Introduction This special issue follows a recent trend in human–robot in- teraction (HRI) that advocates the need to provide robots with compliant motion, so as to ease the interaction [1–3]. This paper follows suit and provides a global overview of the main roles that compliance can take in Human–Robot Interaction. Note of caution: It is not intended as a comprehensive review of all approaches in robotics taken to model compliance, since several comprehensive reviews of the role of compliance in robotics and in human–robot interaction have been published of late. Instead, this paper offers pointers to these works, and emphasize the way by which com- pliance can be taught to the robot, an aspect not covered in these surveys. We first revisit the implications that endowing a robot with compliant behavior entails in HRI. We start by delineating the mechanical, cognitive and social aspects which compliance takes in human–human interactions. We then review briefly which of these aspects have been exploited in robotics to date. We then E-mail address: aude.billard@epfl.ch. discuss the issue of determining the right amount of compliance and how the robot can learn this either through exploration or through human guidance. We briefly review the techniques used in each approach, emphasizing the importance of the interface, and the evolution of these techniques since the early works offered in the 80’s. We then turn to bi-directional compliant behaviors in the context of collaborative human–robot interaction and conclude by discussing the advantage to request at times that the human complies with the robot’s actions and not the converse. 1.1. The etymology of compliance In robotics, compliance relates to impedance control, that is, to the extent to which the robot motion absorbs or resists external forces [4,5]. The concept of compliance in robotics is strongly inspired from, and is akin to, the natural mechanical compliance of the human body. However, the term compliance in human–human interactions takes a broader meaning and goes beyond low-level control of mechanical impedance, which we find traditionally in robotics. Let us first recall the root of the word. Compliance, i.e. the act to comply, finds its etymology in the latin verb ‘‘complire’’, which means ‘‘to accomplish, fulfill, carry http://dx.doi.org/10.1016/j.robot.2016.08.030 0921-8890/© 2016 Elsevier B.V. All rights reserved. 158 A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 Fig. 1. Illustrations of situations in which a robot complies or not to the human’s request. Left: The robot complies to the little girl’s request to start running. Right: The robot does not comply to the girl’s request and forces her to comply to its request to stay put (perhaps to prevent her from crossing a dangerous street). The compliance/non- compliance is conveyed by haptic signals: robot and girl push/pull each other arm’s to indicate the direction to follow. Additionally, facial expressions and body posture are other means in which the two partners express their collaborative or non-collaborative intent. out’’. While the Latin origin referred to the act of accomplishing a task, once translated in French, Spanish and Italian, in the middle age, the word took a more reverential meaning whereby one would comply so as ‘‘to be agreeable, to oblige or to obey’’. In today’s English, to comply relates generically to all situations whereby one individual acquiesces to a command. The command may be emit- ted by another individual or by a group of individuals. Typically, one may comply to a request formulated by one’s supervisor. One also complies to rules set forth by the society, whether these are written rules (bylaws) or implicit social rules. ‘‘To comply’’, hence, requires high-level cognitive skills to interpret and to respond to these various types of requests. It is deeply grounded in an understanding of social rules and how these apply to the current context. 1.2. The communication channels of compliance Compliance is particularly visible in cooperative interactions between humans. In these cases, compliance is regulated through dominance and role distribution. Usually, one agent leads while the other complies. This is particularly the case when there is some form of explicit subordination. Parent–child interactions follow typically a leading-compliant nature. Among adults similar forms of subordination is common in a teacher–learner context, whereby the more knowledgeable agent (the trainer) takes the lead and the trainee complies with the teachers’ guidance. The channels by which a command is conveyed to the compli- ant agent are multiple: direct commands can be uttered verbally or through written rules. Indirect commands may be conveyed through physical touch, gently pulling the person in the direction she should follow. The channel by which the compliant person responds is often the same as that used to generate the command. For instance, when pulled into one direction, one may pull back, hence indicating lack of compliance, or conversely move in the direction indicated by the external push, see Fig. 1. While verbal commands are often the mean by which compliant behavior is requested, compliance can also be achieved and re- quested through non-verbal cues. For instance, the parent holding the child’s hand indicates through her body motion or increased pressure when to turn or when to stop walking. This non-verbal form of compliance appears in a variety of human interaction that requires physical interaction, namely physical contact between the two partners. This contact may be direct, such as when holding the other person (e.g couple dancing), or mediated through an object (two people carrying a piece of furniture). Non-verbal expression of compliance and active regulation of compliance in these physical tasks is particularly relevant to robotics, as it bridges compliance to social rules and the traditional robotics approach to active control of mechanical compliance. A complete review of these works is done in [6]. We will come back to these examples when we discuss the need for the human to comply to the robot’s requests. 2. The different facets of compliance Compliance in human–human interaction can be declined along three main dimensions: mechanical, cognitive and social. We re- view each of these briefly below and summarize the way these concepts have been exploited in robotics. 2.1. Mechanical compliance The primary cause of compliance in human motion is mechani- cal. Thanks to the elasticity of human muscles, human movements show a natural stiffness and damping, the latter being particularly useful to absorb some of the elastic energy generated when the limbs come into contact with external objects. This mechanical compliance is advantageous as it offers greater robustness in all tasks that require physical contact with the environment. For in- stance, when walking on uneven terrain, our ankles absorb part of the torques generated through contact with the ground and help to re-establish balance [8,9]. The mechanical compliance displayed by human muscles can be changed through active control. However, active control of mechanical compliance is not immediate and requires minimum 100ms to become effective. This is the time required for the central nervous system to react in response to sensory feedback. If the person anticipates an impact, the muscles can be pre-loaded and the natural compliance may be shaped in anticipation of the force needed to resist the impact [10]. However, this is not always possible. Hence, mechanical compliance is key to our ability to respond to unexpected and short impacts (see Fig. 2). It is important to distinguish cognitive compliance from me- chanical compliance, when it comes to controlling compliance in robots. The mechanical compliance, also known as passive compli- ance, determines the amount of compliance the actuator or limb can display. It results from mechanical properties of the materials used that determine the range of stiffness and damping values that the limb can adopt. The cognitive compliance refers to the mecha- nisms/algorithms by which the mechanical compliance is varied. In humans, cognitive control of mechanical compliance may be conscious or unconscious. The distinction between mechanical and cognitive compliance is less straightforward, as variation of com- pliance of human limbs may be the results of a variety of physio- logical factors. The central nervous system exploits the mechanical compliance of our body to its advantage, e.g. by changing body posture to orient the resistance to external impact. However, the process by which this variation is performed may be unconscious, and, as mentioned before, may result from a variety of physiologi- cal factors. The distinction applies, hence, primarily to the robotic counterpart. key to our ability to respond to unexpected and short lasting impacts. Active control of the mechanical compliance falls A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 159 Fig. 2. Measurement of mechanical compliance of human arm is required to support design of assistive robotic support during welding (left), [7]. Fig. 3. Illustration of mechanical compliance conveyed by choice of flexible material for the robot’s body (right). under the label of cognitive compliance, which we cover in the next section. In robotics, estimating human mechanical compliance is impor- tant for the control of robotic interfaces in direct physical contact with humans. For instance, the natural damping provided by hu- man muscles can be used to dissipate part of the energy generated by haptic interfaces and increase the stability of the interface [11]. As human natural compliance varies a function of body posture, muscle fatigue and other physiological factors [12], it has appeared crucial to measure the compliance of the limb in contact with the robot and take this into account for control. 1 Models of human natural impedance have, also, been instru- mental to guide the use of new materials for building robots that mimic human mechanical compliance. This has led to the devel- opment of elastic and variable impedance actuators, see recent surveys [13–16]. As with human muscles, these actuators come 1 Identifying the natural compliance of human limbs amounts to estimating the parameters of an impedance model so as to reproduce the limb transient response to impact (within the 20–80 ms after impact). with a limited range of possible impedance values. The impedance of the actuator can, in some cases, be pre-loaded for the task at hand or, in the case of variable impedance actuator, it can be varied according to some external laws. Several applications of these actuators have demonstrated their effectiveness to improve safety during voluntary and involuntary physical contact of the robot with humans [17,18]. In a similar vain, robotics has looked into ways in which it can exploit the use of passive elements (elastic or viscous material) to provide the robot with a form of mechanical compliance that resembles that offered by biological tissues. These elements may be used in conjunction or not with the use of stiff actuators [19], see Fig. 3. One issue arises, however, from the fact that the actuators offer primarily passive compliance. In other words, the actuator’s compliance cannot always be actively controlled and the default compliance of the actuator may not be appropriate for all tasks. For this reason, actuators whose stiffness can be designed and varied on-line are particularly desirable [14,16]. It remains to determine how to change these parameters. This can be done through the 160 A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 design of algorithms to which we refer as cognitive compliance approaches and which we cover next. 2.2. Cognitive compliance A large part of our daily actions require to control actively the compliance we apply when we come into contact with our environment. For instance, when manipulating objects in inser- tion tasks, humans actively control and change the stiffness they produce to compensate for changes in the resistive force [20]. Similarly, humans may increase the natural damping of their legs during complex locomotion tasks, such as when jumping, by actively bending the knees upon contact with the ground. Im- portantly, when performing joint collaborative tasks with other humans, humans vary actively the stiffness of their muscles in response to change in load or to better support a weak partner [21,22]. By actively modeling how the stiffness or the damping must change as an effect of task requirement, one can produce com- pliant control, even when using inherently stiff actuators. A set of recent works have shown how these concepts can be successfully implemented to improve performance and safety in human–robot interaction, e.g. by increasing completion time in a joint insertion task [23,24], in enabling smooth and safe couple dancing [25] to prevent a humanoid robot to fall when entering in contact with the human [26] and when transporting jointly an object [27]. Control of compliance is not necessarily a conscious process. Unconscious control of compliance arises from experience and practice. For instance, while we may actively control the compli- ance in our fingers, when we insert a key into a lock for the first time, we no longer think about it after years of using the lock. Providing robots with the ability to acquire models that control variations of compliance in response to perceived contact forces is highly desirable. Later in this paper, we review briefly current works in this area. The impedance characteristics of these compliant materials are not necessarily fixed and may be changed actively to adapt the mechanical compliance to the task. For instance, one may adapt the stiffness of the robot’s legs or wings depending on the hardness of the ground or the viscosity of the fluid in which the robot navigates [28,29], similarly to its biological counterpart in which the stiffness of animal muscles may be varied through precontraction in antici- pation to the amount of interaction forces that will be encountered at impact. 2.3. Social compliance Social compliance relates to a vast area of research, spanning from psychology to economics, which describes the mechanisms by which humans comply to rules dictated by society. Compliance may as well be explicitly requested or may stem from implicit co- operation. Since the 90’s and predominantly in more recent years, different works have investigated the usage of physiological signals (such as facial electromyography, electrodermal activity, cardiac activity and respiration) to provide quantitative measurement of compliance induced when groups of two or more individuals are engaged in a cooperative task [30–32]. Evidence of such physiolog- ical responsiveness is termed social psychophysiological compliance (SPC) [31]. SPC is crucial in human–human interactions and leads to the development of other social relationships, such as bond- ing, feelings of solidarity and of belonging. Similar psychophysical evidence of cooperativeness and bonding has been demonstrated across species, and in particular when humans interact with bio- logical pets [33], as well as with robotic pets [34]. Conflicts, which arise from team members having unaligned goals, can also be measured through the same physiological signals. One then detects an asynchrony of the physiological signals across the team members. Lack of SPC in human–robot interaction is also revealed through physiological signals [35] and, if not taken into account, can be disruptive for the task completion [36]. For this reason, a growing body of works in robotics have investigated means by which physiological signals can be used to monitor human responses in cooperative tasks and have used the latter to drive changes in the robot’s actions. The robot may for instance stop in its task and wait until the physiological signals indicate that the human has regained some of the required trust before resuming the joint work. A survey of these different works can be found in [37]. 3. Acquisition and modulation of compliance As we have seen in the previous section, promoting compliance at the mechanical, cognitive and social levels is desirable to ensure more efficient and safer human–robot collaboration. The form that compliance must take is highly context depen- dent and requires an understanding of the situation. Some of the forms of compliance we have reviewed previously may be explic- itly programmed in a robot by designing a set of rules. For instance, when detecting signs of lack of SPC through physiological signals, the robot may apply a specific response depending on the type of changes observed in physiological signals. If the physiological signals indicate increased levels of anxiety and stress that cannot be correlated with higher demands from the task itself, the robot may backtrack until the human is more at ease [38,39]. However, designing such rules may be difficult, especially when these rules are implicit and humans may not be consciously aware of them. This is particularly the case, when compliance should emerge from an interpretation of haptic signals during direct phys- ical human–robot interaction. Traditionally, people have taken the approach to make the robot passive, following human guidance through admittance control [23,40]. Parameters of the controllers were fine tuned by hand to the robot and task at hand, e.g. by modeling human arm motion and using this model to produce similar dynamics in the robot’s arm motion [41]. The classical passive (purely compliant) role for the robot was challenged recently in [42,43]. In place of always following human guidance, the robot could, from time to time, switch role with the human and take the lead, requesting the human to ‘‘comply’’ in turn. Since then, a number of other works have investigated the use of such explicit role switching between robots and humans [44–46]. The exchange of roles was prescribed based on a prelimi- nary analysis of similar tasks performed by human dyads. To avoid such explicit modeling of the parameters and rules guiding the robot’s compliance, another body of work has started looking at ways in which the compliance may be taught to the robot during task completion. We review these works next. 3.1. Teaching robots to become compliant The idea of teaching robots to produce the right amount of compliance is not new and can be traced back to the 80’s [47–50]. In these early approaches, teaching was provided by the human pas- sively guiding the robot through the steps of the task and a model of the non-linear relationships between force and displacement was estimated. These works were similar to many approaches we find these days in that they used tele-operation or kinesthetic teaching for guiding the robot through the task. However, these works were limited to off-line training and considered compliance toward static objects. Yet, these were pioneer works that set the stage of the work we find these days. To learn compliant interaction when interacting with a human is more challenging than when interacting with objects, as the hu- man is versatile in his/her behavior. Most importantly, the human A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 161 Fig. 4. Interfaces to teach a robot to become more compliant: The teacher interacts with the robot as it is moving to alter its stiffness. To decrease the stiffness the teacher wiggles the robot around its current position (left figure). To increase the stiffness, the teacher increases the grip force with which he holds the robot (right figure). The robot responds online to these stimuli, so that the teacher gets immediate haptic feedback on the new stiffness. continuously changes his/her impedance when performing tasks. Learning how to interact with humans requires to build a model of the required compliance to adapt to the human [41,51], in addition to learning the compliance required for task completion [52]. Current approaches explore the best ways in which compliance can be taught. The interface, i.e. the means by which the human can train the robot, is key to the teaching. While early works used force/torque sensors at the end-point, new works make use of other means to detect the interaction forces (in addition to force/torque sensors) such as tactile sensors placed on the fingers of robots [53] or on the robot’s body [54], haptic device [55], and using physical signals through electromyography [56]. Fig. 4 shows two ways in which a user can teach a robot to vary its compliance through user-friendly/kinesthetic modes of interaction. Since the compliance may vary during the task, either in re- sponse to new task demands or because the human with which the robot interact changes her impedance, current works advocate the need to be able to model this change of impedance. This may be done through learning from human demonstration, by mod- eling the impedance parameters that would explain the human motion [57,58]. Other approaches exploit reinforcement learning to determine the optimal variation of compliance in the task [59]. This, however, requires to pre-set a reward function. Alternatives determine the reward function and the impedance parameters in one go through inverse optimal control [60,61]. Fig. 5 shows an example of how a user can teach a robotic hand to become compliant in a restricted part of its workspace only. The user moves passively the fingers of the robotic hand while it holds a can full of liquid. By moving passively the fingers of the robotic hand, the user shows to the robot all finger postures that ensure that the can remains sufficiently vertical for the liquid to not spill over. The robot stores a statistical representation of all feasible finger postures. When subjected to external perturbations, the robot can use this representation to decide whether it complies with the external perturbations or not. The robot will comply as long as the external perturbation does not send it away from feasible postures. When the perturbation is too strong and would send it away from the feasible space of postures, the robot resists, showing no compliance, and stay put in the current finger posture. 4. When and how to comply? While there is a clear consensus that compliance is needed to ensure efficient and safe human–robot interaction, there remains many open issues, which one could summarize through two ques- tions (see Fig. 4): 1. When to comply?: This requires the robot to recognize changes in task demand and adapt accordingly. This affects all three levels of compliance. At the mechanical level, this may take the form of a low-level control that, e.g., stiffens up to prevent slippage or increase damping to avoid mechanical damage in the face of a strong impact. When performing tasks that involve physical contact with humans, this re- quires cognitive skills to model changes in the human’s impedance. Finally, as robot will share more and more hu- man inhabited environment, the robot will need to comply to social rules, e.g. speaking when its turn comes when dialoguing with humans [62], keeping a socially acceptable distance when approaching humans [63], etc. 2. How to comply? This relates to the issues of determining the right parameters for the compliance (usually the impedance controller). At the lowest level, this may be driven by an analysis of the variations in the interaction forces and the need to ensure stability. Stiffness may hence be decreased or damping increased to stabilize the interaction. At a more cognitive level, this implies to build a model of the task’s de- mands, or in the case of physical human–robot interaction, to determine on-line the human’s impedance. As reviewed previously, a number of works have offered so- lutions to solve these questions. We, however, lack yet generic frameworks to determine when and how to comply in a variety of tasks, and in particular, when these tasks require to interact physically with humans. We close this paper by discussing the issue of whether the robot should sometimes not comply to human request, but rather take the lead and expect the human to comply in turn. 5. To comply or not to comply? We have started our paper by reminding the reader of the root of the word compliance and the various forms in which humans comply when interacting with one another. Robotics has for many years followed an approach in which the robot was to comply uni- laterally to humans’ will. It was a unidirectional process, whereby the robot acted as a slave. While it is agreed that the robot may show autonomy, the robot must obey human dictate. In an earlier paper, we had already disputed this idea and advocated the importance to enable robots to become companions, and not mere slave [64], and to show some level of leadership in the interaction, when required. A practical implementation of this idea was offered in the context of a task, where a human and a robot transport collaboratively a table. As they maneuver their way through a door, robot and human may exchange role, with the robot taking the lead when its turn comes to walk in front [43]. The three laws of robotics by science fiction author Isaac Azimov [65] 2 are an entertaining example of similar concepts. In the second 2 Azimov’s 3 laws are: 1) A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2) A robot must obey the orders 162 A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 e Fig. 5. Teaching how to comply: (Top) The demonstrator shows to the robot a range of possible finger postures that are all stable grasps. (Middle). The robot learns a probabilistic representation of feasible finger postures and tactile response at fingertip. (Bottom): The robot uses the learned distribution of finger postures to adapt its grasp when the tactile response changes. The robot complies to an external perturbation (e.g. someone pushing on the object, as long as the perturbation does not move it away from the feasible region. When pushed outside the region of feasible postures, the robot remains stiff and does not comply to the perturbation [53]. law, the robot is explicitly said that it must obey the human, although some freedom for disobedience is given if the second law conflicts with the first law, namely if the robot judges that there is an advantage to the human to disobey him. With the recent development of a variety of robots to support and assist humans, we have slowly moved away from the passive role given to the robot and have started seeing benefits in giving a pro-active role to the robot, letting it lead and decide in our place. For instance, the next generation of cars may take the lead to reduce the speed so as to force the driver to comply to speed limit given it by human beings, except where such orders would conflict with the First Law. 3) A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. set by traffic laws. Partial assistance is provided by prostheses to help the disabled regain part of his mobility, providing forces and support to the limb in motion. While such partial or complete leadership is likely beneficial to human mankind at large, it comes with the risk that the robot makes a mistake and harms humans [66]. For instance, if the prosthesis provides too strong a force in the wrong direction, misinterpreting the human’s intention of motion, it may harm severely the limb it supports. Ethical and legal issues unfold from this. As we reviewed in [67,68], there is yet a large void in current laws to determine to whom, of the robot’s designer or of the robot’s user, the responsibility for the harm would fall to. In addition to these issues, other more philosophical questions arise from giving leadership to the robot. Compliance assigns roles A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 163 and regulates dominance in the interaction. Are humans ready to accept that robots may take such a proactive role? How will humans perceive the intervention of the robot in the two examples we gave before? Will they accept and comply to it? In the case of the prosthesis wearers, we can expect that they will comply, as they perceive direct benefit from it. In the case of the careless drivers, chances are high the drivers will resent the car’s interven- tion. With all these issues in mind, we close this paper and look forward to more venues such as the one that led to the special issue that hosts this paper, to bring interdisciplinary views on this topic. Acknowledgments We thank support from the European Community’s Horizon 2020 robotics program ICT-23-2014 under grant agreement no. 644727 (COGIMON) and the Secreteriat d’Etat a la Formation a la Recherche et a l’Innovation (SEFRI) under grant no. 1131-52302. References [1] A. De Santis, B. Siciliano, A. De Luca, A. Bicchi, An atlas of physical human–robot interaction, Mech. Mach. Theory 43 (3) (2008) 253–270. [2] B.D. Argall, A.G. Billard, A survey of tactile human–robot interactions, Robot. Auton. Syst. 58 (10) (2010) 1159–1176. [3] D. Feth, R. Groten, A. Peer, M. Buss, Haptic human–robot collaboration: Com- parison of robot partner implementations in terms of human-likeness and task performance, Presence: Teleoperators and Virtual Environments 20 (2) (2011) 173–189. [4] N. Hogan, Impedance control: An approach to manipulation, J. Dyn. Syst. Meas. Control 107/17 (1985) 304–313. [5] L. Villani, J. De Schutter, Force Control, in: Springer Handbook of Robotics, Springer, 2008, pp. 161–185. [6] S.G. Khan, G. Herrmann, M. Al Grafi, T. Pipe, C. Melhuish, Compliance control and human–robot interaction: Part 1—survey, Int. J. Humanoid Robot. 11 (03) (2014) 1430001. http://dx.doi.org/10.1142/S0219843614300013. [7] Hand impedance measurements during interactive manual welding with a robot. [8] G. Lichtwark, A. Wilson, Is achilles tendon compliance optimised for maximum muscle efficiency during locomotion? J. Biomech. 40 (8) (2007) 1768–1775. [9] R. Versluys, P. Beyl, M.V. Damme, A. Desomer, R.V. Ham, D. Lefeber, Prosthetic feet: State-of-the-art review and the importance of mimicking human ankle- foot biomechanics, Disability Rehabil. Assist. Technol. 4 (2) (2009) 65–75. [10] I. Stokes, M. Gardner-Morse, S. Henry, G. Badger, Decrease in trunk muscular response to perturbation with preactivation of lumbar spinal musculature, Spine (2000) 1957–1964. [11] H.S. Woo, D.Y. Lee, Exploitation of the impedance and characteristics of the human arm in the design of haptic interfaces, IEEE Trans. Ind. Electron. 58 (8) (2011) 3221–3233. [12] T. Tsuji, P.G. Morasso, K. Goto, K. Ito, Human hand impedance characteristics during maintained posture, Biol. Cybern. 72 (6) (1995) 475–485. [13] R. Ham, T. Sugar, B. Vanderborght, K. Hollander, D. Lefeber, Compliant actuator designs, IEEE Robot. Autom. Mag. 16 (3) (2009) 81–94. [14] C. Neves, R. Ventura, Survey of semi-passive locomotion methodologies for humanoid robots, in: 15th International Conference on Climbing and Walking Robots and the Support Tech-nologies for Mobile Mechanics, World Scientific Publishing Company, Singapore, 2012, pp. 393–400. [15] A.L. Visan, N. Alexandrescu, A survey on the evolution of nonconventional pneumatic actuators, in: Advanced Materials Research, Vol. 463, Trans Tech Publ, 2012, pp. 1069–1072. [16] B. Vanderborght, A. Albu-Schäffer, A. Bicchi, E. Burdet, D.G. Caldwell, R. Carloni, M. Catalano, O. Eiberger, W. Friedl, G. Ganesh, et al., Variable impedance actuators: A review, Robot. Auton. Syst. 61 (12) (2013) 1601–1614. [17] K. Junius, P. Cherelle, B. Brackx, J. Geeroms, T. Schepers, B. Vanderborght, D. Lefeber, On the use of adaptable compliant actuators in prosthetics, rehabil- itation and assistive robotics, in: Robot Motion and Control (RoMoCo), 2013 9th Workshop on, IEEE, 2013, pp. 1–6. [18] G. Tonietti, R. Schiavi, A. Bicchi, Design and control of a variable stiffness actuator for safe and fast physical human/robot interaction, in: Robotics and Automation, 2005. ICRA 2005, in: Proceedings of the 2005 IEEE International Conference on, IEEE, 2005, pp. 526–531. [19] S Kawamura, T. Yamamoto, D. Ishida, T. Ogata, Y. Nakayama, O. Tabata, S. Sugiyama, Development of passive elements with variable mechanical impedance for wearable robots, in: Robotics and Automation, 2002. Pro- ceedings. ICRA’02, in: IEEE International Conference on, Vol. 1, IEEE, 2002, pp. 248–253. [20] N. Hogan, Adaptive control of mechanical impedance by coactivation of antag- onist muscles, IEEE Trans. Autom. Control 29 (8) (1984) 681–690. [21] K.e.a. Reed, Haptically linked dyads: Are two motor-control systems better than one? Psychol. Sci. 17 (5) (2006) 365–366. [22] V. der Wel, et al., Let the force be with us: Dyads exploit haptic coupling, J. Exp. Psychol. Human Perception and Performance 37 (2011) 1420–1431. [23] V. Duchaine, C. Gosselin, Safe, stable and intuitive control for physical human–robot interaction, in: Robotics and Automation, 2009. ICRA’09, in: IEEE International Conference on, IEEE, 2009, pp. 3383–3388. [24] T. Tsumugiwa, A. Sakamoto, R. Yokogawa, K. Hara, Switching control of po- sition/torque control for human–robot cooperative task-human–robot coop- erative carrying and peg-in-hole task, in: Robotics and Automation, 2003. Proceedings. ICRA’03, in: IEEE International Conference on, Vol. 2, IEEE, 2003, pp. 1933–1939. [25] H. Wang, K. Kosuge, Control of a robot dancer for enhancing haptic human–robot interaction in waltz, haptics, IEEE Trans. 5 (3) (2012) 264–273. [26] S. Hyon, J.G. Hale, G. Cheng, Full-body compliant human–humanoid interac- tion: Balancing in the presence of unknown external forces, IEEE Trans. Robot. 23 (5) (2007) 884–898. [27] A. Bussy, A. Kheddar, A. Crosnier, F. Keith, Human-humanoid haptic joint object transportation case study, in: Intelligent Robots and Systems (IROS), 2012, in: IEEE/RSJ International Conference on, IEEE, 2012, pp. 3633–3638. [28] K.C. Galloway, J.E. Clark, D.E Koditschek, Design of a tunable stiffness com- posite leg for dynamic locomotion, in: ASME 2009 International Design Engi- neering Technical Conferences and Computers and Information in Engineering Conference, American Society of Mechanical Engineers, 2009, pp. 215–222. [29] S. Kobayashi, M. Nakabayashi, H. Morikawa, Bioinspired propulsion mecha- nism in fluid using fin with dynamic variable-effective-length spring, J. Biomech. Sci. Engng 1 (1) (2006) 280–289. [30] R. Levenson, A. Ruef, Empathy: A physiological substrate, J. Personal. Soc. Psychol. 63 (2) (1992) 234–246. [31] R. Henning, A. Armstead, J. Ferris, Social psychophysiological compliance in a four-person research team, Appl. Ergon. 40 (6) (2009) 1004–1010. http: //dx.doi.org/10.1016/j.apergo.2009.04.009. [32] S. Järvelä, J. Kivikangas, J. Kätsyri, N. Ravaja, Physiological linkage of dyadic gaming experience, Simul. Gaming 45 (1) (2014) 24–40. [33] M.M. Baun, N. Bergstrom, N.F. Langston, L. Thoma, Physiological effects of human/companion animal bonding, Nursing Res. 33 (3) (1984) 126–129. [34] H. Sumioka, A. Nakae, R. Kanai, H. Ishiguro, Huggable communication medium decreases cortisol levels, Scientific reports 3. [35] D. Kulic, E.A. Croft, Affective state estimation for human–robot interaction, robotics, IEEE Trans. 23 (5) (2007) 991–1000. [36] F. Dehais, M. Causse, F. Vachon, S. Tremblay, Cognitive conflict in human au- tomation interactions: A psychophysiological study, Appl. Ergon. 43:3 (2012) 588–595. [37] C.L. Bethel, K. Salomon, R.R. Murphy, J.L. Burke, Survey of psychophysiology measurements applied to human–robot interaction, in: Robot and Human Interactive Communication, 2007. RO-MAN 2007, in: The 16th IEEE Interna- tional Symposium on, IEEE, 2007, pp. 732–737. [38] P. Rani, N. Sarkar, C.A. Smith, L.D. Kirby, Anxiety detecting robotic system–towards implicit human–robot collaboration, Robotica 22 (01) (2004) 85–95. [39] D. Kulić, E. Croft, Pre-collision safety strategies for human–robot interaction, Auton. Robots 22 (2) (2007) 149–164. [40] S.A. Setiawan, J. Yamaguchi, S.-H. Hyon, A. Takanishi, Physical interaction between human and a bipedal humanoid robot-realization of human-follow walking, in: Robotics and Automation, 1999. Proceedings. 1999 IEEE Interna- tional Conference on, Vol. 1, IEEE, 1999, pp. 361–367. [41] M.M. Rahman, R. Ikeura, K. Mizutani, Investigation of the impedance charac- teristic of human arm for development of robots to cooperate with humans, JSME Int. J. Ser. C 45 (2) (2002) 510–518. [42] P. Evrard, A. Kheddar, Homotopy switching model for dyad haptic interaction in physical collaborative tasks, in: EuroHaptics Conference, 2009 and Sympo- sium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics 2009. Third Joint, IEEE, 2009, pp. 45–50. [43] A. Kheddar, Human–robot haptic joint actions is an equal control-sharing ap- proach possible? in: Human System Interactions (HSI), 2011 4th International Conference on, IEEE, 2011, pp. 268–273. [44] J. Corredor, J. Sofrony, Shared control based on roles for telerobotic systems, in: Robotics Symposium, 2011 IEEE IX Latin American and IEEE Colombian Conference on Automatic Control and Industry Applications (LARC), IEEE, 2011, pp. 1–6. [45] M. Lawitzky, J.R.M. Hernández, S. Hirche, Rapid prototyping of planning, learning and control in physical human–robot interaction, in: Experimental Robotics, Springer, 2013, pp. 73–88. 164 A. Billard / Robotics and Autonomous Systems 88 (2017) 157–164 [46] N. Jarrassé, V. Sanguineti, E. Burdet, Slaves no longer: Review on role assign- ment for human–robot joint motor action, Adapt. Behav. 22 (1) (2014) 70–82. [47] H. Asada, Teaching and learning of compliance using neural nets: Repre- sentation and generation of nonlinear compliance, in: Robotics and Automa- tion, 1990. Proceedings., 1990 IEEE International Conference on, IEEE, 1990, pp. 1237–1244. [48] S.J. Buckley, Planning and teaching compliant motion strategies, Tech. rep., DTIC Document, 1987. [49] H. Asada, Studies on prehension and handling by robot hands with elastic fingers, 1979. [50] M. Cohen, T. Flash, Learning impedance parameters for robot control using an associative search network, IEEE Trans. Robot. Autom. 7 (3) (1991) 382–390. [51] R. Ikeura, H. Inooka, Variable impedance control of a robot for cooperation with a human, in: Robotics and Automation, 1995. Proceedings, 1995 IEEE International Conference on, Vol. 3, IEEE, 1995, pp. 3097–3102. [52] E. Gribovskaya, A. Kheddar, A. Billard, Motion learning and adaptive impedance for robot control during physical interaction with humans, in: Robotics and Automation (ICRA), 2011 IEEE International Conference on, IEEE, 2011, pp. 4326–4332. [53] E.L. Sauser, B.D. Argall, G. Metta, A.G. Billard, Iterative learning of grasp adap- tation through human corrections, Robot. Auton. Syst. 60 (1) (2012) 55–71. [54] K. Kronander, A. Billard, Learning compliant manipulation through kinesthetic and tactile human–robot interaction, IEEE Trans. Haptics. http://dx.doi.org/10. 1109/TOH.2013.54. [55] L. Rozo Castañeda, S. Calinon, D. Caldwell, P. Jimenez Schlegl, C. Torras, Learning collaborative impedance-based robot behaviors, in: In Proc. of the AAAI Conference on Artificial Intelligence, Bellevue, WA, USA, 2013, pp. 1422–1428. [56] A. Ajoudani, N.G. Tsagarakis, A. Bicchi, Tele-impedance: Teleoperation with impedance regulation using a body-machine interface, Int. J. Robot. Res. (2012) 0278364912464668. [57] M.S. Erden, A. Billard, End-point impedance measurements across dominant and nondominant hands and robotic assistance with directional damping, IEEE Trans. Cybern. 45 (6) (2014) 1146–1157. http://dx.doi.org/10.1109/TCYB.2014. 2346021. [58] E. Burdet, G. Ganesh, C. Yang, A. Albu-Schäffer, Interaction force, impedance and trajectory adaptation: By humans, for robots, in: Experimental Robotics, Springer, 2014, pp. 331–345. [59] J. Buchli, F. Stulp, E. Theodorou, S. Schaal, Learning variable impedance control, Int. J. Robot. Res. 30 (7) (2011) 820–833. [60] M. Howard, D.J. Braun, S. Vijayakumar, Transferring human impedance be- haviour to heterogeneous variable impedance actuators, IEEE Trans. Robot. 29 (4) (2013) 847–862. [61] A.K. Tanwani, A. Billard, Transfer in inverse reinforcement learning for multi- ple strategies, in: Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ Inter- national Conference on, Ieee, 2013, pp. 3244–3250. [62] C. Breazeal, Toward sociable robots, Robot. Auton. Syst. 42 (3) (2003) 167–175. [63] M.L. Walters, K. Dautenhahn, K.L. Koay, C. Kaouri, R.t. Boekhorst, C. Nehaniv, I. Werry, D. Lee, Close encounters: Spatial distances between people and a robot of mechanistic appearance, in: Humanoid Robots, 2005 5th IEEE-RAS International Conference on, IEEE, 2005, pp. 450–455. [64] A. Billard, Challenges in designing the body and the mind of an interactive robot, in: Proceeding of the AISB05 convention, Symposium on Robot Com- panions: Hard Problems and Open Challenges, 12–15 April, 2005,UH, 2005. [65] I. Asimov, I, robot, Spectra, 2004. [66] P. Lichocki, A. Billard, P.H. Kahn, The ethical landscape of robotics, IEEE Robot. Autom. Mag. 18 (1) (2011) 39–50. [67] P.-A. Mudry, S. Degallier, A. Billard, On the influence of symbols and myths in the responsibility ascription problem in roboethics-a roboticist’s perspective, in: Robot and Human Interactive Communication, 2008. RO-MAN 2008. The 17th IEEE International Symposium on, IEEE, 2008, pp. 563–568. [68] M. Vasic, A. Billard, Safety issues in human–robot interactions, in: Robotics and Automation (ICRA), 2013 IEEE International Conference on, IEEE, 2013, pp. 197–204. Professor Aude Billard is head of the Learning Algorithms and Systems Laboratory (LASA) at the School of Engineer- ing at the EPFL. She received a M.Sc. in Physics from EPFL (1995), a M.Sc. in Knowledge-based Systems (1996) and a Ph.D. in Artificial Intelligence (1998) from the University of Edinburgh. Her research interests include machine- learning tools to support robot learning through human guidance and physical cooperation between humans and robots. This also extends to research on complementary topics, including stable, safe and compliant control, ma- chine vision and its use in human–robot interaction and computational neuroscience to develop models of motor learning in humans. She was the recipient of the Intel Corporation Teaching award, the Swiss National Science Foundation career award in 2002, the Outstanding Young Per- son in Science and Innovation from the Swiss Chamber of Commerce and the IEEE-RAS Best Reviewer award. Aude Billard served as an elected member of the Administrative Committee of the IEEE Robotics and Automation society for two terms (2006–2008 and 2009–2011). She was a keynote speaker at the IEEE-RAS International Conference on Robotics and Automation (ICRA) in 2013 and at the IEEE International Symposium on Human–Robot Interaction (ROMAN) in 2005, general chair for the IEEE International Conference on Human–Robot Interaction in 2011 and co-general chair for the IEEE International Conference on Humanoid Robots in 2006. Her research on human–robot interaction and robot programming by demonstration was featured in numerous premier venues (BBC, IEEE Spectrum) and received several best paper awards at major robotics conferences, among which ICRA, IROS and ROMAN.","libVersion":"0.3.2","langs":""}