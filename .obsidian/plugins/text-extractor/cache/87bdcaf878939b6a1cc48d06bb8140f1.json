{"path":"_aula_virtual/SJK001/Reading Assessments/[Romano11 ]Human-Inspired Robotic Grasp Control With Tactile Sensing.pdf","text":"IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 1067 Human-Inspired Robotic Grasp Control With Tactile Sensing Joseph M. Romano, Student Member, IEEE, Kaijen Hsiao, Member, IEEE,G¨unter Niemeyer, Member, IEEE, Sachin Chitta, Member, IEEE, and Katherine J. Kuchenbecker, Member, IEEE Abstract—We present a novel robotic grasp controller that al- lows a sensorized parallel jaw gripper to gently pick up and set down unknown objects once a grasp location has been selected. Our approach is inspired by the control scheme that humans em- ploy for such actions, which is known to centrally depend on tactile sensation rather than vision or proprioception. Our controller pro- cesses measurements from the gripper’s ﬁngertip pressure arrays and hand-mounted accelerometer in real time to generate robotic tactile signals that are designed to mimic human SA-I, FA-I, and FA-II channels. These signals are combined into tactile event cues that drive the transitions between six discrete states in the grasp controller: Close, Load, Lift and Hold, Replace, Unload, and Open. The controller selects an appropriate initial grasping force, detects when an object is slipping from the grasp, increases the grasp force as needed, and judges when to release an object to set it down. We demonstrate the promise of our approach through implemen- tation on the PR2 robotic platform, including grasp testing on a large number of real-world objects. Index Terms—Robot grasping, tactile sensing. I. INTRODUCTION A S ROBOTS move into human environments, they will need to know how to grasp and manipulate a very wide variety of objects [1]. For example, some items may be soft and light, such as a stuffed animal or an empty cardboard box, while others may be hard and dense, such as a glass bottle or apple. After deciding where such objects should be grasped (ﬁnger placement), the robot must also have a concept of how to execute the grasp (ﬁnger forces and reactions to changes in the grasp state). A robot that operates in the real world must be able to quickly grip a wide variety of objects ﬁrmly, without dropping them, and delicately, without crushing them (see Fig. 1). Noncontact sensors, such as cameras and laser scanners, are essential for robots to recognize objects and plan where to grasp Manuscript received August 1, 2010; revised December 26, 2010 and April 17, 2011; accepted July 7, 2011. Date of publication August 4, 2011; date of current version December 8, 2011. This paper was recommended for publica- tion by Associate Editor G. Metta and Editor W. K. Chung upon evaluation of the reviewers’ comments. J. M. Romano and K. J. Kuchenbecker are with the Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania, Philadelphia, PA 19104 USA (e-mail: jrom@seas.upenn.edu; kuchenbe@seas.upenn.edu). K. Hsiao and S. Chitta are with Willow Garage, Inc., Menlo Park, CA 94025 USA (e-mail: hsiao@willowgarage.com; sachinc@willowgarage.com). G. Niemeyer is with Willow Garage, Inc., Menlo Park, CA 94025 USA, and also with Stanford University, Stanford, CA 94305 USA (e-mail: gunter@willowgarage.com). Color versions of one or more of the ﬁgures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identiﬁer 10.1109/TRO.2011.2162271 Fig. 1. Willow Garage PR2 robot using our grasp controller to carefully handle two sensitive everyday objects. them, e.g., [2], [3]. Recognition and planning may also instruct the grasping action, for example, providing an object’s expected stiffness, weight, frictional properties, or simply the required grasp forces, but to safely handle unknown objects as well as to remain robust to inevitable uncertainties, any such apriori information must be complemented by real-time tactile sensing and observations. Indeed, tactile sensors are superior to other modalities at perceiving interactive events, such as the slip of an object held in the ﬁngers, a glancing collision between the object and an unseen obstacle, or the deliberate contact when placing the object. Understanding contact using tactile informa- tion and reacting in real time will be critical skills for robots to successfully interact with real-world objects, just as they are for humans. A. Human Grasping Neuroscientists have thoroughly studied the human talent to grasp and manipulate objects. As recently reviewed by Johansson and Flanagan [4], human manipulation makes great use of tactile signals from several different types of mechanore- ceptors in the glabrous (nonhairy) skin of the hand, with vision and proprioception providing information that is less essential. Table I provides a list of four types of mechanoreceptors in human glabrous skin. Johansson and Flanagan divide the seem- ingly effortless action to pick up an object and set it back down 1552-3098/$26.00 © 2011 IEEE 1068 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 TABLE I MECHANORECEPTORS IN GLABROUS HUMAN SKIN [4], [6]–[8] into seven distinct states: reach, load, lift, hold, replace, un- load, and release. In the ﬁrst phase, humans close their grasp to establish ﬁnger contact with the object. Speciﬁcally, the tran- sition from reach to load is known to be detected through the FA-I (Meissner) and FA-II (Pacinian) afferents, which are stim- ulated by the initial ﬁngertip contact. FA signiﬁes that these mechanoreceptors are fast adapting; they respond primarily to changes in mechanical stimuli, with FA-I and FA-II having small and large receptive ﬁelds, respectively. Once contact has been detected, humans increase their grasp force to the target level, using both preexisting knowledge about the object and tactile information that is gathered during the interaction. This loading process is regulated largely by the response of the SA-I (Merkel) afferents, which are slowly adapting with small receptive ﬁelds. The load phase ends when the target grasp force is reached with a stable hand posture. Once the object is securely grasped, humans use their arm muscles to lift up the object, hold it in the air, and possibly trans- port it to a new location. Corrective actions (typically increases in grip force) are applied during the lifting and holding phases when the tactile feedback does not match the expected result. Srinivasan et al. [5] showed that the FA-I and FA-II signals are the primary sources of information to detect both ﬁngertip slip and new object contact. Slip is of critical importance to reject disturbances in the lifting and holding phases. When it comes time to place the item back on the table, contact between be- tween the object and the table must be detected to successfully transition to unloading. The SA-I afferents are again important during unload to properly set the object down before full release. These tactile sensing capabilities and corrective reactions en- able humans to adeptly hold a very wide range of objects without crushing or dropping them. Indeed, humans typically apply a grip force that is only 10–40% more than the minimum amount needed to avoid slip [4], thereby achieving the dual goals of safety and efﬁciency. B. Our Approach: Human-Inspired Robotic Grasp Control Inspired by the ﬂuidity of human grasp control, this paper presents a set of methods that enable a robot to delicately and ﬁrmly grasp real-world objects. We assume that ﬁngertip place- ment as well as hand and body movements has already been determined using noncontact sensors and appropriate planners. We describe robotic sensing methods that use ﬁnger-mounted pressure arrays and a hand-mounted accelerometer to mimic the important tactile signals provided by human FA-I, FA-II, and SA-I mechanoreceptors. Noticeably absent in our approach are the SA-II mechanoreceptors, which are known to respond to tangential loading such as skin stretch. We omit this channel be- cause our current experimental system cannot measure such sig- nals, although our approach could be expanded to include them if they were available. The three sensory channels that we con- struct allow us to create a high-level robotic grasp controller that emulates human tactile manipulation: in the words of Johansson and Flanagan, our controller is “centered on mechanical events that mark transitions between consecutive action phases that represent subgoals of the overall task” [4]. As diagrammed in Fig. 2, our approach separates robotic grasping into six discrete states: Close, Load, Lift and Hold, Replace, Unload, and Open. These states purposefully mirror those of human grasping, although we have combined Lift and Hold because their control responses are nearly identical for the hand. Each state deﬁnes a set of rules to control a robotic gripper to perform the speciﬁed behavior that is based on the tactile sensations it experiences. In addition to creating this approach to robotic grasp control, we implemented our methods on the standardized hardware and software of the Willow Garage PR2 robot; our goal was to enable it to perform two-ﬁngered grasps on typical household objects at human-like speeds, without crushing or dropping them. Section II summarizes previous work in the area of tactile robotic grasping and substantiates the novelty of our approach. Section III describes pertinent attributes of the PR2 platform, while Section IV deﬁnes our robotic SA-I, FA-I, and FA-II tactile channels and the low-level position and force control strategies that we created for the PR2’s high-impedance gripper. Section V expounds on the control diagram of Fig. 2 by carefully deﬁning each control rule and state transition. As described in Section VI, we validated our methods through experiments with the PR2 and a large collection of everyday objects under a variety of challenging test conditions. We conclude this paper and discuss our plans for future work in Section VII. II. BACKGROUND The development of tactile sensors for robotic hands has been a very active area of research, as reviewed by Cutkosky et al. [9]. Among the wide range of sensors one could use, Dahiya et al. [10] present a strong case for the importance of having tactile sensors capable of reproducing the rich response of the human tactile sensing system. Along these lines, some recent sensors have even achieved dynamic response [11] and spatial coverage [12] that are comparable to the human ﬁngertip. Using tactile sensory cues with wide dynamic range as a trigger for robotic actions was ﬁrst proposed by Howe et al. over two decades ago [13]. Unfortunately, most such sensors still exist only as research prototypes and are not widely available. The pressure-sensing arrays that are used in our study represent the state of the art in commercially available tactile sensors, and they are available on all PR2 robots. High-bandwidth acceleration sensing is far more established, although such sensors are rarely included in robotic grippers. In this study, we present a fully autonomous PR2 robot that can perceive its environment, pick up objects from a table, and set them back down in a new location. We build on the over- all system that is described in [14], focusing on methods to use tactile feedback (pressure and acceleration) to improve the ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1069 Fig. 2. State diagram for our robotic grasp controller. State transitions occur only after speciﬁc tactile events are detected. The details of this controller are presented in Sections IV and V. Constant-valued parameters, such as VCLOSE, are deﬁned in Table II. gripper’s contact interactions with grasped objects. Prior ver- sions of this system have used predeﬁned grasp forces and were limited to either crushing or dropping many objects; hence, our primary goal is to enable two-ﬁngered grasps that are gentle but secure. Several other research groups have developed autonomous pick-and-place robotic systems, such as [15]–[17]. While these other works often use tactile sensing to guide adjustments in hand position during the pregrasp stage, they rely mainly on predeﬁned grasp forces or positions and the mechanical com- pliance of their gripper in order to hold objects. Such strategies do not work well for the high-impedance parallel jaw grippers with which many robots are equipped. In other previous work, Yussof et al. [18] showed promising results using a custom- built tabletop manipulator with custom optical ﬁngertip sensors that measure normal and shear forces. Takahashi et al. [19] cal- culated robot ﬁngertip slip information using a pressure array similar to that used in our system. However, neither of these systems has yet been validated with a wide set of real-world objects, and we believe that there is room for new approaches to process and respond to tactile signals. Another large but scattered body of work is devoted to understanding ﬁngertip sensor signals at and during contact, e.g., [20]–[23]. Again, these algorithms are typically developed with custom hardware and validated against only a small and ideal set of objects, which makes it hard to draw conclusions about their general utility. In some of our own previous work, we found that sensor information, such as slip [24] and contact stiffness [25], is useful in well-controlled situations but can be extremely susceptible to changes in object texture, sensor ori- entation, and other such factors that will naturally vary when implemented on a fully autonomous robot that needs to interact with unknown objects in human environments. This paper aims to develop robust tactile sensory signals that will apply to as many objects as possible in order to create a tactile-event-driven model for robotic grasp control. Several researchers [10], [13] have noted the importance of such an approach, but to our knowledge, this is among the ﬁrst imple- mentations of such methods in a fully autonomous robot that has not been built for the sole purpose of grasping. Fig. 3. PR2 robot gripper. The accelerometer is rigidly mounted to a printed circuit board in the palm, and the pressure sensors are attached to the robot’s ﬁngertips under the silicone rubber coating. III. ROBOT EXPERIMENTAL SYSTEM Robots have great potential to perform useful work in ev- eryday settings, such as cleaning up a messy room, preparing and delivering orders at a restaurant, or setting up equipment for an outdoor event [1]. Executing such complex tasks requires hardware that is both capable and robust. Consequently, we use the Willow Garage PR2 robotic platform. As shown in Fig. 1, the PR2 is a human-sized robot that is designed for both nav- igation and manipulation. It has an omnidirectional wheeled base, two seven-degree-of-freedom (DOF) arms, and two one- DOF parallel-jaw grippers. Its extensive noncontact sensor suite includes two stereo camera pairs, an LED pattern projector, a high-resolution camera, a camera on each forearm, a head- mounted tilting laser range ﬁnder, a body-mounted ﬁxed laser range ﬁnder, and an intertial measurement unit (IMU). Fig. 3 shows the parallel jaw gripper that is mounted on each of the PR2’s arms. The gripper’s only actuator is a brushless dc motor with a planetary gearbox and an encoder. This motor’s rotary motion is converted to the parallel jaw motion through a custom internal mechanism in the body of the gripper. Unlike many of the robot hands that are currently being developed to grasp, e.g., [15], [17], the PR2 gripper has a high mechanical impedance because of the large gear ratio of the actuation sys- tem; note that it can be slowly back-driven by applying a large force at the ﬁngertips. Motor output torque can be speciﬁed 1070 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 in low-level software, but the transmission include no torque or force sensors. Instead, motor effort is estimated using cur- rent sensing; the transmission’s signiﬁcant friction prevents this signal from corresponding well with the force that the gripper applies to an object during grasping. A high-bandwidth Bosch BMA150 digital accelerometer is embedded in the palm of each gripper, as illustrated in Fig. 3. This tiny sensor measures triaxial acceleration over a range of ±78 m/s2, with a nominal resolution of 0.15 m/s2. The ac- celerometer has a sampling rate of 3 kHz and a bandwidth from dc to 1.5 kHz. Successive triaxial acceleration measurements are grouped together in sets of three (nine total values) and made available to the controller at a rate of 1 kHz. Each of the gripper’s two 2.3 cm × 3.7 cm × 1.1 cm ﬁngertips is equipped with a pressure sensor array that consists of 22 individual cells. The 22 cells are divided between a ﬁve-by-three array on the parallel gripping surface itself, two sensor elements at the end of the ﬁngertip, two elements on each side of the ﬁngertip, and one on the back (see Fig. 3). These capacitive sensors (manufactured by Pressure Proﬁle Systems, Inc., CA) measure the perpendicular compressive force that is applied in each sensed region, and they have a nominal resolution of 6.25 mN. As shown in Fig. 3, the entire sensing surface is covered by a protective layer of silicone rubber that provides the compliance and friction needed for successful grasping. All pressure cells are sampled simultaneously at a rate of 24.4 Hz. Because of manufacturing imperfections and residual stresses in the deformed rubber, each sensor cell has a unique nonzero reading when the ﬁngertips are subjected to zero force. We compensate for this nonideality by averaging the ﬁrst 0.25 s of each cell’s pressure measurements before each grasp and subtracting this offset from subsequent readings. We use the open-source ROS software system for all experi- ments that are conducted with the PR2 robot. The implemented gripper controllers are run inside a 1-kHz soft-real-time loop. Information from the tactile sensors (accelerometer and pres- sure cells) is available directly in this environment, as is the gripper’s encoder reading. IV. LOW-LEVEL SIGNALS AND CONTROL Individual sensor readings and actuator commands are far removed from the task to delicately pick up an object and set it back down on a table. Consequently, the high-level grasp controller that is diagrammed in Fig. 2 rests on an essential low-level processing layer that encompasses both sensing and acting. Here, we describe the three tactile sensory signals that we designed to mimic human SA-I, FA-I, and FA-II afferents, along with the position and force controllers that are needed for the gripper to move smoothly and interact gently with objects. All signal ﬁltering and feedback control was done within the 1-kHz soft-real-time loop. A. Fingertip Force (SA-I) The SA-I tactile sensory channel is known to play a primary role in the human hand’s sensitivity to steady-state and low- frequency skin deformations up to approximately 5 Hz [4]. As listed in Table I, this type of afferent has a small receptive ﬁeld and responds to low-frequency stimulation. The cumulative response of many SA-I nerve endings gives humans the ability to both localize contact on the ﬁngertip and discern the total amount of applied force [7]. The pressure arrays on the PR2 ﬁngertips provide information that is similar to human SA-I signals, although spatially far less dense. A signal similar to the SA-I estimate of total ﬁngertip force can be obtained by summing the readings from all 15 elements in the pad of one ﬁnger on the gripper: Fgl = 3∑ i=1 5∑ j =1 fl(i,j ). (1) The value fl(i,j ) represents the force that acts on the left ﬁnger- pad cell at location i, j. The same procedure is used to calculate Fgr for the right ﬁnger using fr (i,j ). The mean grip force is calculated by averaging the force that is experienced by the two ﬁngers, Fg = 1 2 (Fgl + Fgr ). Fig. 4 shows an example of these ﬁngertip force signals during object contact, along with examples of other tactile signals that are described later. We also attempted to obtain localization information about the ﬁngertip contact using the methods that are described in [19], but we were not able to achieve satisfactory results. We hypothesize that differences in the ﬁngertip shape (the PR2 has a ﬂat ﬁngertip that leads to multiple contact locations, as opposed to the rounded ﬁngertips that were used by Takahashi et al.) and the lower number of tactile cells in our sensors were the main reasons that we could not localize contacts with the PR2. B. Fingertip Force Disturbance (FA-I) Human FA-I signals are believed to be the most important indicator of force-disturbance events during bare-handed ma- nipulation. As listed in Table I, they have small receptive ﬁelds and are attuned to mid-range frequencies. Force disturbances occur at many instants, including initial object contact, object slippage, impacts between a hand-held object and the environ- ment, and the end of object contact. We process the data from the PR2’s pressure arrays to create a signal similar to the human FA-I channel. Our chosen calculation is to sum a high-pass-ﬁltered version of the forces detected in all 15 ﬁngertip cells: ̃Fgl(z)= 3∑ i=1 5∑ j =1 HF (z)fl(i,j )(z). (2) The force that is measured in each cell fl(i,j ) is subjected to a discrete-time ﬁrst-order Butterworth high-pass ﬁlter HF (z) with a cutoff frequency of 5 Hz, which is designed for the 24.4- Hz sampling rate of the pressure signals. The resulting ﬁltered signals are then summed to obtain an estimate of the >5 Hz force disturbances ̃Fgl acting on the entire left ﬁngerpad. The process is repeated for the right ﬁnger to obtain ̃Fgr . C. Hand Vibration (FA-II) FA-II afferents are known to be the primary tactile chan- nel by which humans sense interactions between a handheld ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1071 tool and the items it is touching. As listed in Table I, these mechanoreceptors have wide receptive ﬁelds and are attuned to high frequencies. During object grasping and manipulation, FA-II signals are particularly useful to detect contact between handheld objects and other things in the environment, such as a table surface. We create a robotic analog to the FA-II sensory channel by processing data from the PR2’s hand-mounted accelerometer, as follows: ̃ah (z)= √(Ha (z)ah,x )2 +(Ha (z)ah,y )2 +(Ha (z)ah,z )2. (3) The hand vibration signal ̃ah is calculated by taking the mag- nitude of the high-pass-ﬁltered 3-D acceleration vector. The ﬁlter applied to each of the three Cartesian acceleration compo- nents (ah,x ah,y ah,z ) is a discrete-time ﬁrst-order Butterworth high-pass ﬁlter Ha (z) with a 50-Hz cutoff frequency, which is designed for the 3 kHz sampling rate of our acceleration data stream. D. Position and Force Control In addition to the rich tactile sensations that are described earlier, humans excel at manipulation because they can move competently through free space but quickly transition to reg- ulating grasp force during object contact [4]. Replicating the ﬂuidity of human grasping with a high-impedance parallel jaw gripper requires well-designed position and force controllers. Both of these controllers appear several times in the high-level state diagram of Fig. 2; each controller block is labeled with its type, along with the desired motion or force output. To facil- itate a generic presentation of our approach, the mathematical constants that are used in this paper are designated with an all-capitalized naming convention. The PR2 gripper is a geared mechanism; therefore, it lends itself well to position control. We deﬁne its position xg in meters and its velocity vg in meters per second. The position is zero when the ﬁngers touch and positive otherwise so that the position value corresponds to the grip aperture. The gripper velocity follows the same sign as position, with positive values indicating that the hand is opening. We found that we could achieve good position tracking via a simple proportional-derivative controller with an additional velocity-dependent term to overcome friction E = KP · (xg − xg,des)+ KD · (vg − vg,des) − sign(vg,des) · EFRICTION. (4) Here, E is the motor effort (in Newton), KP is the proportional error gain (in Newtons per meter), KD is the derivative error gain (in Newton seconds per meter), and xg,des and vg,des are the desired gripper position (in meters) and velocity (in meters per second), respectively. EFRICTION is a scalar constant for feed- forward friction compensation, applied to encourage motion in the direction of vg,des. Note that motor effort is deﬁned to be positive in the direction that closes the gripper, which is opposite from the sign convention for the motion variables. Table II lists values and units for all of the constants that are used in our controllers, including KP, KD, and EFRICTION. TABLE II VALUES CHOSEN FOR CONTROLLER CONSTANTS We created a force controller on top of this position controller to enable the PR2 to better interact with delicate objects. This controller requires access to the ﬁngertip force signals Fg that are described earlier in Section IV-A. Forces that compress the ﬁngertips are deﬁned to be positive so that positive motor effort has the tendency to create positive ﬁngertip forces. We have developed a force controller that drives the desired position and velocity terms that are based on the error between the desired force and the actual force [26] Fg,min = min (Fgl,Fgr ) (5) vg,des = KF · (Fg,min − Fg,des) (6) KF = { KFCLOSE if Fg,min − Fg,des < 0 KFOPEN otherwise. (7) We servo on the minimum of the two ﬁnger forces Fg,min ,as deﬁned in (5), to ensure dual-ﬁnger contact. Errors to track the desired force Fg,des are multiplied by the constant gain KF to yield the desired velocity for the position controller (6). This desired velocity is integrated over time to provide the posi- tion controller with a desired grip aperture xg,des. Experimental testing revealed that high values of the gain KF improved force tracking but caused the commonly encountered force-controller effect of chattering as well. We found that an asymmetric gain deﬁnition (7), where KFCLOSE is greater than KFOPEN, al- lows for the best balance of stability and responsiveness during grasping. V. ROBOTIC GRASP CONTROL This section describes the high-level grasp controller that we developed to enable a robotic gripper to mimic the human capacity to pick up and set down objects without crushing or dropping them. This control architecture is diagrammed in Fig. 2 and explained sequentially later. It makes use of all three of the low-level tactile signals and both of the low-level controllers that are deﬁned in the previous section. A. Close Closing is the starting point for any grasp. In this phase, the hand goes from having no contact with the object to contacting the object with both ﬁngers. The Close state begins when a client software module requests a grasp, which is denoted by the Grasp() signal in Fig. 2. It is assumed that prior to this request, the gripper has been maneuvered so that its ﬁngers are 1072 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 Fig. 4. Time history data for an interaction between the gripper and an Odwalla juice bottle. Controller states are marked with bars at the top, and important signals (state transitions and tactile events) are indicated with arrows at the bottom. on opposing sides of an object, approximately equidistant to both surfaces, and that closing the gripper’s jaws will result in stable two-ﬁngered contact with the object. Selection of such poses without requiring a model of the object is an ongoing area of research, e.g., [3], [14], as is correcting for errors in execution of the selected pose, e.g., [14], [17]. Here, we assume a reasonable starting arm pose and focus solely on controlling the gripper’s one DOF. After the Grasp() request has been received, we use the po- sition controller that is deﬁned in (4) to close the gripper with a constant desired velocity vg,des = −VCLOSE. This closing movement continues until contact is detected on both the left and right ﬁngers LeftContact =(Fgl > FLIMIT) ∥ ( ̃Fgl > DLIMIT) (8) RightContact =(Fgr > FLIMIT) ∥ ( ̃Fgr > DLIMIT). (9) Each ﬁngertip force signal is compared with the constant FLIMIT, and each ﬁngertip force-disturbance signal is com- pared with the constant DLIMIT. Note that a ﬁnger’s contact signal is true if the threshold on either its Fg or its ̃Fg has been exceeded, and recall that the second of these signals is merely a high-pass-ﬁltered version of the ﬁrst. Through implementa- tion on the PR2, we found that DLIMIT can be much smaller than FLIMIT because ̃Fgl is not sensitive to the pressure cells’ low-frequency ﬂuctuations; consequently, the force-disturbance condition is almost always ﬁrst to trigger a contact event. A sample instance of LeftContact && RightContact is indicated in Fig. 4. While several groups [5], [13] have noted that higher fre- quency vibrations are useful in detection of contact, we could not ﬁnd a reliable indication of such events in our ̃ah signal. This difﬁculty is likely because of the speciﬁc hardware of the PR2. A large distance separates the accelerometer from the ﬁn- gertips, and the compliant silicone ﬁngertip coverings soften impacts considerably. Furthermore, signiﬁcant high-frequency vibration noise occurs whenever the gripper motor is turning, which masks out any small tactile cues that might occur (see the ̃ah signal during the Close and Open phases in Fig. 4). For- tunately, the tactile signals that are derived from the pressure cells are very reliable to detect the start of contact, providing a consistent cue to transition to the load phase. B. Load The goal of the load phase is to apply a grip force that is appropriate for the target object so that the object can be lifted, and manipulation can begin. Selection of this grip force level is challenging when the robot does not have a detailed mechanical model of the object or prior experience in gripping it. Conse- quently, we designed a novel method to choose a reasonable starting grasp force; many alternatives were tested, and here, we report only the most reliable one. After contact is detected, the robot pauses for a short period of time (TSETTLE) while trying to hold the position of the ﬁrst contact xc . This pause serves two purposes: First, many objects take a short amount of time to mechanically respond to the initial contact, and second, the pressure sensor cells update at a rate that is much slower than the controller rate (24.4 versus 1000 Hz). We have found that the force response during this contact settling time is a very useful indicator of how ﬁrmly an object should be grasped. Thus, the robot records the maximum average force seen by the gripper ﬁngers during this time and calculates the target force to hold the object as Fc = max t (Fg ) · KHARDNESS VCLOSE . (10) ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1073 As one would expect, the force response of an object strongly depends on the speed of the ﬁnger impact. We remove this de- pendence on velocity by dividing the gain KHARDNESS by VCLOSE, which is the speed at which the ﬁngers are com- manded to impact the object. This approach enables the con- troller to calculate a contact force Fc that is relatively inde- pendent of the closing speed. The values of KHARDNESS and VCLOSE both contribute strongly to the robot’s ability to deli- cately grasp objects. Large values of VCLOSE give the gripper high momentum during closing so that the robot may not be able to stop quickly upon object contact; therefore, we chose a more moderate closing speed. KHARDNESS has been tuned to estimate the initial grasp force for a generic set of everyday objects, but it tends to be too low for very soft heavy objects and too high for stiff delicate objects, occasionally leading to object drops or breakage, respectively. At the end of the brief pause after the start of contact, the robot transitions to force control, using the computed Fc value as the desired force Fg,des. If the robot had apriori knowledge of the appropriate force with which to grasp a certain object, that value would be used in conjunction with Fc to develop an estimate for Fg,des. The force control mode continues until the gripper achieves StableContact, which we deﬁne as StableContact =(|Fg,min − Fg,des| < FTHRESH) && (|vg | < VTHRESH) . (11) This condition requires that smaller of the two ﬁngertip forces Fg,min be within the tolerance FTHRESH of the command force Fg,des, and that the gripper speed be below the toler- ance VTHRESH. Through testing, we have found that grasps become stable very quickly on most everyday objects. Slower stabilizations occur with very hard objects, which require ad- ditional effort after contact to ramp up the steady-state force command, and extremely soft objects, which need some time before the velocity settles to zero. C. Lift and Hold After StableContact is achieved, the controller transitions to the next grasp phase: Lift and Hold. In this phase, the robot holds the object between its ﬁngertips and moves its arm to accomplish higher level tasks. It is desirable for the grasp con- troller to hold the object ﬁrmly enough to avoid slipping, but gently enough to avoid crushing. As with FA-I signals in human grasping, the high-pass ﬁltered force signal ̃Fg is a strong indi- cator of slip. This signal is more reliable than Fg itself, since it does not vary signiﬁcantly with low-frequency robot motion and reorientation of the object with respect to gravity. We calculate the Slip condition as follows: Slip =(| ̃Fg | >Fg · SLIPTHRESH) && (F BP g < FBPTHRESH) . (12) Our Slip condition is met only when both subsidiary sensory comparisons evaluate to true. First, the magnitude of the force- disturbance signal ̃Fg must exceed a threshold that is deﬁned by the product of the total force Fg and a constant SLIPTHRESH; using this product rather than a constant value makes the robot less sensitive to force variations as the grip force increases. This approach was again inspired by human capabilities: Hu- man force perception is known to follow Weber’s law, where a stimulus must vary by a certain percent of its magnitude to have the change be detectable [27]. Second, slips are considered only when the average grasp force does not change very quickly. We evaluate this force sta- bility condition by treating the average ﬁngerpad force Fg with a ﬁrst-order Chebyshev discrete-time band-pass ﬁlter with a pass- band from 1 to 5 Hz to produce F BP g . We compare this signal with the empirically tuned constant FBPTHRESH. The lower frequency cutoff of 1 Hz removes the mean value from the grip force signal, while the higher cutoff of 5 Hz removes the slip effects that are seen in ̃Fg . This condition prevents the formation of a feedback loop when the controller increases its grip force to stop slip events, as discussed next. Every time Slip occurs, the controller increases the desired grip force Fc by a small percentage of its current value, such that Fc = Fc · KSLIP. Several alternative methods to respond to slip were tested, such as increasing the desired grip force propor- tional to the magnitude of the slip event, as done by Takahashi et al. [19]. However, we found that our system does not exhibit a strong correlation between the amount of slip (in either speed or distance) and the ̃Fg signal; instead, this signal depends some- what on the properties of the grasped object; therefore, we use it to detect only that the grasp has been disturbed. In contrast with [19], we found that allowing the grip force to decay over time resulted in a higher percentage of dropped objects. This difference is most likely because of the slower response time of the PR2 gripper when compared with the hardware devel- oped in [19]. This approach does occasionally cause objects to be held tighter than the minimum necessary grasp force when transient slip conditions are experienced, e.g., after incidental contact between the object and the environment. D. Replace The transition from Lift and Hold to Replace occurs when a client software module sends the Place() signal. The robot should enter this mode only when it is ready to put the held object down, which is typically after it has moved the gripper to within a few centimeters of the desired placement location. After issuing the Place() command, a higher level robot controller moves the gripper toward the target surface at a moderate speed. During this time, a low-level force controller holds the ﬁnal target force from the previous phase, Fc , and the Replace controller monitors the Slip and Vibration signals to detect contact between the object and the environment. We deﬁne the Vibration condition as a threshold on the high-frequency hand acceleration signal Vibration =(̃ah > ATHRESH). (13) Reducing ATHRESH improves the robot’s ability to detect light contacts within the world, but it also makes the PR2 more sensi- tive to false-positive triggers because of mechanical vibrations from its own drivetrain. In practice, we have found that this value can be lowered to 1.25 m/s 2 during slow arm motion. 1074 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 When either Slip or Vibration becomes true, the robot assumes that contact has occurred between the object and the target sur- face, and it moves into the Unload phase. It is necessary to observe both the Slip and Vibration con- ditions to account for the variety of contact scenarios that can occur. In the case of very light objects, which are appropriately held lightly during the Lift and Hold phase, the object slips between the robot’s ﬁngers; therefore, the hand does not experi- ence signiﬁcant vibrations. In the case of heavy objects, which are held ﬁrmly, no Slip signal is detected since the object is held ﬁrmly enough to prevent slip, but the impact vibrations are easily apparent in the ̃ah signal. For many objects between these two extremes, both Slip and Vibration conditions are often true at contact. E. Unload The Unload phase is entered automatically after the held object contacts the target surface. The goal of this phase is simply to let go, but our controller performs this unloading gradually to avoid abrupt transitions. The desired grip force is linearly reduced to zero over a set period of time using Fg,des = Fc − Fc t − ts TUNLOAD . (14) Here, t represents the present time, and ts represents the starting time for the Unload state. TUNLOAD is a constant that deter- mines the unloading duration. The state is exited when Fg,des reaches zero, which occurs when t − ts = TUNLOAD. F. Open Once the robot has released the object on the surface, it pro- ceeds to open the gripper. This movement is accomplished by the position controller, using a constant positive desired velocity of VOPEN. VI. EXPERIMENTAL VALIDATION This section ﬁrst presents a method to tune our two main con- troller constants, i.e., KSLIP and KHARDNESS, which helps show how these parameters affect grasp success. We then dis- cuss experiments that test the performance of the novel aspects of our controller: the grip force chosen during Load, the re- sponse to Slip during Lift and Hold, and the effect of Slip and Vibration signals during Replace. To understand how our ap- proach compares with more simplistic grasping solutions, we then conducted a more general test of the PR2’s capabilities using a collection of 50 everyday household objects. A. Parameter Tuning One challenge in implementing the grasp controller in Section V is to select appropriate values for KSLIP and KHARDNESS. These two parameters play a major role in the successful manipulation of objects. Proper tuning will result in delicate manipulation, while improper tuning will result in high rates of object crushing or slippage. We deﬁne crushing to be a deformation of 10 mm beyond the initial surface contact. We Fig. 5. Effect of varying KSLIP and KHARDNESS. As KHARDNESS is decreased away from the tuned value, objects tend to slip within the grasp because of inadequate initial grip forces. As KHARDNESS is increased, the grip force is overestimated, and crushing occurs. As KSLIP is decreased, the robot does not respond strongly enough to slip events, and the object is allowed to slip an unacceptable amount. As KSLIP is increased, the robot tends to overcompensate and crush objects. deﬁne slippage in two forms: translation (greater than 5 mm) or rotation (greater than 0.175 rad). For the PR2 robot, our parameter-tuning experiments pro- ceeded as follows. We chose a plastic cylindrical-shaped Coffee- Mate canister with a mass of 0.45 kg for our parameter-tuning study. This object was selected for its heavy weight, which ne- cessitates using a strong grasp to prevent object slip, and its ﬂexible body, which necessitates using a delicate grasp to avoid crushing the object. First, the canister was placed on a table in front of the robot, and the open robot gripper was positioned around the object. Next, the Grasp() command was sent to the robot to initiate the event-driven grasping sequence. Finally, after the StableContact signal was achieved, the robot lifted the object 0.3 m upward and rotated it 1.05 rad. During the task, the grasp aperture was continuously monitored to detect crushing conditions. After the task, the experimenter measured the translation and rotation of the object to determine whether slip had occurred. This experiment was repeated ten times for all combinations of ﬁve values of KSLIP and ﬁve values of KHARDNESS, for a total of 250 trials. The ﬁve different val- ues for KSLIP and KHARDNESS were selected such that they caused the controller response to vary between the two extremes of crushing and slipping. The resulting state of each grasping attempt was recorded as either crushed, slipped (which included object drops), or successful. These results are presented in Fig. 5. Setting KSLIP and KHARDNESS to the central values, as listed in Table II, yields a system that is very likely to succeed at grasp- ing. Small deviations away from the ideal parameter values can often yield somewhat successful grasp behavior, but they typi- cally increase the likelihood for slip and/or crush results. Large deviations from the ideal parameter values result in completely unsuccessful grasp results, as deﬁned by our strict crushing and slipping metrics. Very large value for KHARDNESS will cause the robot to saturate its grip force, resulting in the tightest pos- sible grasp. In practice, this parameter space can be sparsely ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1075 Fig. 6. Target grip force chosen by our Load method when grasping eight everyday objects. The gain KHARDNESS was empirically tuned to yield a grip force (red bar) that is consistently above the minimum grip force necessary to lift the object (blue bar). This calculation provides a good estimate for a large range of objects, but one can see its tendency to overestimate the force necessary to hold objects that are both hard and light, such as an egg. The red × symbol marks the crushing force for all objects that can be crushed by the robot gripper. sampled during tuning, but it should be explored completely to obtain a thorough understanding of the parameter interaction as seen here. B. Grip Force Estimation During Loading As described in Section V-B, the controller in the Load phase selects the target grip force that is based on the maximum force measured during the gripper’s initial contact with the object, nor- malized by contact speed. We evaluated this technique through grasp testing on eight different objects: a paper cup, a paper- board tea box, a ripe banana, an empty soda can, a raw chicken egg, a tennis ball, a glass bottle, and a full soda can. We began the experiment by obtaining ground-truth measure- ments of the minimum grip force necessary for the PR2 to lift each object. These tests were done by placing each object in a known location and orientation on a table. The robot then closed its gripper on the object using only the force controller that is described in (5)–(7), with the desired force Fg,des settoasmall value (starting between 0.5 and 6 N, depending on the object). The robot then used its arm joints to move the gripper up by 10 cm. If slip occurred during lifting, the trial was repeated with the grasp force incremented by 0.1 N. If the object did not slip, the desired grip force was recorded as the minimum grip force, which is needed for lifting. This entire process was re- peated eight times per object. The blue “Minimum Force” bars in Fig. 6 show the mean and standard deviation of the eight ground-truth measurements for each of the eight objects. The experiment was then repeated using the grasp controller that is described in this paper. We performed eight trials with each of the same eight objects, located in the same position and orientation as earlier. For each trial, the desired loading force Fc was recorded, as calculated with (10). The dark red “Grip Force” bars in Fig. 6 show the mean and standard deviation of Fig. 7. Slip test results for three different trials. The glass cup was repeatedly ﬁlled with marbles to promote slip at a variety of grip force levels. The ground- truth data (red dashed line) indicate the minimum grip force, which is needed to prevent slip. As seen here, our Lift and Hold controller has been designed to grasp objects more tightly when it detects slip. This behavior reduces the likelihood of a dropped object without requiring unnecessarily high grasp forces. the eight grip force levels that the robot chose during the Load phase for each of the eight test objects. Finally, we determined the force which is necessary to crush each object (if crushing was possible) by successively incre- menting the force controller’s desired grip force by 0.1 N. Only a single recording was done for the crushing force because this operation damages the object. These crush force measurements appear as red ×s with the other results in Fig. 6. In all cases, our controller chose a grip force above the minimum level needed to avoid slip. For crushable objects, it chose grip forces well below the crush limit for all objects except the egg, which it crushed in three of the eight trials. C. Slip Response During Lift and Hold We conducted a separate experiment to test slip compensation in the Lift and Hold phase. As described in Section V-C, Lift and Hold uses the force controller to try to maintain a constant target grasp force; it watches for Slip events, which are derived from the pressure transducer data, and it responds by increasing the target grasp force by a small percentage. We sought to understand our system’s slip response by having the gripper hold a smooth straight-sided object that periodically increased in weight. At the start of this experiment, the cylin- drical section of a glass cup was placed in the robot gripper, as seen in the inset of Fig. 7. The weight of the cup was measured to be 0.6 N, and it was oriented vertically. The experimenter began a trial by activating the Lift and Hold controller with an initial desired grip force of 5 N. Batches of 15 marbles (about 0.6 N per batch) were then added to the cup at intervals of 3 s. The gripper was lightly shaken for 2 s after each batch of marbles was added, during which time the controller reacted to any detected Slip events. The ﬁnal selected grip force value was recorded in software before the experimenter added another batch of marbles. The marbles were added ﬁve times to give the 1076 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 Fig. 8. Fifty objects used in our robustness test. These objects were chosen for no speciﬁc reason except that they have a wide range of properties, including hard/soft, heavy/light, sticky/slippery, brittle/elastic. cup a ﬁnal weight of approximately 3.7 N. This procedure was repeated three times to produce the data shown in the solid traces of Fig. 7. This test’s ground-truth data were obtained for each of the six cup weights using the force controller in Section IV-D. The controller’s desired grip force was started at 1.0 N. After the cup was grasped by the robot, the experimenter lightly shook the gripper to emulate the slight disturbances that occur during arm motion. If the cup fell out of the gripper or slipped after 2 s of shaking, the trial was repeated with a grasp force incremented by 0.1 N. The grasp force needed to hold the cup at each of the six weights is shown by the red dashed “Minimum Grip Force” line in Fig. 7. One can see that this value increases up to an object weight of about 2.5 N and then levels off at approximately 8.3 N. The controller always chose a grip force value above the level that is needed to prevent slip. The variation between the three trials is primarily because of differences in how the experimenter shook the gripper; stronger external disturbances cause more corrective actions and higher grip force levels. D. Grasping Robustness Beyond testing speciﬁc aspects of our controller’s perfor- mance, we wanted to understand how the methods that are proposed in this paper would work on their intended subject, everyday real-world objects. We, thus, gathered the collection of 50 common objects shown in Fig. 8, purposefully seeking out items that could be challenging for a robot to grasp. The only re- quirement on these objects is that they are all within the robot’s perception and manipulation capabilities (have at least one grasp location that is less than the size of the maximum gripper aper- ture, weight less than the arm’s maximum payload, etc.). The objects that are included in the collection are as follows: apple, banana (rotten), Band-Aid box, beer bottle (empty), beer bottle (full), can of Spam, can of peas, candle, cereal box (empty), Coffee-Mate bottle, duct tape roll, foam ball, gum container, Jello cup, juice box, large plastic bowl, magic marker, masking tape roll, medicine bottle, milk carton (empty), ofﬁce tape dis- penser (heavy), ointment tube (full), peach (soft), plastic juice bottle (empty), plastic juice bottle (full), plum (rotten), rubber football, Solo plastic cup, Saran wrap box, ShiKai shampoo TABLE III OUTCOMES OF GRASP TESTING WITH EVERYDAY OBJECTS bottle (empty), small wooden bowl, soap bottle (empty), soap box (full), soda can (empty), soda can (full), soup can (full), stress ball, stuffed bear, stuffed elephant, Suave shampoo bottle (empty), sunglasses case, tea box (metal), tea box (paperboard), tennis ball, thin plastic cup, Tide bottle (full), towel, Vasoline container (full), water bottle (empty), and wood plank. The robot’s task for this experiment was to pick up each object from a table and set it down in a different location. We used the object perception and motion planning code of Hsiao et al. to enable the PR2 to accomplish this task autonomously [14]. The grasp selection and reactive grasping components of [14] were used to select stable grasps, with the object centered within the grasp before starting, as per our assumptions for the Close phase. After grasping the object, the robot lifted the object off the table, and then moved it from above the table to the side of the robot so that the arm would not interfere with perception for object placement. The object was then moved to the opposite side of the table for placement. This motion was planned using a randomized joint-angle planner and typically involved a great deal of object rotation, as is typical of many complex pick-and- place operations. During such motion, the robot would ideally prevent the object from rotating or slipping out of the grasp, while continuing to avoid crushing the object. Each object was tested under two grasp control conditions. The ﬁrst condition was the original manipulation code that is de- scribed in [14], which takes a naive approach by always closing the gripper with 100% motor effort. The second condition was a portion of the human-inspired robotic grasp controller that is described in this paper, which included our Close, Load, and Lift and Hold phases. The Replace, Unload, and Open phases could not be included because of code integration difﬁculties at the time of this experiment, but they have since been included in our grasping pipeline, and the following section describes rel- evant results. Our controller was tested ﬁrst because the naive controller tends to damage crushable objects. During testing, the experimenter presented all 50 objects to the robot one by one and manually recorded the outcome of each trial. As tabulated in Table III, four different types of errors occurred: The robot might crush a crushable object (30 of the 50 objects were crushable; crushable statistics are reported out of these 30 objects), it might let the object rotate or translate signiﬁcantly within its grasp, and it might drop the object either by failing to raise it off the table at the start of the Lift and Hold phase or by allowing it to slip from its grasp later in Lift and Hold. The naive controller was found to crush all 30 of the crush- able objects, while our controller crushed only one (the rubber football). This drastic improvement in handling delicate objects ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1077 Fig. 9. Histogram for initial grasp force in object marathon. Fig. 10. Histogram for motor effort in object marathon. is balanced by a higher incidence of objects that rotate, slip, and/or drop; our controller commits all three of these errors about twice as often as the naive controller. The two most chal- lenging items were the rubber football and the full Tide bottle; both controllers allowed them to slip within the grasp and fall on the table. This was because of the large size of both objects: the grasp location that was automatically selected (using [14]) for the football was larger than the maximum gripper diameter, and the position that was selected for the Tide bottle was within 1 cm of the maximum diameter. Quantitative data were also recorded for our controller’s grasp of each of the 50 objects. Fig. 9 presents a histogram of the grip force that is chosen during the Load phase for the 49 objects that were successfully lifted. These values range from 2.5 to 27.5 N with most objects below 7.5 N. The objects with the lowest initial grasp force were the towel, the Coffee-Mate bottle, the large plastic bowl, and the stress ball, in ascending order. The objects with the highest initial grasp force were the wood plank, the duct tape roll, and the sunglasses case, in descending order. From this, we observe that soft objects generally receive lower initial grip forces than hard objects, as one would expect from the design of our controller. Fig. 10 provides a histogram of the average motor effort required during Lift and Hold for the 46 objects that were not dropped. One can quickly see that our controller is much more efﬁcient than the naive controller, which always uses 100% motor effort. Because the PR2 gripper has high internal friction and compliant ﬁngertips, it can securely hold objects such as the foam ball and the Band-Aid box with zero motor effort. At the other end of the spectrum, the wood plank, the duct tape roll, and the full beer bottle all had an average motor effort of 100%. Fig. 11. Scans of the pressure-sensitive ﬁlm used to capture the forces applied by the robot to the object during object placement. Darker color indicates regions of higher pressure. (a) Standard approach assumes perfect knowledge of the table position and object position within the grasp, which both contain errors that lead to forceful impacts. (b) Our approach releases the object when table contact is detected, yielding gentler placements. (a) Standard approach. (b) Our approach. E. Contact Sensing During Replace Finally, we carried out a set of tests to evaluate the effec- tiveness of the Slip and Vibration signals used to detect contact between the held object and the table surface during Replace, as described in Section V-D. Using the same grasping procedure as the experiment with 50 objects, the PR2 robot was programmed to detect, grasp, lift, and replace a full soda can on a table. A sheet of pressure-sensitive ﬁlm (Pressurex-micro 2-20 PSI ﬁlm) was laid on the table at the placement location to record the pressure between the can and table. The ﬁlm darkens in color with the application of additional pressure from 13 800 to 138 000 N/m 2. The ﬁrst test consisted of 25 trials using the standard non- contact sensing approach to object placement. The height of the table was found from stereo camera and scanning laser sensor data. Once the object was grasped, it was assumed to be rigidly attached to the hand, and the robot attempted to place the object back down at the exact height of the table and release it. The second set of 25 trials used our contact-sensing replace strategy to release the object once the Slip or Vibration signal was de- tected. The resulting pressure-sensitive ﬁlm images can be seen in Fig. 11. Qualitative inspection of the resulting ﬁlms suggests that our human-inspired controller is signiﬁcantly more gentle than the standard approach. When using the noncontact sensing ap- proach, the robot released the object in the air (prior to contact with the table) a total of four out of the 25 trials, which caused the object to drop onto the table. In the remaining 21 trials with the standard controller, the robot pressed the object down into the table very ﬁrmly because the visually estimated target lo- cation was beneath the surface of the actual table. In contrast, our contact-sensing controller uses the tactile sensors to detect contact with the table and release the object, as shown in Fig. 12. 1078 IEEE TRANSACTIONS ON ROBOTICS, VOL. 27, NO. 6, DECEMBER 2011 Fig. 12. Elbow joint torque (top) and hand acceleration from (3) (bottom) comparing the two different approaches. The standard approach exerts signiﬁ- cantly greater torque after the initial contact, pushing the object down into the table. The Slip signal triggered in 4 of our controller’s 25 trials, and the Vibration signal triggered in the remaining 21. One Vibration- triggered release occurred prior to contact between the object and the table because of mechanical noise on the accelerometer, highlighting the opportunity for further improvements in tactile signal processing during robot motion. VII. CONCLUSION This paper has introduced a set of sensory signals and con- trol approaches that attempt to mimic the human reliance on cutaneous sensations during grasping tasks. We have presented a framework that highlights how tactile feedback can be used as a primary means to complete a manipulation task, with encour- aging results. While it is clear to the authors that not all tasks can be completed solely via tactile information, we feel that it is a promising and underutilized tool in robotic manipulation systems. In future work, we hope to add additional sensing modal- ities into our object handling framework, including estimates of the necessary object grip force from visual and laser recog- nition, audio feedback about crushing and damaging objects, weight sensing after an object has been lifted off the table, grasp disturbance prediction from arm motion data, grasp quality in- formation that is based on the ﬁngerpad contacts, and combining these data together into higher level percept information. Our current estimate of the initial grip force necessary to lift an ob- ject depends solely on the hardness information gleaned during contact. While it has shown to be a strong indicator for many ev- eryday objects, it does have certain failure cases where hardness information is deceptive, such as soft but heavy objects (e.g., a heavy trash bag or a stuffed animal) and light but hard objects (e.g., an egg or a thin wine glass). Supplementing this informa- tion with additional object data will likely lead to a superior grip force estimator. Our signal to detect slip information was successful, but the force response to slip events could be improved by using a gripper with superior dynamic response. The PR2 gripper is admittedly inferior to several of the compliant and well- modeled designs existing in the literature; we hypothesize that these methods would be even more successful if implemented with these alternative research systems. Furthermore, Takahashi et al. [19] have shown that it is possible to obtain useful centroid- of-contact information with spherical ﬁngertips. This is a feature we were unable to reproduce with the PR2’s ﬂat ﬁngertips, but we may attempt to redesign the ﬁngertip shape in the future if it proves highly beneﬁcial. The addition of shear-force sensing capabilities to the ﬁngertip may also prove an important indica- tor of slip information, and it is close parallel to an important mechanoreceptor of the hand we do not currently mimic, i.e., the SA-II channel, which detects skin stretch. The implementation presented in this paper deals with the case of a two-ﬁngered parallel-jaw gripper. The grasp strategies with such a hand are limited, and this is one aspect of why we have focused so narrowly on simple pick-and-place tasks. As more advanced multiﬁngered robot hands come into use, we see these same contact signals that we have described being extremely useful in less task-speciﬁc ways. Research is currently underway to develop a tactile-event-driven state machine for object interaction, which should allow for rich feedback during a wider variety of interesting multiﬁngered interactions and arm motions. The work presented in this paper is actively used in multiple PR2 robots around the world. As we continue to reﬁne this system and increase the range of objects it can handle, all relevant code is freely available at https://code.ros.org/, and we encourage interested researchers to contact us about porting these capabilities to additional robot platforms. ACKNOWLEDGMENT The authors thank M. Ciocarlie for his help in adding the de- scribed grasp controller to the PR2 object manipulation frame- work, and they would also like to thank D. King for his assistance in troubleshooting the PR2 accelerometer signals. REFERENCES [1] C. C. Kemp, A. Edsinger, and E. Torres-Jara, “Challenges for robot ma- nipulation in human environments: Developing robots that perform useful work in everyday settings,” IEEE Robot. Autom. Mag., vol. 14, no. 1, pp. 20–29, Mar. 2007. [2] R. B. Rusu, A. Holzbach, R. Diankov, G. Bradski, and M. Beetz, “Per- ception for mobile manipulation and grasping using active stereo,” in Proc. IEEE-RAS Int. Conf. Humanoid Robots, Paris, France, Dec. 2009, pp. 632–638. [3] A. Saxena, J. Driemeyer, and A. Y. Ng, “Robotic grasping of novel objects using vision,” Int. J. Robot. Res., vol. 27, no. 2, pp. 157–173, Feb. 2008. [4] R. S. Johansson and J. R. Flanagan, “Coding and use of tactile signals from the ﬁngertips in object manipulation tasks,” Nat. Rev. Neurosci., vol. 10, pp. 345–359, May 2009. [5] M. A. Srinivasan, J. M. Whitehouse and R. H. LaMotte, “Tactile detection of slip: Surface microgeometry and peripheral neural codes,” J. Neuro- physiol., vol. 63, pp. 1323–1332, 1990. [6] J. Bell, S. Bolanowski, and M. H. Holmes, “The structure and function of pacinian corpuscles: A review,” Progr. Neurobiol., vol. 42, no. 1, pp. 79–128, Jan. 1994. [7] G. A. Gescheider, J. H. Wright, and R. T. Verrillo, Information-Processing Channels in the Tactile Sensory System: A Psychophysical and Physiolog- ical Analysis. New York: Psychology, Taylor & Francis Group, 2009. [8] K. O. Johnson, “The roles and functions of cutaneous mechanoreceptors,” Curr. Opin. Neurobiol., vol. 11, pp. 455–461, 2001. ROMANO et al.: HUMAN-INSPIRED ROBOTIC GRASP CONTROL WITH TACTILE SENSING 1079 [9] M. R. Cutkosky, R. D. Howe, and W. R. Provancher, Springer Handbook of Robotics, B. Siciliano and O. Khatib, Eds., New York: Springer-Verlag, 2008. [10] R. S. Dahiya, M. Gori, G. Metta, and G. Sandini, “Better manipulation with human inspired tactile sensing,” in Proc. Robot. Sci. Syst. Workshop Understand. Human Hand Advanc. Robot. Manipulat., 2011, pp. 36–37. [11] A. Schmitz, M. Maggiali, M. Randazzo, L. Natale, and G. Metta, “A prototype ﬁngertip with high spatial resolution pressure sensing for the robot iCub,” in Proc. IEEE-RAS Int. Conf. Humanoid Robots, Dec. 2008, pp. 423–428. [12] T. Martin, R. Ambrose, M. Diftler, R. Platt, and M. J. Butzer, “Tactile gloves for autonomous grasping with the NASA/DARPA Robonaut,” in Proc. Int. Conf. Robot. Autom., 2004, pp. 1713–1718. [13] R. D. Howe, N. Popp, P. Akella, I. Kao, and M. R. Cutkosky, “Grasping, manipulation, and control with tactile sensing,” in Proc. IEEE Int. Conf. Robot. Autom., 1990, pp. 1258–1263. [14] K. Hsiao, S. Chitta, M. Ciocarlie, and E. G. Jones, “Contact-reactive grasp- ing of objects with partial shape information,” presented at the Intelligent Robots Syst. Conf., St. Louis, MO, 2010. [15] L. Natale and E. Torres-Jara, “A sensitive approach to grasping,” Presented at the 6th Int. Conf. Epigenetic Robots, Paris, France, Sep. 2006. [16] A. Jain and C. C. Kemp, “EL-E: An assistive mobile manipulator that autonomously fetches objects from ﬂat surfaces,” Auton. Robots, vol. 28, pp. 45–64, 2010. [17] A. M. Dollar, L. P. Jentoft, J. H. Gao, and R. D. Howe, “Contact sensing and grasping performance of compliant hands,” Auton. Robots, vol. 28, pp. 65–75, 2010. [18] H. Yussof, M. Ohka, H. Suzuki, N. Morisawa, and J. Takata, “Tactile sensing-based control architecture in multi-ﬁngered arm for object ma- nipulation,” Eng. Lett., vol. 16, no. 2, 2009. [19] T. Takahashi, T. Tsuboi, T. Kishida, Y. Kawanami, S. Shimizu, M. Iribe, T. Fukushima, and M. Fujita, “Adaptive grasping by mulit ﬁngered hand with tactile sensor based on robust force and position control,” in Proc. Int. Conf. Robot. Autom., 2008, pp. 264–271. [20] B. Eberman and J. K. Salisbury, “Application of change detection to dynamic contact sensing,” Int. J. Robot. Res., vol. 13, pp. 369–394, 1994. [21] D. Dornfeld and C. Handy, “Slip detection using acoustic emission signal analysis,” in Proc. Int. Conf. Robot. Autom., 1987, pp. 1868–1875. [22] N. Wettels, V. J. Santos, R. S. Johansson, and G. E. Loeb, “Biomimetic tactile sensor array,” Adv. Robot., vol. 22, pp. 829–849, 2008. [23] A. Kis, F. Kov´acs, and P. Szolgay, “Grasp planning based on ﬁngertip contact forces and torques,” presented at the Eurohaptics Conf., Paris, France, 2006. [24] J. M. Romano, S. R. Gray, N. T. Jacobs, and K. J. Kuchenbecker, “To- ward tactilely transparent glove: Collocated slip sensing and vibrotactile actuation,” in Proc. World Haptics, 2009, pp. 279–284. [25] S. Chitta, M. Piccoli, and J. Sturm, “Tactile object class and internal state recognition,” in Proc. Int. Conf. Robot. Autom., 2010, pp. 2342–2348. [26] D. Whitney, “Historical perspective and state of the art in robot force control,” Int. J. Robot. Res., vol. 6, no. 1, pp. 3–14, 1987. [27] S. Allin, Y. Matsuoka, and R. Klatzky, “Measuring just noticeable differ- ences for haptic force feedback: Implications for rehabilitation,” in Proc. IEEE Hapt. Symp., Mar. 2002, pp. 299–302. Joseph M. Romano (S’08) received the B.S. degrees in both computer science and engineering mechan- ics from Johns Hopkins University, Baltimore, MD, in 2007 and the Master’s degree in mechanical en- gineering and applied mechanics (MEAM) from the University of Pennsylvania, Philadelphia, PA, in May 2010. He is currently working toward the Ph.D. de- gree with the Department of MEAM, University of Pennsylvania. He is in Dr. K. Kuchenbecker’s Haptics Research Group, which is a part of the General Robotics, Au- tomation, Sensing and Perception (GRASP) Laboratory, where his research is focused on investigating the relationship between kinesthetic large-scale motion information and localized tactile and cutaneous signals to improve human and machine haptic systems. Kaijen Hsiao (M’10) received the Ph.D. degree in computer science from the Massachusetts Institute of Technology, Cambridge, MA, in 2009. She is currently a Research Scientist with Willow Garage, Inc., Menlo Park, CA. Her research interests include grasping and manipulation, particularly with respect to dealing with uncertainty, incorporating in- formation from diverse sensors, and adding robust- ness to robot capabilities. G ¨unter Niemeyer (M’11) received the M.S. and Ph.D. degrees from the Massachusetts Institute of Technology (MIT), Cambridge, in the areas of adap- tive robot control and bilateral teleoperation, intro- ducing the concept of wave variables. He is currently a Senior Research Scientist with Willow Garage, Inc., Menlo Park, CA, and a Consult- ing Professor of mechanical engineering with Stan- ford University, Stanford, CA. He also held a Post- doctoral Research Position at MIT, where he develops surgical robotics. In 1997, he joined Intuitive Surgical Inc., where he helped create the da Vinci Minimally Invasive Surgical System. In the Fall of 2001, he joined the Stanford Faculty, where he directs the Telerobotics Laboratory and teaches dynamics, controls, and telerobotics. Since 2009, he has been a member of the Willow Garage Research Group. His research examines physical human–robot interactions, force sensitivity and feedback, teleoperation with and without communication delays, and haptic interfaces. This involves efforts ranging from real-time motor and robot control to user interface design. Sachin Chitta (M’11) received the Ph.D. degree in mechanical engineering and applied mechanics from the University of Pennsylvania, Philadelphia, in 2005. From 2005 to 2007, he was a Postdoctoral Re- searcher with the GRASP Laboratory, University of Pennsylvania, where he was involved in research on learning for quadruped locomotion and modular robotics. He is currently a Research Scientist with Willow Garage, Inc., Menlo Park, CA. At Willow Garage, Inc., he has been involved in motion plan- ning and control for mobile manipulation tasks such as door opening, cart pushing, and object retrieval, as well as tactile sensing for reactive grasping and object state recognition with the PR2 mobile manipulation platform. His research interests include motion planning, control, and sensing for mobile manipulation with a particular interest in making robots useful in unstructured indoor environments. Katherine J. Kuchenbecker (M’06) received the Ph.D. degree in mechanical engineering from Stan- ford University, Stanford, CA, in 2006. She is currently the Skirkanich Assistant Profes- sor of innovation with the Department of Mechanical Engineering and Applied Mechanics, University of Pennsylvania, Philadelphia. She also directs the Penn Haptics Group, which is part of the GRASP Robotics Laboratory. Prior to becoming a Professor, she was a Postdoctoral Fellow with the Johns Hopkins Univer- sity, Baltimore, MD. Her research interests include the design and control of haptic interfaces. Dr. Kuchenbecker serves on the program committee for the IEEE Haptics Symposium. She has won several awards for her research, including a National Science Foundation CAREER award in 2009 and a Popular Science Brilliant 10 Award in 2010.","libVersion":"0.3.2","langs":""}